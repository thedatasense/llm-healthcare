{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccf97e0",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thedatasense/llm-healthcare/blob/main/App-C%20-%20Building%20LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837842c559df940",
   "metadata": {
    "id": "2837842c559df940"
   },
   "source": [
    "# Building LLM from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f055daed1bbd480",
   "metadata": {
    "id": "4f055daed1bbd480"
   },
   "source": [
    "This part is heavily inspired based on the work by Sebastian Raschka <br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "viKt9FYNR6V5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viKt9FYNR6V5",
    "outputId": "622a82ba-f705-46c3-e001-436139f4eb2d",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:54.472879Z",
     "start_time": "2025-01-23T04:53:53.073399Z"
    }
   },
   "source": [
    "!pip install tiktoken"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/homebrew/Caskroom/miniconda/base/envs/explore/lib/python3.12/site-packages (0.8.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/Caskroom/miniconda/base/envs/explore/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/explore/lib/python3.12/site-packages (from tiktoken) (2.28.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/explore/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/explore/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/explore/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/explore/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\r\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "35c8cf19-92d5-4ce7-db8c-935ad140f3bc",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:54.481928Z",
     "start_time": "2025-01-23T04:53:54.477760Z"
    }
   },
   "source": [
    "from importlib.metadata import version\n",
    "import os\n",
    "import urllib.request\n",
    "import importlib\n",
    "import tiktoken\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:54.498691Z",
     "start_time": "2025-01-23T04:53:54.496586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(1)  # Set GPU 1 as the active device\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ],
   "id": "80bf18535372096a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "7dfd9986-1004-449f-bc83-88b358c67942",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is from a [Kaggle Medical Text dataset](https://www.kaggle.com/datasets/chaitanyakck/medical-text) dataset that is from abstracts written during routine rounds by doctors explaining the current conditions of the patient"
   ]
  },
  {
   "cell_type": "code",
   "id": "hjf87p6Tq1dg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hjf87p6Tq1dg",
    "outputId": "bbce503d-967a-4e86-a776-2faa104781c2",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:58.510137Z",
     "start_time": "2025-01-23T04:53:54.514028Z"
    }
   },
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/sebischair/Medical-Abstracts-TC-Corpus/refs/heads/main/medical_tc_train.csv\")\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   condition_label  \\\n",
       "0                5   \n",
       "1                1   \n",
       "2                2   \n",
       "3                1   \n",
       "4                3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              medical_abstract  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Tissue changes around loose prostheses. A canine model to investigate the effects of an antiinflammatory agent. The aseptically loosened prosthesis provided a means for investigating the in vivo and in vitro activity of the cells associated with the loosening process in seven dogs. The cells were isolated and maintained in culture for sufficient periods of time so that their biologic activity could be studied as well as the effect of different agents added to the cells in vivo or in vitro. The biologic response as determined by interleukin-1 and prostaglandin E2 activity paralleled the roentgenographic appearance of loosening and the technetium images and observations made at the time of revision surgery. The correlation between clinical, roentgenographic, histologic, and biochemical loosening indicates that the canine model is suitable for investigating the mechanisms of prosthetic failure. A canine model permits the study of possible nonsurgical therapeutic interventions with the ultimate hope of stopping or slowing the loosening process.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Neuropeptide Y and neuron-specific enolase levels in benign and malignant pheochromocytomas. Neuron-specific enolase (NSE) is the isoform of enolase, a glycolytic enzyme found in the neuroendocrine system. Neuropeptide Y (NPY) is a peptide recently discovered in the peripheral and central nervous systems. Serum NSE and plasma NPY levels have been reported to be increased in some patients with pheochromocytoma. The authors evaluated whether the measurement of these molecules could help to discriminate between benign and malignant forms of pheochromocytoma. The NSE levels were normal in all patients with benign pheochromocytoma (n = 13) and elevated in one half of those with malignant pheochromocytoma (n = 13). Plasma NPY levels were on the average significantly higher in the malignant (177.1 +/- 38.9 pmol/l, n = 16) than in the benign forms of the disease (15.7 +/- 389 pmol/l, n = 24). However, there was no difference in the percentage of patients with elevated NPY levels. These results show that determination of serum NSE may be useful for distinguishing between malignant and benign pheochromocytoma; the measurement of plasma NPY is not useful for differentiating the two kinds of tumors.   \n",
       "2  Sexually transmitted diseases of the colon, rectum, and anus. The challenge of the nineties. During the past two decades, an explosive growth in both the prevalence and types of sexually transmitted diseases has occurred. Up to 55 percent of homosexual men with anorectal complaints have gonorrhea; 80 percent of the patients with syphilis are homosexuals. Chlamydia is found in 15 percent of asymptomatic homosexual men, and up to one third of homosexuals have active anorectal herpes simplex virus. In addition, a host of parasites, bacterial, viral, and protozoan are all rampant in the homosexual population. Furthermore, the global epidemic of AIDS has produced a plethora of colorectal manifestations. Acute cytomegalovirus ileocolitis is the most common indication for emergency abdominal surgery in the homosexual AIDS population. Along with cryptosporidia and isospora, the patient may present to the colorectal surgeon with bloody diarrhea and weight loss before the diagnosis of human immunodeficiency virus (HIV) disease. Other patients may present with colorectal Kaposi's sarcoma or anorectal lymphoma, and consequently will be found to have seropositivity for HIV. However, in addition to these protean manifestations, one third of patients with AIDS consult the colorectal surgeon with either condylomata acuminata, anorectal sepsis, or proctitis before the diagnosis of HIV disease. Although aggressive anorectal surgery is associated with reasonable surgical results in some asymptomatic HIV positive patients, the same procedures in AIDS (symptomatic HIV positive) patients will often be met with disastrous results. It is incumbent upon the surgeon, therefore, to recognize the manifestations of HIV disease and diagnose these conditions accordingly.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Lipolytic factors associated with murine and human cancer cachexia. We have identified a lipolytic factor in extracts of a cachexia-inducing murine carcinoma (MAC16) that shows characteristics of an acidic peptide and appears to be composed of three fractions of apparent molecular weights corresponding to 3 kd, 1.5 kd, and 0.7 kd, as determined by exclusion chromatography. Material with identical chromatographic and molecular weight characteristics was also present in the serum of patients with clinical cancer cachexia but absent from normal serum, even under conditions of starvation. The MAC16 lipid factor, when injected into animals bearing the non-cachexia-inducing tumor MAC13, was capable of inducing weight loss without a significant reduction in food intake. Similar lipolytic material, although in lower concentration, was also found in the MAC13 tumor extracts. These findings suggest that cachexia may arise from the enhanced expression of a lipolytic factor associated with tumor cells.   \n",
       "4                                                                                                                                                                                                                                                                                            Does carotid restenosis predict an increased risk of late symptoms, stroke, or death? The identification of carotid restenosis as an unexpected late complication of carotid endarterectomy has prompted concerns regarding its importance as a source of new cerebral symptoms, stroke, and death. To investigate these concerns, we analyzed a consecutive series of 507 patients undergoing 566 carotid endarterectomies, each documented as technically satisfactory. Post-operative duplex Doppler examination data at 3 days, 1, 3, 6, 12 months, and annually thereafter in 484 arteries (85.5%) permitted classification of these arteries according to the most severe degree of postoperative stenosis: normal (n = 306); 1% to 19% (n = 89); 20% to 50% (n = 40); more than 50% (n = 49, including 8 occluded). The incidence of more than 50% restenosis was 14.5% in female and 7.7% in male patients (p = 0.003). Life table analyses to 10 years revealed a significantly greater life expectancy among those with restenosis (p = 0.05). Stroke was also less likely in patients with restenosis, although this difference did not reach statistical significance. When survival and stroke were both endpoints, the likelihood of patients with more than 50% restenosis remaining alive and stroke free was also greater than the less than 20% stenotic group (p = 0.03). Thus patients with carotid restenosis were less likely than patients with normal postoperative scans to have late symptoms, stroke, or early death.   "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition_label</th>\n",
       "      <th>medical_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Tissue changes around loose prostheses. A canine model to investigate the effects of an antiinflammatory agent. The aseptically loosened prosthesis provided a means for investigating the in vivo and in vitro activity of the cells associated with the loosening process in seven dogs. The cells were isolated and maintained in culture for sufficient periods of time so that their biologic activity could be studied as well as the effect of different agents added to the cells in vivo or in vitro. The biologic response as determined by interleukin-1 and prostaglandin E2 activity paralleled the roentgenographic appearance of loosening and the technetium images and observations made at the time of revision surgery. The correlation between clinical, roentgenographic, histologic, and biochemical loosening indicates that the canine model is suitable for investigating the mechanisms of prosthetic failure. A canine model permits the study of possible nonsurgical therapeutic interventions with the ultimate hope of stopping or slowing the loosening process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Neuropeptide Y and neuron-specific enolase levels in benign and malignant pheochromocytomas. Neuron-specific enolase (NSE) is the isoform of enolase, a glycolytic enzyme found in the neuroendocrine system. Neuropeptide Y (NPY) is a peptide recently discovered in the peripheral and central nervous systems. Serum NSE and plasma NPY levels have been reported to be increased in some patients with pheochromocytoma. The authors evaluated whether the measurement of these molecules could help to discriminate between benign and malignant forms of pheochromocytoma. The NSE levels were normal in all patients with benign pheochromocytoma (n = 13) and elevated in one half of those with malignant pheochromocytoma (n = 13). Plasma NPY levels were on the average significantly higher in the malignant (177.1 +/- 38.9 pmol/l, n = 16) than in the benign forms of the disease (15.7 +/- 389 pmol/l, n = 24). However, there was no difference in the percentage of patients with elevated NPY levels. These results show that determination of serum NSE may be useful for distinguishing between malignant and benign pheochromocytoma; the measurement of plasma NPY is not useful for differentiating the two kinds of tumors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sexually transmitted diseases of the colon, rectum, and anus. The challenge of the nineties. During the past two decades, an explosive growth in both the prevalence and types of sexually transmitted diseases has occurred. Up to 55 percent of homosexual men with anorectal complaints have gonorrhea; 80 percent of the patients with syphilis are homosexuals. Chlamydia is found in 15 percent of asymptomatic homosexual men, and up to one third of homosexuals have active anorectal herpes simplex virus. In addition, a host of parasites, bacterial, viral, and protozoan are all rampant in the homosexual population. Furthermore, the global epidemic of AIDS has produced a plethora of colorectal manifestations. Acute cytomegalovirus ileocolitis is the most common indication for emergency abdominal surgery in the homosexual AIDS population. Along with cryptosporidia and isospora, the patient may present to the colorectal surgeon with bloody diarrhea and weight loss before the diagnosis of human immunodeficiency virus (HIV) disease. Other patients may present with colorectal Kaposi's sarcoma or anorectal lymphoma, and consequently will be found to have seropositivity for HIV. However, in addition to these protean manifestations, one third of patients with AIDS consult the colorectal surgeon with either condylomata acuminata, anorectal sepsis, or proctitis before the diagnosis of HIV disease. Although aggressive anorectal surgery is associated with reasonable surgical results in some asymptomatic HIV positive patients, the same procedures in AIDS (symptomatic HIV positive) patients will often be met with disastrous results. It is incumbent upon the surgeon, therefore, to recognize the manifestations of HIV disease and diagnose these conditions accordingly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Lipolytic factors associated with murine and human cancer cachexia. We have identified a lipolytic factor in extracts of a cachexia-inducing murine carcinoma (MAC16) that shows characteristics of an acidic peptide and appears to be composed of three fractions of apparent molecular weights corresponding to 3 kd, 1.5 kd, and 0.7 kd, as determined by exclusion chromatography. Material with identical chromatographic and molecular weight characteristics was also present in the serum of patients with clinical cancer cachexia but absent from normal serum, even under conditions of starvation. The MAC16 lipid factor, when injected into animals bearing the non-cachexia-inducing tumor MAC13, was capable of inducing weight loss without a significant reduction in food intake. Similar lipolytic material, although in lower concentration, was also found in the MAC13 tumor extracts. These findings suggest that cachexia may arise from the enhanced expression of a lipolytic factor associated with tumor cells.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Does carotid restenosis predict an increased risk of late symptoms, stroke, or death? The identification of carotid restenosis as an unexpected late complication of carotid endarterectomy has prompted concerns regarding its importance as a source of new cerebral symptoms, stroke, and death. To investigate these concerns, we analyzed a consecutive series of 507 patients undergoing 566 carotid endarterectomies, each documented as technically satisfactory. Post-operative duplex Doppler examination data at 3 days, 1, 3, 6, 12 months, and annually thereafter in 484 arteries (85.5%) permitted classification of these arteries according to the most severe degree of postoperative stenosis: normal (n = 306); 1% to 19% (n = 89); 20% to 50% (n = 40); more than 50% (n = 49, including 8 occluded). The incidence of more than 50% restenosis was 14.5% in female and 7.7% in male patients (p = 0.003). Life table analyses to 10 years revealed a significantly greater life expectancy among those with restenosis (p = 0.05). Stroke was also less likely in patients with restenosis, although this difference did not reach statistical significance. When survival and stroke were both endpoints, the likelihood of patients with more than 50% restenosis remaining alive and stroke free was also greater than the less than 20% stenotic group (p = 0.03). Thus patients with carotid restenosis were less likely than patients with normal postoperative scans to have late symptoms, stroke, or early death.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "OB-1THNfqNeg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OB-1THNfqNeg",
    "outputId": "8258c727-fc0f-44e5-dbdf-bdb643ef8f17",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:58.534562Z",
     "start_time": "2025-01-23T04:53:58.530495Z"
    }
   },
   "source": [
    "medical_abstracts = df['medical_abstract'].astype(str).tolist()  # Extract the column and convert to a list of strings\n",
    "text_data = ' '.join(medical_abstracts)  # Combine all abstracts into a single string\n",
    "# Display the total number of characters and a preview of the combined text\n",
    "print(\"Total number of characters:\", len(text_data))\n",
    "print(text_data[300:700])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 14207702\n",
      "olated and maintained in culture for sufficient periods of time so that their biologic activity could be studied as well as the effect of different agents added to the cells in vivo or in vitro. The biologic response as determined by interleukin-1 and prostaglandin E2 activity paralleled the roentgenographic appearance of loosening and the technetium images and observations made at the time of rev\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "f0266df8f28e3565",
   "metadata": {
    "id": "f0266df8f28e3565"
   },
   "source": [
    "### GPT-2 Tokenization: Byte-Pair Encoding ###\n",
    "\n",
    "- GPT-2 used Byte-Pair Encoding  as its tokenizer.\n",
    "- It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words.\n",
    "- For instance, if GPT-2’s vocabulary doesn’t have the word *\"unfamiliarword\"*, it might tokenize it as `[\"unfam\", \"iliar\", \"word\"]` or some other subword breakdown, depending on its trained BPE merges.\n",
    "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1a8b22655248c74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1a8b22655248c74",
    "outputId": "a26e24ae-11d7-4eb9-986d-1935dc7d4153",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:59.252458Z",
     "start_time": "2025-01-23T04:53:58.555121Z"
    }
   },
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "enc_text = tokenizer.encode(text_data)\n",
    "print(\"Total number of character:\", len(enc_text))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 3055877\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "a3bf7629-b7a9-4c57-84bb-088dd9771d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:59.294471Z",
     "start_time": "2025-01-23T04:53:59.292376Z"
    }
   },
   "source": [
    "enc_text[0:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 11407, 4442, 2212, 20784]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "aab09b328d0a4cd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aab09b328d0a4cd4",
    "outputId": "2d0999f2-2066-41c3-fb20-c6650c410ea8",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:59.339984Z",
     "start_time": "2025-01-23T04:53:59.312354Z"
    }
   },
   "source": [
    "context_size = 10\n",
    "enc_sample = enc_text[200:]\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "\n",
    "# what are we expecting the model to predict based on the context\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [6147, 1920, 13, 220, 4275, 4176, 47309, 816, 323, 49384]\n",
      "y:      [1920, 13, 220, 4275, 4176, 47309, 816, 323, 49384, 19440]\n",
      "ening ---->  process\n",
      "ening process ----> .\n",
      "ening process. ---->  \n",
      "ening process.  ---->  Ne\n",
      "ening process.  Ne ----> urope\n",
      "ening process.  Neurope ----> ptide\n",
      "ening process.  Neuropeptide ---->  Y\n",
      "ening process.  Neuropeptide Y ---->  and\n",
      "ening process.  Neuropeptide Y and ---->  neuron\n",
      "ening process.  Neuropeptide Y and neuron ----> -specific\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "951e579ffe61f859",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "951e579ffe61f859",
    "outputId": "53cae6ab-4705-45a5-b717-7f7ca9029401",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:54:00.074018Z",
     "start_time": "2025-01-23T04:53:59.345430Z"
    }
   },
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "#Characters:→ Total raw characters in our text data.\n",
    "print(\"Characters:\", total_characters)\n",
    "#Tokens:→ Number of units after applying the tokenizer (subwords, spaces, punctuation and so on)\n",
    "print(\"Tokens:\", total_tokens)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 14207702\n",
      "Tokens: 3055877\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "57966b060ca1a145",
   "metadata": {
    "id": "57966b060ca1a145"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "102dde1e-eef2-4b66-8671-a1a0e43650f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:57:56.650055Z",
     "start_time": "2025-01-23T04:57:56.646808Z"
    }
   },
   "source": [
    "class MedCorpusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_len,stride):\n",
    "        self.input_ids = []\n",
    "        self.output_ids = []\n",
    "        token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
    "        # lets use a sliding window to chunk the text in to overlapping sequences of maximum length\n",
    "        for i in range (0,len(token_ids) - max_len,stride):\n",
    "            input_chunk = token_ids[i:i+max_len]\n",
    "            target_chunk = token_ids[i+1 :i+max_len+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.output_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.output_ids[idx]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "id": "7b2f62b774790d6",
   "metadata": {
    "id": "7b2f62b774790d6"
   },
   "source": [
    "Here we will code the architecture of the smallest GPT-2 model (124 million parameters), as outlined in Radford et al.'s [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "- `\"vocab_size\"` indicates a vocabulary size of 50,257 words, supported by the BPE tokenizer\n",
    "- `\"context_length\"` represents the model's maximum input token count, as enabled by positional embeddings covered\n",
    "- `\"emb_dim\"` is the embedding size for token inputs, converting each input token into a 768-dimensional vector\n",
    "- `\"n_heads\"` is the number of attention heads in the multi-head attention mechanism implemented\n",
    "- `\"n_layers\"` is the number of transformer blocks within the model\n",
    "- `\"drop_rate\"` is the dropout mechanism's intensity,.1 means dropping 10% of hidden units during training to mitigate overfitting\n",
    "- `\"qkv_bias\"` decides if the `Linear` layers in the multi-head attention mechanism should include a bias vector when computing query (Q), key (K), and value (V) tensors;"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:57:58.900584Z",
     "start_time": "2025-01-23T04:57:58.898303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_loader(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    dataset = MedCorpusDataset(txt,tokenizer,max_length,stride)\n",
    "    dataloader = DataLoader (\n",
    "        dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
    "    return dataloader"
   ],
   "id": "609f59fd3a54d7d4",
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "id": "cf5ae3fc732ce15b",
   "metadata": {
    "id": "cf5ae3fc732ce15b"
   },
   "source": [
    "### Transformer Architecture Setup\n",
    "\n",
    "*   **Multi-Head Attention** learns relationships between inputs by projecting them into **queries**, **keys**, and **values**.\n",
    "*   Input is split into multiple **heads**, each learning different attention patterns.\n",
    "*   **Scaled dot-product attention** calculates similarity between queries and keys, creating attention weights.\n",
    "*   A **causal mask** prevents the model from attending to future tokens in sequence generation.\n",
    "*   Outputs from all heads are concatenated and projected, providing a context-aware representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "650e854dd524c24a",
   "metadata": {
    "id": "650e854dd524c24a",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:01.060807Z",
     "start_time": "2025-01-23T04:58:01.056501Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Dimension of each attention head\n",
    "\n",
    "        # Linear projections for query, key, and value\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask to prevent attending to future tokens\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # Batch size, sequence length, input dimension\n",
    "\n",
    "        # Project the input to query, key, and value\n",
    "        keys = self.W_key(x)     # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x) # Shape: (b, num_tokens, d_out)\n",
    "        values = self.W_value(x) # Shape: (b, num_tokens, d_out)\n",
    "\n",
    "        # Split the projected vectors into multiple heads\n",
    "        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to prepare for matrix multiplication\n",
    "        # (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Calculate attention scores (scaled dot-product attention)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # Apply causal mask to attention scores\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # Truncate mask to current sequence length\n",
    "        attn_scores = attn_scores.masked_fill(mask_bool, float('-inf')) # Mask out future tokens\n",
    "\n",
    "        # Normalize attention scores with softmax\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) # Scale by sqrt(head_dim)\n",
    "\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Calculate the weighted sum of values\n",
    "        # (b, num_heads, num_tokens, num_tokens) @ (b, num_heads, num_tokens, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        # Concatenate the outputs from all heads\n",
    "        # (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "\n",
    "        # Reshape to combine heads: (b, num_tokens, num_heads, head_dim) -> (b, num_tokens, d_out)\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "\n",
    "        # Apply final linear projection\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GELU Activation Function\n",
    "\n",
    "The **Gaussian Error Linear Unit (GELU)** is an activation function defined as:\n",
    "\n",
    "$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$\n",
    "\n",
    "where \\(\\Phi(x)\\) is the cumulative distribution function (CDF) of the standard normal distribution:\n",
    "\n",
    "$\n",
    "\\Phi(x) = 0.5 \\cdot \\left(1 + \\text{tanh}\\left(\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right)\\right)\n",
    "$\n",
    "\n"
   ],
   "id": "2edb68de22e24c2b"
  },
  {
   "cell_type": "code",
   "id": "232f12f463a33a44",
   "metadata": {
    "id": "232f12f463a33a44",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:04.145656Z",
     "start_time": "2025-01-23T04:58:04.143688Z"
    }
   },
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "68c19144d9d7e134",
   "metadata": {
    "id": "68c19144d9d7e134",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:05.270372Z",
     "start_time": "2025-01-23T04:58:05.267996Z"
    }
   },
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "dfa2ad07917f340a",
   "metadata": {
    "id": "dfa2ad07917f340a",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:08.347572Z",
     "start_time": "2025-01-23T04:58:08.345445Z"
    }
   },
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "id": "61852b0c8dd5a3f5",
   "metadata": {
    "id": "61852b0c8dd5a3f5"
   },
   "source": [
    "### Transformer Block\n",
    "\n",
    "#### 1. Multi-Head Attention (`self.att`)\n",
    "\n",
    "*   **Purpose:** This is where the model learns to \"attend\" to different parts of the input sequence. It calculates relationships between words (or tokens) in the sequence, regardless of their distance. This is crucial for understanding context and long-range dependencies.\n",
    "*   **How it works:**\n",
    "    *   The input `x` is projected into three different representations: **queries (Q)**, **keys (K)**, and **values (V)**.\n",
    "    *   The model calculates **attention scores** by taking the dot product of queries and keys, scaled down by the square root of the key dimension.\n",
    "    *   These scores are passed through a softmax to get **attention weights**, representing the importance of each word to every other word.\n",
    "    *   The attention weights are used to compute a weighted sum of the values, producing the **context vector**.\n",
    "    *   This process is done multiple times in parallel with different learned projections (**multiple heads**), allowing the model to capture diverse relationships.\n",
    "*   **Why it's important:** Unlike recurrent models (like RNNs), self-attention can process the entire sequence in parallel, making it much faster. It also directly models relationships between any two words, regardless of their position, which helps with long-range dependencies.\n",
    "\n",
    "#### 2. Feed-Forward (`self.ff`)\n",
    "\n",
    "*   **Purpose:** A simple, fully connected feed-forward network that processes each token's representation independently. This adds non-linearity and further transforms the information learned by the attention mechanism.\n",
    "*   **How it works:** Typically, it consists of two linear layers with a non-linear activation function (like GELU or ReLU) in between.\n",
    "*   **Why it's important:** It provides additional processing power and allows the model to learn more complex patterns from the attention outputs.\n",
    "\n",
    "#### 3. LayerNorm (`self.norm1`, `self.norm2`)\n",
    "\n",
    "*   **Purpose:** Layer normalization normalizes the activations across the feature dimension (embedding dimension). It helps stabilize training and can improve performance.\n",
    "*   **How it works:** It subtracts the mean and divides by the standard deviation of the activations within each individual embedding vector (across the embedding dimension).\n",
    "*   **Why it's important:** It prevents the activations from becoming too large or too small, which can lead to unstable gradients during training. It also makes the model less sensitive to the scale of the input features.\n",
    "\n",
    "#### 4. Dropout (`self.drop_shortcut`)\n",
    "\n",
    "*   **Purpose:** A regularization technique that randomly sets a fraction of the activations to zero during training. This helps prevent overfitting.\n",
    "*   **How it works:** During each forward pass, it randomly \"drops out\" (sets to zero) a certain percentage of the activations, determined by the `drop_rate`.\n",
    "*   **Why it's important:** It forces the model to learn more robust features that are not overly reliant on any single activation.\n",
    "\n",
    "#### 5. Residual Connections (Shortcut Connections)\n",
    "\n",
    "*   **Purpose:** The output of each sub-block (MHA and FFN) is added to its original input (`x = x + shortcut`). This helps with the flow of gradients during training, especially in deep networks.\n",
    "*   **How it works:** The original input `x` is added to the output of the sub-block before being passed to the next layer.\n",
    "*   **Why it's important:**\n",
    "    *   **Vanishing Gradients:** In very deep networks, gradients can become very small as they are backpropagated through many layers, making training difficult. Residual connections provide a \"shortcut\" for the gradients to flow back, mitigating this problem.\n",
    "    *   **Identity Mapping:** They make it easier for the network to learn the identity function (where the output is the same as the input). This is because if a layer is not needed, the network can easily learn to set its weights to zero, effectively making it pass through the input unchanged.\n",
    "\n",
    "#### `forward(self, x)` Method:\n",
    "\n",
    "The `forward` method defines how the input `x` is processed through the `TransformerBlock`:\n",
    "\n",
    "1.  **Attention Block:**\n",
    "    *   Store the original input for the residual connection.\n",
    "    *   Apply layer normalization.\n",
    "    *   Pass through the multi-head attention mechanism.\n",
    "    *   Apply dropout to the output of the attention block\n",
    "    *   Add the original input back (residual connection).\n",
    "\n",
    "2.  **Feed-Forward Block:**\n",
    "    *   Store the output from the attention block for the residual connection.\n",
    "    *   Apply layer normalization.\n",
    "    *   Pass through the feed-forward network.\n",
    "    *   Apply dropout to the output of the feed-forward block.\n",
    "    *   Add the output from the attention block back (residual connection).\n",
    "\n",
    "3.  Return the processed output.\n",
    "\n",
    "**In essence, the `TransformerBlock` takes an input sequence, allows it to attend to itself, processes it further with a feed-forward network, and uses layer normalization and residual connections to improve training and performance. Multiple `TransformerBlock` instances are stacked together to create the full Transformer model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9b626139fb43755",
   "metadata": {
    "id": "a9b626139fb43755",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:19.248400Z",
     "start_time": "2025-01-23T04:58:19.243601Z"
    }
   },
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "943c3193786f7f3",
   "metadata": {
    "id": "943c3193786f7f3",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:21.396953Z",
     "start_time": "2025-01-23T04:58:21.394075Z"
    }
   },
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "4e01cc589eabd27a",
   "metadata": {
    "id": "4e01cc589eabd27a",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:29.402806Z",
     "start_time": "2025-01-23T04:58:28.750543Z"
    }
   },
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 512, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "id": "184a40a60cf9e62e",
   "metadata": {
    "id": "184a40a60cf9e62e",
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:32.062938Z",
     "start_time": "2025-01-23T04:58:30.813518Z"
    }
   },
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = data_loader(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = data_loader(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "3c03aa1963323d6e",
   "metadata": {
    "id": "3c03aa1963323d6e",
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:20.530198Z",
     "start_time": "2025-01-23T05:26:20.526822Z"
    }
   },
   "source": [
    "def generate_text(model, idx, max_new_tokens, context_size, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text token-by-token using an autoregressive language model.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Use only the most recent context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)[:, -1, :]  # Get logits for the last token\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits /= temperature\n",
    "\n",
    "        # Convert logits to probabilities and sample the next token\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)  # Sample from the probability distribution\n",
    "\n",
    "        # Append the sampled token to the sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "id": "14bcebed163905c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14bcebed163905c9",
    "outputId": "1f98cef2-7668-4045-ef3a-50f603a3d158",
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:24.148716Z",
     "start_time": "2025-01-23T05:26:23.681805Z"
    }
   },
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"It is recommended to take rest when\"\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_ids = generate_text(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:26.378915Z",
     "start_time": "2025-01-23T05:26:26.376795Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Untrained Model Says This :\\n\", token_ids_to_text(token_ids, tokenizer))",
   "id": "2fc0476788ab9a3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained Model Says This :\n",
      " It is recommended to take rest whenestructorCompany=\"#\" QUE Pick]\n",
      "\n",
      "\n",
      "\n",
      "\tcur.Doc labelscop\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "id": "c9310227a8309847",
   "metadata": {
    "id": "c9310227a8309847",
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:35.726880Z",
     "start_time": "2025-01-23T05:26:35.724441Z"
    }
   },
   "source": [
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "id": "5d6ef9137ee0029e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d6ef9137ee0029e",
    "outputId": "f15d0801-c520-460a-d9f9-b5d1fc47a53a",
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:40.894821Z",
     "start_time": "2025-01-23T05:26:40.840038Z"
    }
   },
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 2750464\n",
      "Validation tokens: 305152\n",
      "All tokens: 3055616\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "id": "25409a8ea980f371",
   "metadata": {
    "id": "25409a8ea980f371",
    "ExecuteTime": {
     "end_time": "2025-01-23T06:30:21.431349Z",
     "start_time": "2025-01-23T06:30:21.427376Z"
    }
   },
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"\n",
    "     Computes the loss for a single batch of input and target data.\n",
    "    :param input_batch:\n",
    "    :param target_batch:\n",
    "    :param model:\n",
    "    :param device:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "id": "a3ac1d459789e667",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3ac1d459789e667",
    "outputId": "51e93e7f-edc0-404a-bf1d-3683f0843527",
    "ExecuteTime": {
     "end_time": "2025-01-23T06:36:39.794366Z",
     "start_time": "2025-01-23T06:30:22.765387Z"
    }
   },
   "source": [
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.108678046572324\n",
      "Validation loss: 10.107906194341263\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "id": "3d8c925b397ad7",
   "metadata": {
    "id": "3d8c925b397ad7",
    "ExecuteTime": {
     "end_time": "2025-01-23T06:39:17.972073Z",
     "start_time": "2025-01-23T06:39:17.965813Z"
    }
   },
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "id": "6390565cd43f2e1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6390565cd43f2e1c",
    "outputId": "ba93b3a8-74f2-4c63-be04-1ba9a0a3408c",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-23T06:56:47.472461Z"
    }
   },
   "source": [
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 7\n",
    "train_losses, val_losses, tokens_seen = train_model(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"patient has high\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.396, Val loss 9.405\n",
      "Ep 1 (Step 000005): Train loss 7.986, Val loss 8.031\n",
      "Ep 1 (Step 000010): Train loss 7.108, Val loss 7.259\n",
      "Ep 1 (Step 000015): Train loss 6.982, Val loss 7.000\n",
      "Ep 1 (Step 000020): Train loss 7.063, Val loss 6.948\n",
      "Ep 1 (Step 000025): Train loss 7.065, Val loss 6.872\n",
      "Ep 1 (Step 000030): Train loss 6.524, Val loss 6.746\n",
      "Ep 1 (Step 000035): Train loss 6.680, Val loss 6.705\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "12468471-e84b-41d6-acf8-c7dbc5cf85ed",
   "metadata": {
    "id": "12468471-e84b-41d6-acf8-c7dbc5cf85ed"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"./models/basic_llm_med.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f3012207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:24:52.072636Z",
     "start_time": "2025-01-09T06:24:49.827535Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "f3012207",
    "outputId": "8d776296-3626-4526-8a06-33ba04bc3c1b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "id": "242ee74c13102b9a",
   "metadata": {
    "id": "242ee74c13102b9a",
    "outputId": "c4a458c5-9c46-44f3-c439-51b4861b9bbb",
    "ExecuteTime": {
     "end_time": "2025-01-22T21:10:46.111146Z",
     "start_time": "2025-01-22T21:10:46.078437Z"
    }
   },
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"The effect of smoking\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m      3\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m tiktoken\u001B[38;5;241m.\u001B[39mget_encoding(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b1bd2e8f8f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:26:45.549385Z",
     "start_time": "2025-01-09T06:26:45.542159Z"
    },
    "id": "8e2b1bd2e8f8f2b"
   },
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67a93815df682",
   "metadata": {
    "id": "6b67a93815df682"
   },
   "source": [
    "**Top-k and Temperature**\n",
    "\n",
    " **Top-k** limits the model's choice to the *k* most probable next words, discarding less likely options. **Temperature** scales the probability distribution; higher values increase randomness by making the distribution more uniform, while lower values favor the most likely word by making it more peaked.  For instance, when creating a poem, high temperature with low Top-k might yield unusual word choices. Conversely, low temperature with high Top-k would produce a more predictable, coherent verse.  In essence, they dictate how \"focused\" versus \"creative\" the generated text becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bead5dd339a179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:26:48.861257Z",
     "start_time": "2025-01-09T06:26:48.248391Z"
    },
    "id": "82bead5dd339a179",
    "outputId": "6f5c1249-037f-4449-cf61-9c629fadf4bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you, I will spare for you. Here it was much\n",
      "a gentleman and\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4e6a685d321b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:26:56.639638Z",
     "start_time": "2025-01-09T06:26:56.027106Z"
    },
    "id": "20e4e6a685d321b9"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
