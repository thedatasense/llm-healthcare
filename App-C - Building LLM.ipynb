{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccf97e0",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thedatasense/llm-healthcare/blob/main/App-C%20-%20Building%20LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837842c559df940",
   "metadata": {
    "id": "2837842c559df940"
   },
   "source": [
    "# Building LLM from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f055daed1bbd480",
   "metadata": {
    "id": "4f055daed1bbd480"
   },
   "source": [
    "This part is heavily inspired based on the work by Sebastian Raschka <br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "viKt9FYNR6V5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:54.472879Z",
     "start_time": "2025-01-23T04:53:53.073399Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viKt9FYNR6V5",
    "outputId": "622a82ba-f705-46c3-e001-436139f4eb2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/bsada1/miniconda3/envs/explorer/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/bsada1/miniconda3/envs/explorer/lib/python3.11/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/bsada1/miniconda3/envs/explorer/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/bsada1/miniconda3/envs/explorer/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bsada1/miniconda3/envs/explorer/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bsada1/miniconda3/envs/explorer/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bsada1/miniconda3/envs/explorer/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:54.481928Z",
     "start_time": "2025-01-23T04:53:54.477760Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "35c8cf19-92d5-4ce7-db8c-935ad140f3bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import os\n",
    "import urllib.request\n",
    "import importlib\n",
    "import tiktoken\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bf18535372096a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:54.498691Z",
     "start_time": "2025-01-23T04:53:54.496586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(1)  # Set GPU 1 as the active device\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd9986-1004-449f-bc83-88b358c67942",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is from a [Kaggle Medical Text dataset](https://www.kaggle.com/datasets/chaitanyakck/medical-text) dataset that is from abstracts written during routine rounds by doctors explaining the current conditions of the patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hjf87p6Tq1dg",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:58.510137Z",
     "start_time": "2025-01-23T04:53:54.514028Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hjf87p6Tq1dg",
    "outputId": "bbce503d-967a-4e86-a776-2faa104781c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition_label</th>\n",
       "      <th>medical_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Tissue changes around loose prostheses. A cani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Neuropeptide Y and neuron-specific enolase lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sexually transmitted diseases of the colon, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Lipolytic factors associated with murine and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Does carotid restenosis predict an increased r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   condition_label                                   medical_abstract\n",
       "0                5  Tissue changes around loose prostheses. A cani...\n",
       "1                1  Neuropeptide Y and neuron-specific enolase lev...\n",
       "2                2  Sexually transmitted diseases of the colon, re...\n",
       "3                1  Lipolytic factors associated with murine and h...\n",
       "4                3  Does carotid restenosis predict an increased r..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/sebischair/Medical-Abstracts-TC-Corpus/refs/heads/main/medical_tc_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "OB-1THNfqNeg",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:58.534562Z",
     "start_time": "2025-01-23T04:53:58.530495Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OB-1THNfqNeg",
    "outputId": "8258c727-fc0f-44e5-dbdf-bdb643ef8f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 14207702\n",
      "olated and maintained in culture for sufficient periods of time so that their biologic activity could be studied as well as the effect of different agents added to the cells in vivo or in vitro. The biologic response as determined by interleukin-1 and prostaglandin E2 activity paralleled the roentgenographic appearance of loosening and the technetium images and observations made at the time of rev\n"
     ]
    }
   ],
   "source": [
    "medical_abstracts = df['medical_abstract'].astype(str).tolist()  # Extract the column and convert to a list of strings\n",
    "text_data = ' '.join(medical_abstracts)  # Combine all abstracts into a single string\n",
    "# Display the total number of characters and a preview of the combined text\n",
    "print(\"Total number of characters:\", len(text_data))\n",
    "print(text_data[300:700])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0266df8f28e3565",
   "metadata": {
    "id": "f0266df8f28e3565"
   },
   "source": [
    "### GPT-2 Tokenization: Byte-Pair Encoding ###\n",
    "\n",
    "- GPT-2 used Byte-Pair Encoding  as its tokenizer.\n",
    "- It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words.\n",
    "- For instance, if GPT-2’s vocabulary doesn’t have the word *\"unfamiliarword\"*, it might tokenize it as `[\"unfam\", \"iliar\", \"word\"]` or some other subword breakdown, depending on its trained BPE merges.\n",
    "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a8b22655248c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:59.252458Z",
     "start_time": "2025-01-23T04:53:58.555121Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1a8b22655248c74",
    "outputId": "a26e24ae-11d7-4eb9-986d-1935dc7d4153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 3055877\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "enc_text = tokenizer.encode(text_data)\n",
    "print(\"Total number of character:\", len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3bf7629-b7a9-4c57-84bb-088dd9771d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:59.294471Z",
     "start_time": "2025-01-23T04:53:59.292376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 11407, 4442, 2212, 20784]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab09b328d0a4cd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:53:59.339984Z",
     "start_time": "2025-01-23T04:53:59.312354Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aab09b328d0a4cd4",
    "outputId": "2d0999f2-2066-41c3-fb20-c6650c410ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [6147, 1920, 13, 220, 4275, 4176, 47309, 816, 323, 49384]\n",
      "y:      [1920, 13, 220, 4275, 4176, 47309, 816, 323, 49384, 19440]\n",
      "ening ---->  process\n",
      "ening process ----> .\n",
      "ening process. ---->  \n",
      "ening process.  ---->  Ne\n",
      "ening process.  Ne ----> urope\n",
      "ening process.  Neurope ----> ptide\n",
      "ening process.  Neuropeptide ---->  Y\n",
      "ening process.  Neuropeptide Y ---->  and\n",
      "ening process.  Neuropeptide Y and ---->  neuron\n",
      "ening process.  Neuropeptide Y and neuron ----> -specific\n"
     ]
    }
   ],
   "source": [
    "context_size = 10\n",
    "enc_sample = enc_text[200:]\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "\n",
    "# what are we expecting the model to predict based on the context\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "951e579ffe61f859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:54:00.074018Z",
     "start_time": "2025-01-23T04:53:59.345430Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "951e579ffe61f859",
    "outputId": "53cae6ab-4705-45a5-b717-7f7ca9029401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 14207702\n",
      "Tokens: 3055877\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "#Characters:→ Total raw characters in our text data.\n",
    "print(\"Characters:\", total_characters)\n",
    "#Tokens:→ Number of units after applying the tokenizer (subwords, spaces, punctuation and so on)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57966b060ca1a145",
   "metadata": {
    "id": "57966b060ca1a145"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "102dde1e-eef2-4b66-8671-a1a0e43650f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:57:56.650055Z",
     "start_time": "2025-01-23T04:57:56.646808Z"
    }
   },
   "outputs": [],
   "source": [
    "class MedCorpusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_len,stride):\n",
    "        self.input_ids = []\n",
    "        self.output_ids = []\n",
    "        token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
    "        # lets use a sliding window to chunk the text in to overlapping sequences of maximum length\n",
    "        for i in range (0,len(token_ids) - max_len,stride):\n",
    "            input_chunk = token_ids[i:i+max_len]\n",
    "            target_chunk = token_ids[i+1 :i+max_len+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.output_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.output_ids[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f62b774790d6",
   "metadata": {
    "id": "7b2f62b774790d6"
   },
   "source": [
    "Here we will code the architecture of the smallest GPT-2 model (124 million parameters), as outlined in Radford et al.'s [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "- `\"vocab_size\"` indicates a vocabulary size of 50,257 words, supported by the BPE tokenizer\n",
    "- `\"context_length\"` represents the model's maximum input token count, as enabled by positional embeddings covered\n",
    "- `\"emb_dim\"` is the embedding size for token inputs, converting each input token into a 768-dimensional vector\n",
    "- `\"n_heads\"` is the number of attention heads in the multi-head attention mechanism implemented\n",
    "- `\"n_layers\"` is the number of transformer blocks within the model\n",
    "- `\"drop_rate\"` is the dropout mechanism's intensity,.1 means dropping 10% of hidden units during training to mitigate overfitting\n",
    "- `\"qkv_bias\"` decides if the `Linear` layers in the multi-head attention mechanism should include a bias vector when computing query (Q), key (K), and value (V) tensors;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "609f59fd3a54d7d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:57:58.900584Z",
     "start_time": "2025-01-23T04:57:58.898303Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_loader(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    dataset = MedCorpusDataset(txt,tokenizer,max_length,stride)\n",
    "    dataloader = DataLoader (\n",
    "        dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ae3fc732ce15b",
   "metadata": {
    "id": "cf5ae3fc732ce15b"
   },
   "source": [
    "### Transformer Architecture Setup\n",
    "\n",
    "*   **Multi-Head Attention** learns relationships between inputs by projecting them into **queries**, **keys**, and **values**.\n",
    "*   Input is split into multiple **heads**, each learning different attention patterns.\n",
    "*   **Scaled dot-product attention** calculates similarity between queries and keys, creating attention weights.\n",
    "*   A **causal mask** prevents the model from attending to future tokens in sequence generation.\n",
    "*   Outputs from all heads are concatenated and projected, providing a context-aware representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "650e854dd524c24a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:01.060807Z",
     "start_time": "2025-01-23T04:58:01.056501Z"
    },
    "id": "650e854dd524c24a"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Dimension of each attention head\n",
    "\n",
    "        # Linear projections for query, key, and value\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask to prevent attending to future tokens\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # Batch size, sequence length, input dimension\n",
    "\n",
    "        # Project the input to query, key, and value\n",
    "        keys = self.W_key(x)     # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x) # Shape: (b, num_tokens, d_out)\n",
    "        values = self.W_value(x) # Shape: (b, num_tokens, d_out)\n",
    "\n",
    "        # Split the projected vectors into multiple heads\n",
    "        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to prepare for matrix multiplication\n",
    "        # (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Calculate attention scores (scaled dot-product attention)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # Apply causal mask to attention scores\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # Truncate mask to current sequence length\n",
    "        attn_scores = attn_scores.masked_fill(mask_bool, float('-inf')) # Mask out future tokens\n",
    "\n",
    "        # Normalize attention scores with softmax\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) # Scale by sqrt(head_dim)\n",
    "\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Calculate the weighted sum of values\n",
    "        # (b, num_heads, num_tokens, num_tokens) @ (b, num_heads, num_tokens, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        # Concatenate the outputs from all heads\n",
    "        # (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "\n",
    "        # Reshape to combine heads: (b, num_tokens, num_heads, head_dim) -> (b, num_tokens, d_out)\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "\n",
    "        # Apply final linear projection\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb68de22e24c2b",
   "metadata": {},
   "source": [
    "### GELU Activation Function\n",
    "\n",
    "The **Gaussian Error Linear Unit (GELU)** is an activation function defined as:\n",
    "\n",
    "$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$\n",
    "\n",
    "where \\(\\Phi(x)\\) is the cumulative distribution function (CDF) of the standard normal distribution:\n",
    "\n",
    "$\n",
    "\\Phi(x) = 0.5 \\cdot \\left(1 + \\text{tanh}\\left(\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right)\\right)\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232f12f463a33a44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:04.145656Z",
     "start_time": "2025-01-23T04:58:04.143688Z"
    },
    "id": "232f12f463a33a44"
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c19144d9d7e134",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:05.270372Z",
     "start_time": "2025-01-23T04:58:05.267996Z"
    },
    "id": "68c19144d9d7e134"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfa2ad07917f340a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:08.347572Z",
     "start_time": "2025-01-23T04:58:08.345445Z"
    },
    "id": "dfa2ad07917f340a"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61852b0c8dd5a3f5",
   "metadata": {
    "id": "61852b0c8dd5a3f5"
   },
   "source": [
    "### Transformer Block\n",
    "\n",
    "#### 1. Multi-Head Attention (`self.att`)\n",
    "\n",
    "*   **Purpose:** This is where the model learns to \"attend\" to different parts of the input sequence. It calculates relationships between words (or tokens) in the sequence, regardless of their distance. This is crucial for understanding context and long-range dependencies.\n",
    "*   **How it works:**\n",
    "    *   The input `x` is projected into three different representations: **queries (Q)**, **keys (K)**, and **values (V)**.\n",
    "    *   The model calculates **attention scores** by taking the dot product of queries and keys, scaled down by the square root of the key dimension.\n",
    "    *   These scores are passed through a softmax to get **attention weights**, representing the importance of each word to every other word.\n",
    "    *   The attention weights are used to compute a weighted sum of the values, producing the **context vector**.\n",
    "    *   This process is done multiple times in parallel with different learned projections (**multiple heads**), allowing the model to capture diverse relationships.\n",
    "*   **Why it's important:** Unlike recurrent models (like RNNs), self-attention can process the entire sequence in parallel, making it much faster. It also directly models relationships between any two words, regardless of their position, which helps with long-range dependencies.\n",
    "\n",
    "#### 2. Feed-Forward (`self.ff`)\n",
    "\n",
    "*   **Purpose:** A simple, fully connected feed-forward network that processes each token's representation independently. This adds non-linearity and further transforms the information learned by the attention mechanism.\n",
    "*   **How it works:** Typically, it consists of two linear layers with a non-linear activation function (like GELU or ReLU) in between.\n",
    "*   **Why it's important:** It provides additional processing power and allows the model to learn more complex patterns from the attention outputs.\n",
    "\n",
    "#### 3. LayerNorm (`self.norm1`, `self.norm2`)\n",
    "\n",
    "*   **Purpose:** Layer normalization normalizes the activations across the feature dimension (embedding dimension). It helps stabilize training and can improve performance.\n",
    "*   **How it works:** It subtracts the mean and divides by the standard deviation of the activations within each individual embedding vector (across the embedding dimension).\n",
    "*   **Why it's important:** It prevents the activations from becoming too large or too small, which can lead to unstable gradients during training. It also makes the model less sensitive to the scale of the input features.\n",
    "\n",
    "#### 4. Dropout (`self.drop_shortcut`)\n",
    "\n",
    "*   **Purpose:** A regularization technique that randomly sets a fraction of the activations to zero during training. This helps prevent overfitting.\n",
    "*   **How it works:** During each forward pass, it randomly \"drops out\" (sets to zero) a certain percentage of the activations, determined by the `drop_rate`.\n",
    "*   **Why it's important:** It forces the model to learn more robust features that are not overly reliant on any single activation.\n",
    "\n",
    "#### 5. Residual Connections (Shortcut Connections)\n",
    "\n",
    "*   **Purpose:** The output of each sub-block (MHA and FFN) is added to its original input (`x = x + shortcut`). This helps with the flow of gradients during training, especially in deep networks.\n",
    "*   **How it works:** The original input `x` is added to the output of the sub-block before being passed to the next layer.\n",
    "*   **Why it's important:**\n",
    "    *   **Vanishing Gradients:** In very deep networks, gradients can become very small as they are backpropagated through many layers, making training difficult. Residual connections provide a \"shortcut\" for the gradients to flow back, mitigating this problem.\n",
    "    *   **Identity Mapping:** They make it easier for the network to learn the identity function (where the output is the same as the input). This is because if a layer is not needed, the network can easily learn to set its weights to zero, effectively making it pass through the input unchanged.\n",
    "\n",
    "#### `forward(self, x)` Method:\n",
    "\n",
    "The `forward` method defines how the input `x` is processed through the `TransformerBlock`:\n",
    "\n",
    "1.  **Attention Block:**\n",
    "    *   Store the original input for the residual connection.\n",
    "    *   Apply layer normalization.\n",
    "    *   Pass through the multi-head attention mechanism.\n",
    "    *   Apply dropout to the output of the attention block\n",
    "    *   Add the original input back (residual connection).\n",
    "\n",
    "2.  **Feed-Forward Block:**\n",
    "    *   Store the output from the attention block for the residual connection.\n",
    "    *   Apply layer normalization.\n",
    "    *   Pass through the feed-forward network.\n",
    "    *   Apply dropout to the output of the feed-forward block.\n",
    "    *   Add the output from the attention block back (residual connection).\n",
    "\n",
    "3.  Return the processed output.\n",
    "\n",
    "**In essence, the `TransformerBlock` takes an input sequence, allows it to attend to itself, processes it further with a feed-forward network, and uses layer normalization and residual connections to improve training and performance. Multiple `TransformerBlock` instances are stacked together to create the full Transformer model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b626139fb43755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:19.248400Z",
     "start_time": "2025-01-23T04:58:19.243601Z"
    },
    "id": "a9b626139fb43755"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943c3193786f7f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:21.396953Z",
     "start_time": "2025-01-23T04:58:21.394075Z"
    },
    "id": "943c3193786f7f3"
   },
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e01cc589eabd27a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:29.402806Z",
     "start_time": "2025-01-23T04:58:28.750543Z"
    },
    "id": "4e01cc589eabd27a"
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 512, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "184a40a60cf9e62e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:58:32.062938Z",
     "start_time": "2025-01-23T04:58:30.813518Z"
    },
    "id": "184a40a60cf9e62e"
   },
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "train_loader = data_loader(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = data_loader(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c03aa1963323d6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:20.530198Z",
     "start_time": "2025-01-23T05:26:20.526822Z"
    },
    "id": "3c03aa1963323d6e"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, idx, max_new_tokens, context_size, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text token-by-token using an autoregressive language model.\n",
    "    \"\"\"\n",
    "    model.to\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Use only the most recent context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)[:, -1, :]  # Get logits for the last token\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits /= temperature\n",
    "\n",
    "        # Convert logits to probabilities and sample the next token\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)  # Sample from the probability distribution\n",
    "\n",
    "        # Append the sampled token to the sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14bcebed163905c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:24.148716Z",
     "start_time": "2025-01-23T05:26:23.681805Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14bcebed163905c9",
    "outputId": "1f98cef2-7668-4045-ef3a-50f603a3d158"
   },
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"It is recommended to take rest when\"\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_ids = generate_text(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fc0476788ab9a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:26.378915Z",
     "start_time": "2025-01-23T05:26:26.376795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained Model Says This :\n",
      " senatorombie validatorLeg rest when.price에inventoryFlightXXXX\n"
     ]
    }
   ],
   "source": [
    "print(\"Untrained Model Says This :\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9310227a8309847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:35.726880Z",
     "start_time": "2025-01-23T05:26:35.724441Z"
    },
    "id": "c9310227a8309847"
   },
   "outputs": [],
   "source": [
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d6ef9137ee0029e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:26:40.894821Z",
     "start_time": "2025-01-23T05:26:40.840038Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d6ef9137ee0029e",
    "outputId": "f15d0801-c520-460a-d9f9-b5d1fc47a53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 2750464\n",
      "Validation tokens: 305152\n",
      "All tokens: 3055616\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25409a8ea980f371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:34:04.475869Z",
     "start_time": "2025-01-23T05:34:04.472955Z"
    },
    "id": "25409a8ea980f371"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"\n",
    "     Computes the loss for a single batch of input and target data.\n",
    "    :param input_batch:\n",
    "    :param target_batch:\n",
    "    :param model:\n",
    "    :param device:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model.to(device) \n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3ac1d459789e667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:34:04.435324Z",
     "start_time": "2025-01-23T05:27:49.141015Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3ac1d459789e667",
    "outputId": "51e93e7f-edc0-404a-bf1d-3683f0843527"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# Disable gradient tracking for efficiency because we are not training, yet\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m calc_loss_loader(val_loader, model, device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss)\n",
      "Cell \u001b[0;32mIn[27], line 29\u001b[0m, in \u001b[0;36mcalc_loss_loader\u001b[0;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_batch, target_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m num_batches:\n\u001b[0;32m---> 29\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m, in \u001b[0;36mcalc_loss_batch\u001b[0;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m Computes the loss for a single batch of input and target data.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m:param input_batch:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m---> 11\u001b[0m input_batch, target_batch \u001b[38;5;241m=\u001b[39m \u001b[43minput_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_batch)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), target_batch\u001b[38;5;241m.\u001b[39mflatten())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "#torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c925b397ad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:35:58.942822Z",
     "start_time": "2025-01-23T05:35:58.784471Z"
    },
    "id": "3d8c925b397ad7"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390565cd43f2e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:24:40.339600Z",
     "start_time": "2025-01-09T05:22:11.905227Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6390565cd43f2e1c",
    "outputId": "ba93b3a8-74f2-4c63-be04-1ba9a0a3408c"
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 7\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"patient has high\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12468471-e84b-41d6-acf8-c7dbc5cf85ed",
   "metadata": {
    "id": "12468471-e84b-41d6-acf8-c7dbc5cf85ed"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"./models/basic_llm_med.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3012207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:24:52.072636Z",
     "start_time": "2025-01-09T06:24:49.827535Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "f3012207",
    "outputId": "8d776296-3626-4526-8a06-33ba04bc3c1b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ee74c13102b9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T21:10:46.111146Z",
     "start_time": "2025-01-22T21:10:46.078437Z"
    },
    "id": "242ee74c13102b9a",
    "outputId": "c4a458c5-9c46-44f3-c439-51b4861b9bbb"
   },
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"The effect of smoking\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b1bd2e8f8f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:26:45.549385Z",
     "start_time": "2025-01-09T06:26:45.542159Z"
    },
    "id": "8e2b1bd2e8f8f2b"
   },
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67a93815df682",
   "metadata": {
    "id": "6b67a93815df682"
   },
   "source": [
    "**Top-k and Temperature**\n",
    "\n",
    " **Top-k** limits the model's choice to the *k* most probable next words, discarding less likely options. **Temperature** scales the probability distribution; higher values increase randomness by making the distribution more uniform, while lower values favor the most likely word by making it more peaked.  For instance, when creating a poem, high temperature with low Top-k might yield unusual word choices. Conversely, low temperature with high Top-k would produce a more predictable, coherent verse.  In essence, they dictate how \"focused\" versus \"creative\" the generated text becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bead5dd339a179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:26:48.861257Z",
     "start_time": "2025-01-09T06:26:48.248391Z"
    },
    "id": "82bead5dd339a179",
    "outputId": "6f5c1249-037f-4449-cf61-9c629fadf4bd"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4e6a685d321b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:26:56.639638Z",
     "start_time": "2025-01-09T06:26:56.027106Z"
    },
    "id": "20e4e6a685d321b9"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "explorer",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
