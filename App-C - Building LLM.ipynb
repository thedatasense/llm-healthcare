{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedatasense/llm-healthcare/blob/main/App-C%20-%20Building%20LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2837842c559df940"
      },
      "cell_type": "markdown",
      "source": [
        "# Building LLM from Scratch"
      ],
      "id": "2837842c559df940"
    },
    {
      "cell_type": "markdown",
      "id": "4f055daed1bbd480",
      "metadata": {
        "id": "4f055daed1bbd480"
      },
      "source": [
        "This part is heavily inspired based on the work by Sebastian Raschka\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "viKt9FYNR6V5",
        "outputId": "679594db-d91f-4cf9-e570-dd60fda10d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "viKt9FYNR6V5",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:15.899097Z",
          "start_time": "2025-01-09T05:17:15.888657Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "fafc0ef1-75bc-4654-e18f-d2406397903e"
      },
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "import os\n",
        "import urllib.request\n",
        "import importlib\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "id": "initial_id",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.5.1+cu121\n",
            "tiktoken version: 0.8.0\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "id": "9dd938f87ccd368e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:19.201380Z",
          "start_time": "2025-01-09T05:17:19.138027Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dd938f87ccd368e",
        "outputId": "e93b216f-21d2-4663-9771-03db63405d49"
      },
      "source": [
        "# We will train our model on The Project Gutenberg eBook of The Complete Works of William Shakespeare\n",
        "# You may copy it, give it away or re-use it under the terms\n",
        "# of the Project Gutenberg License included with this ebook or online\n",
        "# at www.gutenberg.org\n",
        "\n",
        "if not os.path.exists(\"pg100.txt\"):\n",
        "    url = (\"https://www.gutenberg.org/cache/epub/100/pg100.txt\")\n",
        "    file_path = \"pg100.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"pg100.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "\n",
        "print(\"Total number of character:\", len(text_data))\n",
        "print(text_data[:99])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 5378663\n",
            "﻿The Project Gutenberg eBook of The Complete Works of William Shakespeare\n",
            "    \n",
            "This ebook is for th\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "id": "f0266df8f28e3565",
      "metadata": {
        "id": "f0266df8f28e3565"
      },
      "source": [
        "**GPT-2 Tokenization: Byte-Pair Encoding **\n",
        "\n",
        "- GPT-2 used Byte-Pair Encoding  as its tokenizer.\n",
        "- It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words.\n",
        "- For instance, if GPT-2’s vocabulary doesn’t have the word *\"unfamiliarword\"*, it might tokenize it as `[\"unfam\", \"iliar\", \"word\"]` or some other subword breakdown, depending on its trained BPE merges.\n",
        "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "c1a8b22655248c74",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:22.442360Z",
          "start_time": "2025-01-09T05:17:21.724773Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1a8b22655248c74",
        "outputId": "97fc1867-4b62-4656-83a5-56ec9a52b259"
      },
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "enc_text = tokenizer.encode(text_data)\n",
        "print(\"Total number of character:\", len(enc_text))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 1691007\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "id": "aab09b328d0a4cd4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:24.116230Z",
          "start_time": "2025-01-09T05:17:24.064504Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aab09b328d0a4cd4",
        "outputId": "6732d081-6fc2-44b3-8bcf-c898a57d31cd"
      },
      "source": [
        "context_size = 10\n",
        "enc_sample = enc_text[200:]\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [30936, 50, 3963, 17682, 40, 2390, 6006, 10206, 1546, 11401]\n",
            "y:      [50, 3963, 17682, 40, 2390, 6006, 10206, 1546, 11401, 12203]\n",
            " WORK ----> S\n",
            " WORKS ---->  OF\n",
            " WORKS OF ---->  WILL\n",
            " WORKS OF WILL ----> I\n",
            " WORKS OF WILLI ----> AM\n",
            " WORKS OF WILLIAM ---->  SH\n",
            " WORKS OF WILLIAM SH ----> AK\n",
            " WORKS OF WILLIAM SHAK ----> ES\n",
            " WORKS OF WILLIAM SHAKES ----> PE\n",
            " WORKS OF WILLIAM SHAKESPE ----> ARE\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "id": "951e579ffe61f859",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:27.607057Z",
          "start_time": "2025-01-09T05:17:26.895883Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "951e579ffe61f859",
        "outputId": "ecdbac3b-b6e2-44d6-c30f-1fd38d5c5b4e"
      },
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "#Characters:→ Total raw characters in our text data.\n",
        "print(\"Characters:\", total_characters)\n",
        "#Tokens:→ Number of units after applying the tokenizer (subwords, spaces, punctuation and so on)\n",
        "print(\"Tokens:\", total_tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 5378663\n",
            "Tokens: 1691007\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "id": "57966b060ca1a145",
      "metadata": {
        "id": "57966b060ca1a145"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "id": "9a5b060d6ec492bd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:30.763283Z",
          "start_time": "2025-01-09T05:17:30.756995Z"
        },
        "id": "9a5b060d6ec492bd"
      },
      "source": [
        "class LLMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,txt,tokenizer,max_len,stride):\n",
        "        self.input_ids = []\n",
        "        self.output_ids = []\n",
        "        # Lets tokenize the entire input text\n",
        "        token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # we use a sliding windows to chuck the text in to overlapping sequences of max lenght\n",
        "        for i in range(0,len(token_ids) - max_len,stride):\n",
        "            input_chunk = token_ids[i:i+max_len]\n",
        "            target_chunk = token_ids[i+ 1 : i+max_len +1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.output_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.output_ids[idx]"
      ],
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "id": "7b2f62b774790d6",
      "metadata": {
        "id": "7b2f62b774790d6"
      },
      "source": [
        "Here we will code the architecture of the smallest GPT-2 model (124 million parameters), as outlined in Radford et al.'s [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "- `\"vocab_size\"` indicates a vocabulary size of 50,257 words, supported by the BPE tokenizer\n",
        "- `\"context_length\"` represents the model's maximum input token count, as enabled by positional embeddings covered\n",
        "- `\"emb_dim\"` is the embedding size for token inputs, converting each input token into a 768-dimensional vector\n",
        "- `\"n_heads\"` is the number of attention heads in the multi-head attention mechanism implemented\n",
        "- `\"n_layers\"` is the number of transformer blocks within the model\n",
        "- `\"drop_rate\"` is the dropout mechanism's intensity,.1 means dropping 10% of hidden units during training to mitigate overfitting\n",
        "- `\"qkv_bias\"` decides if the `Linear` layers in the multi-head attention mechanism should include a bias vector when computing query (Q), key (K), and value (V) tensors;"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:33.241419Z",
          "start_time": "2025-01-09T05:17:33.235442Z"
        },
        "id": "99785b33d78297a3"
      },
      "cell_type": "code",
      "source": [
        "def create_dataloader(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = LLMDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ],
      "id": "99785b33d78297a3",
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "cf5ae3fc732ce15b"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformer Architecture Setup\n",
        "\n",
        "*   **Multi-Head Attention** learns relationships between inputs by projecting them into **queries**, **keys**, and **values**.\n",
        "*   Input is split into multiple **heads**, each learning different attention patterns.\n",
        "*   **Scaled dot-product attention** calculates similarity between queries and keys, creating attention weights.\n",
        "*   A **causal mask** prevents the model from attending to future tokens in sequence generation.\n",
        "*   Outputs from all heads are concatenated and projected, providing a context-aware representation.\n"
      ],
      "id": "cf5ae3fc732ce15b"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:35.433173Z",
          "start_time": "2025-01-09T05:17:35.417311Z"
        },
        "id": "650e854dd524c24a"
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Dimension of each attention head\n",
        "\n",
        "        # Linear projections for query, key, and value\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Causal mask to prevent attending to future tokens\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape  # Batch size, sequence length, input dimension\n",
        "\n",
        "        # Project the input to query, key, and value\n",
        "        keys = self.W_key(x)     # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x) # Shape: (b, num_tokens, d_out)\n",
        "        values = self.W_value(x) # Shape: (b, num_tokens, d_out)\n",
        "\n",
        "        # Split the projected vectors into multiple heads\n",
        "        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose to prepare for matrix multiplication\n",
        "        # (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Calculate attention scores (scaled dot-product attention)\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Shape: (b, num_heads, num_tokens, num_tokens)\n",
        "\n",
        "        # Apply causal mask to attention scores\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # Truncate mask to current sequence length\n",
        "        attn_scores = attn_scores.masked_fill(mask_bool, float('-inf')) # Mask out future tokens\n",
        "\n",
        "        # Normalize attention scores with softmax\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) # Scale by sqrt(head_dim)\n",
        "\n",
        "        # Apply dropout to attention weights\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Calculate the weighted sum of values\n",
        "        # (b, num_heads, num_tokens, num_tokens) @ (b, num_heads, num_tokens, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        context_vec = attn_weights @ values\n",
        "\n",
        "        # Concatenate the outputs from all heads\n",
        "        # (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = context_vec.transpose(1, 2)\n",
        "\n",
        "        # Reshape to combine heads: (b, num_tokens, num_heads, head_dim) -> (b, num_tokens, d_out)\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "\n",
        "        # Apply final linear projection\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ],
      "id": "650e854dd524c24a",
      "outputs": [],
      "execution_count": 9
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:38.609893Z",
          "start_time": "2025-01-09T05:17:38.603311Z"
        },
        "id": "232f12f463a33a44"
      },
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the Gaussian Error Linear Unit (GELU) activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Applies the GELU activation function element-wise.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying GELU.\n",
        "        \"\"\"\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n"
      ],
      "id": "232f12f463a33a44",
      "outputs": [],
      "execution_count": 10
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:40.529056Z",
          "start_time": "2025-01-09T05:17:40.521953Z"
        },
        "id": "68c19144d9d7e134"
      },
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ],
      "id": "68c19144d9d7e134",
      "outputs": [],
      "execution_count": 11
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:17:42.463549Z",
          "start_time": "2025-01-09T05:17:42.457588Z"
        },
        "id": "dfa2ad07917f340a"
      },
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "id": "dfa2ad07917f340a",
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "61852b0c8dd5a3f5"
      },
      "cell_type": "markdown",
      "source": [
        "### Components of Transformer Block\n",
        "\n",
        "#### 1. Multi-Head Attention (`self.att`)\n",
        "\n",
        "*   **Purpose:** This is where the model learns to \"attend\" to different parts of the input sequence. It calculates relationships between words (or tokens) in the sequence, regardless of their distance. This is crucial for understanding context and long-range dependencies.\n",
        "*   **How it works:**\n",
        "    *   The input `x` is projected into three different representations: **queries (Q)**, **keys (K)**, and **values (V)**.\n",
        "    *   The model calculates **attention scores** by taking the dot product of queries and keys, scaled down by the square root of the key dimension.\n",
        "    *   These scores are passed through a softmax to get **attention weights**, representing the importance of each word to every other word.\n",
        "    *   The attention weights are used to compute a weighted sum of the values, producing the **context vector**.\n",
        "    *   This process is done multiple times in parallel with different learned projections (**multiple heads**), allowing the model to capture diverse relationships.\n",
        "*   **Why it's important:** Unlike recurrent models (like RNNs), self-attention can process the entire sequence in parallel, making it much faster. It also directly models relationships between any two words, regardless of their position, which helps with long-range dependencies.\n",
        "\n",
        "#### 2. Feed-Forward (`self.ff`)\n",
        "\n",
        "*   **Purpose:** A simple, fully connected feed-forward network that processes each token's representation independently. This adds non-linearity and further transforms the information learned by the attention mechanism.\n",
        "*   **How it works:** Typically, it consists of two linear layers with a non-linear activation function (like GELU or ReLU) in between.\n",
        "*   **Why it's important:** It provides additional processing power and allows the model to learn more complex patterns from the attention outputs.\n",
        "\n",
        "#### 3. LayerNorm (`self.norm1`, `self.norm2`)\n",
        "\n",
        "*   **Purpose:** Layer normalization normalizes the activations across the feature dimension (embedding dimension). It helps stabilize training and can improve performance.\n",
        "*   **How it works:** It subtracts the mean and divides by the standard deviation of the activations within each individual embedding vector (across the embedding dimension).\n",
        "*   **Why it's important:** It prevents the activations from becoming too large or too small, which can lead to unstable gradients during training. It also makes the model less sensitive to the scale of the input features.\n",
        "\n",
        "#### 4. Dropout (`self.drop_shortcut`)\n",
        "\n",
        "*   **Purpose:** A regularization technique that randomly sets a fraction of the activations to zero during training. This helps prevent overfitting.\n",
        "*   **How it works:** During each forward pass, it randomly \"drops out\" (sets to zero) a certain percentage of the activations, determined by the `drop_rate`.\n",
        "*   **Why it's important:** It forces the model to learn more robust features that are not overly reliant on any single activation.\n",
        "\n",
        "#### 5. Residual Connections (Shortcut Connections)\n",
        "\n",
        "*   **Purpose:** The output of each sub-block (MHA and FFN) is added to its original input (`x = x + shortcut`). This helps with the flow of gradients during training, especially in deep networks.\n",
        "*   **How it works:** The original input `x` is added to the output of the sub-block before being passed to the next layer.\n",
        "*   **Why it's important:**\n",
        "    *   **Vanishing Gradients:** In very deep networks, gradients can become very small as they are backpropagated through many layers, making training difficult. Residual connections provide a \"shortcut\" for the gradients to flow back, mitigating this problem.\n",
        "    *   **Identity Mapping:** They make it easier for the network to learn the identity function (where the output is the same as the input). This is because if a layer is not needed, the network can easily learn to set its weights to zero, effectively making it pass through the input unchanged.\n",
        "\n",
        "#### `forward(self, x)` Method:\n",
        "\n",
        "The `forward` method defines how the input `x` is processed through the `TransformerBlock`:\n",
        "\n",
        "1.  **Attention Block:**\n",
        "    *   Store the original input for the residual connection.\n",
        "    *   Apply layer normalization.\n",
        "    *   Pass through the multi-head attention mechanism.\n",
        "    *   Apply dropout to the output of the attention block\n",
        "    *   Add the original input back (residual connection).\n",
        "\n",
        "2.  **Feed-Forward Block:**\n",
        "    *   Store the output from the attention block for the residual connection.\n",
        "    *   Apply layer normalization.\n",
        "    *   Pass through the feed-forward network.\n",
        "    *   Apply dropout to the output of the feed-forward block.\n",
        "    *   Add the output from the attention block back (residual connection).\n",
        "\n",
        "3.  Return the processed output.\n",
        "\n",
        "**In essence, the `TransformerBlock` takes an input sequence, allows it to attend to itself, processes it further with a feed-forward network, and uses layer normalization and residual connections to improve training and performance. Multiple `TransformerBlock` instances are stacked together to create the full Transformer model.**\n"
      ],
      "id": "61852b0c8dd5a3f5"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:18:44.835055Z",
          "start_time": "2025-01-09T05:18:44.825624Z"
        },
        "id": "a9b626139fb43755"
      },
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "id": "a9b626139fb43755",
      "outputs": [],
      "execution_count": 13
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:18:47.013283Z",
          "start_time": "2025-01-09T05:18:47.003819Z"
        },
        "id": "943c3193786f7f3"
      },
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "id": "943c3193786f7f3",
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "id": "4e01cc589eabd27a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:18:50.543117Z",
          "start_time": "2025-01-09T05:18:49.397854Z"
        },
        "id": "4e01cc589eabd27a"
      },
      "source": [
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();  # Disable dropout during inference"
      ],
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "id": "184a40a60cf9e62e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:18:52.253763Z",
          "start_time": "2025-01-09T05:18:51.006215Z"
        },
        "id": "184a40a60cf9e62e"
      },
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 16
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:19:32.062595Z",
          "start_time": "2025-01-09T05:19:32.055582Z"
        },
        "id": "3c03aa1963323d6e"
      },
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "id": "3c03aa1963323d6e",
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "id": "14bcebed163905c9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:19:40.601328Z",
          "start_time": "2025-01-09T05:19:40.308098Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14bcebed163905c9",
        "outputId": "1f98cef2-7668-4045-ef3a-50f603a3d158"
      },
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "id": "c9310227a8309847",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:19:50.922299Z",
          "start_time": "2025-01-09T05:19:50.917082Z"
        },
        "id": "c9310227a8309847"
      },
      "source": [
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")"
      ],
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "id": "b612d183410f3647",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:19:54.199852Z",
          "start_time": "2025-01-09T05:19:53.956903Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b612d183410f3647",
        "outputId": "df82c619-7f2f-4a79-911c-0b4b8d45fa81"
      },
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "id": "5d6ef9137ee0029e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:19:58.922395Z",
          "start_time": "2025-01-09T05:19:58.748104Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d6ef9137ee0029e",
        "outputId": "f15d0801-c520-460a-d9f9-b5d1fc47a53a"
      },
      "source": [
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "print(\"Training tokens:\", train_tokens)\n",
        "print(\"Validation tokens:\", val_tokens)\n",
        "print(\"All tokens:\", train_tokens + val_tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokens: 1524224\n",
            "Validation tokens: 166400\n",
            "All tokens: 1690624\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "id": "25409a8ea980f371",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:20:09.112761Z",
          "start_time": "2025-01-09T05:20:09.103694Z"
        },
        "id": "25409a8ea980f371"
      },
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "id": "a3ac1d459789e667",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:21:18.985999Z",
          "start_time": "2025-01-09T05:20:13.552684Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ac1d459789e667",
        "outputId": "51e93e7f-edc0-404a-bf1d-3683f0843527"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "   device = torch.device(\"mps\")\n",
        "else:\n",
        "   device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device.\n",
            "Training loss: 10.969000630223354\n",
            "Validation loss: 10.97602192218487\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "id": "3d8c925b397ad7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T05:21:59.984444Z",
          "start_time": "2025-01-09T05:21:59.969150Z"
        },
        "id": "3d8c925b397ad7"
      },
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "6390565cd43f2e1c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T06:24:40.339600Z",
          "start_time": "2025-01-09T05:22:11.905227Z"
        },
        "id": "6390565cd43f2e1c",
        "outputId": "39422e5d-57d4-40db-9caf-386576cc8a4c"
      },
      "source": [
        "# Note:\n",
        "# Uncomment the following code to calculate the execution time\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.297, Val loss 9.448\n",
            "Ep 1 (Step 000005): Train loss 8.177, Val loss 8.235\n",
            "Ep 1 (Step 000010): Train loss 7.152, Val loss 7.315\n",
            "Ep 1 (Step 000015): Train loss 6.871, Val loss 6.857\n",
            "Ep 1 (Step 000020): Train loss 6.611, Val loss 6.699\n",
            "Ep 1 (Step 000025): Train loss 6.721, Val loss 6.562\n",
            "Ep 1 (Step 000030): Train loss 6.825, Val loss 6.461\n",
            "Ep 1 (Step 000035): Train loss 6.461, Val loss 6.361\n",
            "Ep 1 (Step 000040): Train loss 6.462, Val loss 6.296\n",
            "Ep 1 (Step 000045): Train loss 6.136, Val loss 6.242\n",
            "Ep 1 (Step 000050): Train loss 6.175, Val loss 6.276\n",
            "Ep 1 (Step 000055): Train loss 6.142, Val loss 6.185\n",
            "Ep 1 (Step 000060): Train loss 6.056, Val loss 6.164\n",
            "Ep 1 (Step 000065): Train loss 6.088, Val loss 6.138\n",
            "Ep 1 (Step 000070): Train loss 6.199, Val loss 6.133\n",
            "Ep 1 (Step 000075): Train loss 6.176, Val loss 6.094\n",
            "Ep 1 (Step 000080): Train loss 5.998, Val loss 6.107\n",
            "Ep 1 (Step 000085): Train loss 6.379, Val loss 6.058\n",
            "Ep 1 (Step 000090): Train loss 6.256, Val loss 6.010\n",
            "Ep 1 (Step 000095): Train loss 5.929, Val loss 6.065\n",
            "Ep 1 (Step 000100): Train loss 5.873, Val loss 6.033\n",
            "Ep 1 (Step 000105): Train loss 6.071, Val loss 6.031\n",
            "Ep 1 (Step 000110): Train loss 6.098, Val loss 6.053\n",
            "Ep 1 (Step 000115): Train loss 6.030, Val loss 5.989\n",
            "Ep 1 (Step 000120): Train loss 5.665, Val loss 5.955\n",
            "Ep 1 (Step 000125): Train loss 5.872, Val loss 5.920\n",
            "Ep 1 (Step 000130): Train loss 5.740, Val loss 5.942\n",
            "Ep 1 (Step 000135): Train loss 5.903, Val loss 5.914\n",
            "Ep 1 (Step 000140): Train loss 5.750, Val loss 5.950\n",
            "Ep 1 (Step 000145): Train loss 5.826, Val loss 5.884\n",
            "Ep 1 (Step 000150): Train loss 5.664, Val loss 5.872\n",
            "Ep 1 (Step 000155): Train loss 5.829, Val loss 5.843\n",
            "Ep 1 (Step 000160): Train loss 5.881, Val loss 5.868\n",
            "Ep 1 (Step 000165): Train loss 5.837, Val loss 5.811\n",
            "Ep 1 (Step 000170): Train loss 5.554, Val loss 5.821\n",
            "Ep 1 (Step 000175): Train loss 5.760, Val loss 5.824\n",
            "Ep 1 (Step 000180): Train loss 5.873, Val loss 5.835\n",
            "Ep 1 (Step 000185): Train loss 6.007, Val loss 5.785\n",
            "Ep 1 (Step 000190): Train loss 5.966, Val loss 5.755\n",
            "Ep 1 (Step 000195): Train loss 5.680, Val loss 5.731\n",
            "Ep 1 (Step 000200): Train loss 5.871, Val loss 5.770\n",
            "Ep 1 (Step 000205): Train loss 5.708, Val loss 5.712\n",
            "Ep 1 (Step 000210): Train loss 5.575, Val loss 5.698\n",
            "Ep 1 (Step 000215): Train loss 5.724, Val loss 5.680\n",
            "Ep 1 (Step 000220): Train loss 5.366, Val loss 5.693\n",
            "Ep 1 (Step 000225): Train loss 5.548, Val loss 5.693\n",
            "Ep 1 (Step 000230): Train loss 5.595, Val loss 5.700\n",
            "Ep 1 (Step 000235): Train loss 5.611, Val loss 5.648\n",
            "Ep 1 (Step 000240): Train loss 5.782, Val loss 5.647\n",
            "Ep 1 (Step 000245): Train loss 5.449, Val loss 5.603\n",
            "Ep 1 (Step 000250): Train loss 5.511, Val loss 5.608\n",
            "Ep 1 (Step 000255): Train loss 5.612, Val loss 5.599\n",
            "Ep 1 (Step 000260): Train loss 5.846, Val loss 5.558\n",
            "Ep 1 (Step 000265): Train loss 5.760, Val loss 5.567\n",
            "Ep 1 (Step 000270): Train loss 5.660, Val loss 5.553\n",
            "Ep 1 (Step 000275): Train loss 5.682, Val loss 5.525\n",
            "Ep 1 (Step 000280): Train loss 5.500, Val loss 5.540\n",
            "Ep 1 (Step 000285): Train loss 5.735, Val loss 5.547\n",
            "Ep 1 (Step 000290): Train loss 5.651, Val loss 5.576\n",
            "Ep 1 (Step 000295): Train loss 5.914, Val loss 5.509\n",
            "Ep 1 (Step 000300): Train loss 5.573, Val loss 5.485\n",
            "Ep 1 (Step 000305): Train loss 5.385, Val loss 5.517\n",
            "Ep 1 (Step 000310): Train loss 5.570, Val loss 5.514\n",
            "Ep 1 (Step 000315): Train loss 5.494, Val loss 5.484\n",
            "Ep 1 (Step 000320): Train loss 5.442, Val loss 5.514\n",
            "Ep 1 (Step 000325): Train loss 5.568, Val loss 5.509\n",
            "Ep 1 (Step 000330): Train loss 5.498, Val loss 5.519\n",
            "Ep 1 (Step 000335): Train loss 5.640, Val loss 5.507\n",
            "Ep 1 (Step 000340): Train loss 5.426, Val loss 5.464\n",
            "Ep 1 (Step 000345): Train loss 5.490, Val loss 5.456\n",
            "Ep 1 (Step 000350): Train loss 5.694, Val loss 5.451\n",
            "Ep 1 (Step 000355): Train loss 5.367, Val loss 5.465\n",
            "Ep 1 (Step 000360): Train loss 5.495, Val loss 5.448\n",
            "Ep 1 (Step 000365): Train loss 5.475, Val loss 5.461\n",
            "Ep 1 (Step 000370): Train loss 5.400, Val loss 5.469\n",
            "Ep 1 (Step 000375): Train loss 5.294, Val loss 5.469\n",
            "Ep 1 (Step 000380): Train loss 5.238, Val loss 5.449\n",
            "Ep 1 (Step 000385): Train loss 5.515, Val loss 5.440\n",
            "Ep 1 (Step 000390): Train loss 5.230, Val loss 5.428\n",
            "Ep 1 (Step 000395): Train loss 5.575, Val loss 5.383\n",
            "Ep 1 (Step 000400): Train loss 5.489, Val loss 5.414\n",
            "Ep 1 (Step 000405): Train loss 5.426, Val loss 5.397\n",
            "Ep 1 (Step 000410): Train loss 5.471, Val loss 5.416\n",
            "Ep 1 (Step 000415): Train loss 5.520, Val loss 5.408\n",
            "Ep 1 (Step 000420): Train loss 5.491, Val loss 5.399\n",
            "Ep 1 (Step 000425): Train loss 5.432, Val loss 5.380\n",
            "Ep 1 (Step 000430): Train loss 5.209, Val loss 5.374\n",
            "Ep 1 (Step 000435): Train loss 5.417, Val loss 5.371\n",
            "Ep 1 (Step 000440): Train loss 5.225, Val loss 5.390\n",
            "Ep 1 (Step 000445): Train loss 5.487, Val loss 5.433\n",
            "Ep 1 (Step 000450): Train loss 5.214, Val loss 5.383\n",
            "Ep 1 (Step 000455): Train loss 5.209, Val loss 5.377\n",
            "Ep 1 (Step 000460): Train loss 5.107, Val loss 5.362\n",
            "Ep 1 (Step 000465): Train loss 5.452, Val loss 5.344\n",
            "Ep 1 (Step 000470): Train loss 5.307, Val loss 5.352\n",
            "Ep 1 (Step 000475): Train loss 5.339, Val loss 5.365\n",
            "Ep 1 (Step 000480): Train loss 5.071, Val loss 5.376\n",
            "Ep 1 (Step 000485): Train loss 5.188, Val loss 5.354\n",
            "Ep 1 (Step 000490): Train loss 4.894, Val loss 5.311\n",
            "Ep 1 (Step 000495): Train loss 5.406, Val loss 5.329\n",
            "Ep 1 (Step 000500): Train loss 5.336, Val loss 5.379\n",
            "Ep 1 (Step 000505): Train loss 5.003, Val loss 5.339\n",
            "Ep 1 (Step 000510): Train loss 5.215, Val loss 5.359\n",
            "Ep 1 (Step 000515): Train loss 5.267, Val loss 5.360\n",
            "Ep 1 (Step 000520): Train loss 5.105, Val loss 5.337\n",
            "Ep 1 (Step 000525): Train loss 5.538, Val loss 5.342\n",
            "Ep 1 (Step 000530): Train loss 5.194, Val loss 5.273\n",
            "Ep 1 (Step 000535): Train loss 5.366, Val loss 5.258\n",
            "Ep 1 (Step 000540): Train loss 5.151, Val loss 5.228\n",
            "Ep 1 (Step 000545): Train loss 5.198, Val loss 5.265\n",
            "Ep 1 (Step 000550): Train loss 5.435, Val loss 5.262\n",
            "Ep 1 (Step 000555): Train loss 4.945, Val loss 5.247\n",
            "Ep 1 (Step 000560): Train loss 4.894, Val loss 5.234\n",
            "Ep 1 (Step 000565): Train loss 5.229, Val loss 5.245\n",
            "Ep 1 (Step 000570): Train loss 5.236, Val loss 5.288\n",
            "Ep 1 (Step 000575): Train loss 5.471, Val loss 5.278\n",
            "Ep 1 (Step 000580): Train loss 5.367, Val loss 5.275\n",
            "Ep 1 (Step 000585): Train loss 4.941, Val loss 5.272\n",
            "Ep 1 (Step 000590): Train loss 5.197, Val loss 5.262\n",
            "Ep 1 (Step 000595): Train loss 5.054, Val loss 5.282\n",
            "Ep 1 (Step 000600): Train loss 5.257, Val loss 5.265\n",
            "Ep 1 (Step 000605): Train loss 5.343, Val loss 5.293\n",
            "Ep 1 (Step 000610): Train loss 5.247, Val loss 5.241\n",
            "Ep 1 (Step 000615): Train loss 5.202, Val loss 5.248\n",
            "Ep 1 (Step 000620): Train loss 4.952, Val loss 5.230\n",
            "Ep 1 (Step 000625): Train loss 5.212, Val loss 5.194\n",
            "Ep 1 (Step 000630): Train loss 5.453, Val loss 5.177\n",
            "Ep 1 (Step 000635): Train loss 5.423, Val loss 5.161\n",
            "Ep 1 (Step 000640): Train loss 5.177, Val loss 5.155\n",
            "Ep 1 (Step 000645): Train loss 5.087, Val loss 5.138\n",
            "Ep 1 (Step 000650): Train loss 5.137, Val loss 5.120\n",
            "Ep 1 (Step 000655): Train loss 5.071, Val loss 5.108\n",
            "Ep 1 (Step 000660): Train loss 5.051, Val loss 5.123\n",
            "Ep 1 (Step 000665): Train loss 5.378, Val loss 5.144\n",
            "Ep 1 (Step 000670): Train loss 4.901, Val loss 5.152\n",
            "Ep 1 (Step 000675): Train loss 4.848, Val loss 5.170\n",
            "Ep 1 (Step 000680): Train loss 5.185, Val loss 5.179\n",
            "Ep 1 (Step 000685): Train loss 5.113, Val loss 5.180\n",
            "Ep 1 (Step 000690): Train loss 4.852, Val loss 5.144\n",
            "Ep 1 (Step 000695): Train loss 5.009, Val loss 5.092\n",
            "Ep 1 (Step 000700): Train loss 5.024, Val loss 5.078\n",
            "Ep 1 (Step 000705): Train loss 5.285, Val loss 5.090\n",
            "Ep 1 (Step 000710): Train loss 4.966, Val loss 5.086\n",
            "Ep 1 (Step 000715): Train loss 5.050, Val loss 5.077\n",
            "Ep 1 (Step 000720): Train loss 4.908, Val loss 5.096\n",
            "Ep 1 (Step 000725): Train loss 5.037, Val loss 5.089\n",
            "Ep 1 (Step 000730): Train loss 4.986, Val loss 5.092\n",
            "Ep 1 (Step 000735): Train loss 4.784, Val loss 5.068\n",
            "Ep 1 (Step 000740): Train loss 4.987, Val loss 5.094\n",
            "Ep 1 (Step 000745): Train loss 5.130, Val loss 5.082\n",
            "Ep 1 (Step 000750): Train loss 5.304, Val loss 5.090\n",
            "Ep 1 (Step 000755): Train loss 5.155, Val loss 5.102\n",
            "Ep 1 (Step 000760): Train loss 4.930, Val loss 5.082\n",
            "Ep 1 (Step 000765): Train loss 4.969, Val loss 5.044\n",
            "Ep 1 (Step 000770): Train loss 5.334, Val loss 5.050\n",
            "Ep 1 (Step 000775): Train loss 5.099, Val loss 5.055\n",
            "Ep 1 (Step 000780): Train loss 5.244, Val loss 5.024\n",
            "Ep 1 (Step 000785): Train loss 4.933, Val loss 5.048\n",
            "Ep 1 (Step 000790): Train loss 5.025, Val loss 5.026\n",
            "Ep 1 (Step 000795): Train loss 5.290, Val loss 5.023\n",
            "Ep 1 (Step 000800): Train loss 5.389, Val loss 5.002\n",
            "Ep 1 (Step 000805): Train loss 4.953, Val loss 5.000\n",
            "Ep 1 (Step 000810): Train loss 4.944, Val loss 4.992\n",
            "Ep 1 (Step 000815): Train loss 4.827, Val loss 4.991\n",
            "Ep 1 (Step 000820): Train loss 5.212, Val loss 4.982\n",
            "Ep 1 (Step 000825): Train loss 5.204, Val loss 5.010\n",
            "Ep 1 (Step 000830): Train loss 4.841, Val loss 4.989\n",
            "Ep 1 (Step 000835): Train loss 5.241, Val loss 4.995\n",
            "Ep 1 (Step 000840): Train loss 4.931, Val loss 5.026\n",
            "Ep 1 (Step 000845): Train loss 5.247, Val loss 5.031\n",
            "Ep 1 (Step 000850): Train loss 5.024, Val loss 5.004\n",
            "Ep 1 (Step 000855): Train loss 4.908, Val loss 4.985\n",
            "Ep 1 (Step 000860): Train loss 4.955, Val loss 4.996\n",
            "Ep 1 (Step 000865): Train loss 5.057, Val loss 4.981\n",
            "Ep 1 (Step 000870): Train loss 4.951, Val loss 4.976\n",
            "Ep 1 (Step 000875): Train loss 5.018, Val loss 4.980\n",
            "Ep 1 (Step 000880): Train loss 5.102, Val loss 4.976\n",
            "Ep 1 (Step 000885): Train loss 4.733, Val loss 5.002\n",
            "Ep 1 (Step 000890): Train loss 5.096, Val loss 5.023\n",
            "Ep 1 (Step 000895): Train loss 5.276, Val loss 5.020\n",
            "Ep 1 (Step 000900): Train loss 5.106, Val loss 5.026\n",
            "Ep 1 (Step 000905): Train loss 5.060, Val loss 5.005\n",
            "Ep 1 (Step 000910): Train loss 4.855, Val loss 4.989\n",
            "Ep 1 (Step 000915): Train loss 4.799, Val loss 5.000\n",
            "Ep 1 (Step 000920): Train loss 4.723, Val loss 4.993\n",
            "Ep 1 (Step 000925): Train loss 5.052, Val loss 5.039\n",
            "Ep 1 (Step 000930): Train loss 5.271, Val loss 5.088\n",
            "Ep 1 (Step 000935): Train loss 4.800, Val loss 5.072\n",
            "Ep 1 (Step 000940): Train loss 4.937, Val loss 5.036\n",
            "Ep 1 (Step 000945): Train loss 4.687, Val loss 5.014\n",
            "Ep 1 (Step 000950): Train loss 4.891, Val loss 5.028\n",
            "Ep 1 (Step 000955): Train loss 4.576, Val loss 5.006\n",
            "Ep 1 (Step 000960): Train loss 5.041, Val loss 5.004\n",
            "Ep 1 (Step 000965): Train loss 4.873, Val loss 5.018\n",
            "Ep 1 (Step 000970): Train loss 5.047, Val loss 5.031\n",
            "Ep 1 (Step 000975): Train loss 4.854, Val loss 5.037\n",
            "Ep 1 (Step 000980): Train loss 4.891, Val loss 5.039\n",
            "Ep 1 (Step 000985): Train loss 4.826, Val loss 5.018\n",
            "Ep 1 (Step 000990): Train loss 4.931, Val loss 5.034\n",
            "Ep 1 (Step 000995): Train loss 4.934, Val loss 5.039\n",
            "Ep 1 (Step 001000): Train loss 4.960, Val loss 5.012\n",
            "Ep 1 (Step 001005): Train loss 4.492, Val loss 5.011\n",
            "Ep 1 (Step 001010): Train loss 4.830, Val loss 5.014\n",
            "Ep 1 (Step 001015): Train loss 4.900, Val loss 5.006\n",
            "Ep 1 (Step 001020): Train loss 5.073, Val loss 5.012\n",
            "Ep 1 (Step 001025): Train loss 4.798, Val loss 4.992\n",
            "Ep 1 (Step 001030): Train loss 5.092, Val loss 5.001\n",
            "Ep 1 (Step 001035): Train loss 4.618, Val loss 4.961\n",
            "Ep 1 (Step 001040): Train loss 4.782, Val loss 4.948\n",
            "Ep 1 (Step 001045): Train loss 4.724, Val loss 4.945\n",
            "Ep 1 (Step 001050): Train loss 5.137, Val loss 4.963\n",
            "Ep 1 (Step 001055): Train loss 4.800, Val loss 4.960\n",
            "Ep 1 (Step 001060): Train loss 4.970, Val loss 4.962\n",
            "Ep 1 (Step 001065): Train loss 4.889, Val loss 4.993\n",
            "Ep 1 (Step 001070): Train loss 4.811, Val loss 4.983\n",
            "Ep 1 (Step 001075): Train loss 4.706, Val loss 4.982\n",
            "Ep 1 (Step 001080): Train loss 5.003, Val loss 4.966\n",
            "Ep 1 (Step 001085): Train loss 4.668, Val loss 4.960\n",
            "Ep 1 (Step 001090): Train loss 5.126, Val loss 4.958\n",
            "Ep 1 (Step 001095): Train loss 4.992, Val loss 4.965\n",
            "Ep 1 (Step 001100): Train loss 4.804, Val loss 4.967\n",
            "Ep 1 (Step 001105): Train loss 4.849, Val loss 4.937\n",
            "Ep 1 (Step 001110): Train loss 4.953, Val loss 4.940\n",
            "Ep 1 (Step 001115): Train loss 4.844, Val loss 4.906\n",
            "Ep 1 (Step 001120): Train loss 5.327, Val loss 4.888\n",
            "Ep 1 (Step 001125): Train loss 4.744, Val loss 4.903\n",
            "Ep 1 (Step 001130): Train loss 4.769, Val loss 4.906\n",
            "Ep 1 (Step 001135): Train loss 4.746, Val loss 4.902\n",
            "Ep 1 (Step 001140): Train loss 5.085, Val loss 4.889\n",
            "Ep 1 (Step 001145): Train loss 5.167, Val loss 4.906\n",
            "Ep 1 (Step 001150): Train loss 4.766, Val loss 4.914\n",
            "Ep 1 (Step 001155): Train loss 4.287, Val loss 4.910\n",
            "Ep 1 (Step 001160): Train loss 4.790, Val loss 4.936\n",
            "Ep 1 (Step 001165): Train loss 4.952, Val loss 4.948\n",
            "Ep 1 (Step 001170): Train loss 4.991, Val loss 4.924\n",
            "Ep 1 (Step 001175): Train loss 5.007, Val loss 4.911\n",
            "Ep 1 (Step 001180): Train loss 4.851, Val loss 4.917\n",
            "Ep 1 (Step 001185): Train loss 4.915, Val loss 4.945\n",
            "Ep 1 (Step 001190): Train loss 4.729, Val loss 4.905\n",
            "Ep 1 (Step 001195): Train loss 5.198, Val loss 4.909\n",
            "Ep 1 (Step 001200): Train loss 5.090, Val loss 4.927\n",
            "Ep 1 (Step 001205): Train loss 5.077, Val loss 4.926\n",
            "Ep 1 (Step 001210): Train loss 4.835, Val loss 4.922\n",
            "Ep 1 (Step 001215): Train loss 4.872, Val loss 4.933\n",
            "Ep 1 (Step 001220): Train loss 4.761, Val loss 4.931\n",
            "Ep 1 (Step 001225): Train loss 4.778, Val loss 4.919\n",
            "Ep 1 (Step 001230): Train loss 4.646, Val loss 4.886\n",
            "Ep 1 (Step 001235): Train loss 4.743, Val loss 4.870\n",
            "Ep 1 (Step 001240): Train loss 4.688, Val loss 4.858\n",
            "Ep 1 (Step 001245): Train loss 5.022, Val loss 4.870\n",
            "Ep 1 (Step 001250): Train loss 4.873, Val loss 4.877\n",
            "Ep 1 (Step 001255): Train loss 4.695, Val loss 4.879\n",
            "Ep 1 (Step 001260): Train loss 4.980, Val loss 4.866\n",
            "Ep 1 (Step 001265): Train loss 4.967, Val loss 4.882\n",
            "Ep 1 (Step 001270): Train loss 4.916, Val loss 4.888\n",
            "Ep 1 (Step 001275): Train loss 4.606, Val loss 4.892\n",
            "Ep 1 (Step 001280): Train loss 5.074, Val loss 4.877\n",
            "Ep 1 (Step 001285): Train loss 4.862, Val loss 4.881\n",
            "Ep 1 (Step 001290): Train loss 4.961, Val loss 4.884\n",
            "Ep 1 (Step 001295): Train loss 4.825, Val loss 4.873\n",
            "Ep 1 (Step 001300): Train loss 4.864, Val loss 4.865\n",
            "Ep 1 (Step 001305): Train loss 4.979, Val loss 4.855\n",
            "Ep 1 (Step 001310): Train loss 4.596, Val loss 4.854\n",
            "Ep 1 (Step 001315): Train loss 4.557, Val loss 4.866\n",
            "Ep 1 (Step 001320): Train loss 4.626, Val loss 4.878\n",
            "Ep 1 (Step 001325): Train loss 4.747, Val loss 4.884\n",
            "Ep 1 (Step 001330): Train loss 4.784, Val loss 4.895\n",
            "Ep 1 (Step 001335): Train loss 4.886, Val loss 4.872\n",
            "Ep 1 (Step 001340): Train loss 4.901, Val loss 4.875\n",
            "Ep 1 (Step 001345): Train loss 4.957, Val loss 4.852\n",
            "Ep 1 (Step 001350): Train loss 4.967, Val loss 4.852\n",
            "Ep 1 (Step 001355): Train loss 4.639, Val loss 4.844\n",
            "Ep 1 (Step 001360): Train loss 4.597, Val loss 4.839\n",
            "Ep 1 (Step 001365): Train loss 4.703, Val loss 4.831\n",
            "Ep 1 (Step 001370): Train loss 4.689, Val loss 4.842\n",
            "Ep 1 (Step 001375): Train loss 5.146, Val loss 4.849\n",
            "Ep 1 (Step 001380): Train loss 5.037, Val loss 4.861\n",
            "Ep 1 (Step 001385): Train loss 4.675, Val loss 4.880\n",
            "Ep 1 (Step 001390): Train loss 4.677, Val loss 4.883\n",
            "Ep 1 (Step 001395): Train loss 4.885, Val loss 4.870\n",
            "Ep 1 (Step 001400): Train loss 5.068, Val loss 4.843\n",
            "Ep 1 (Step 001405): Train loss 4.832, Val loss 4.856\n",
            "Ep 1 (Step 001410): Train loss 4.544, Val loss 4.868\n",
            "Ep 1 (Step 001415): Train loss 4.554, Val loss 4.845\n",
            "Ep 1 (Step 001420): Train loss 4.837, Val loss 4.858\n",
            "Ep 1 (Step 001425): Train loss 4.753, Val loss 4.874\n",
            "Ep 1 (Step 001430): Train loss 4.744, Val loss 4.862\n",
            "Ep 1 (Step 001435): Train loss 4.949, Val loss 4.855\n",
            "Ep 1 (Step 001440): Train loss 5.020, Val loss 4.865\n",
            "Ep 1 (Step 001445): Train loss 4.821, Val loss 4.858\n",
            "Ep 1 (Step 001450): Train loss 4.525, Val loss 4.861\n",
            "Ep 1 (Step 001455): Train loss 4.639, Val loss 4.840\n",
            "Ep 1 (Step 001460): Train loss 4.781, Val loss 4.870\n",
            "Ep 1 (Step 001465): Train loss 4.914, Val loss 4.833\n",
            "Ep 1 (Step 001470): Train loss 4.818, Val loss 4.828\n",
            "Ep 1 (Step 001475): Train loss 4.484, Val loss 4.828\n",
            "Ep 1 (Step 001480): Train loss 4.541, Val loss 4.831\n",
            "Ep 1 (Step 001485): Train loss 4.735, Val loss 4.862\n",
            "Ep 1 (Step 001490): Train loss 4.612, Val loss 4.855\n",
            "Ep 1 (Step 001495): Train loss 4.443, Val loss 4.869\n",
            "Ep 1 (Step 001500): Train loss 4.603, Val loss 4.890\n",
            "Ep 1 (Step 001505): Train loss 4.771, Val loss 4.880\n",
            "Ep 1 (Step 001510): Train loss 4.624, Val loss 4.886\n",
            "Ep 1 (Step 001515): Train loss 4.661, Val loss 4.858\n",
            "Ep 1 (Step 001520): Train loss 4.777, Val loss 4.825\n",
            "Ep 1 (Step 001525): Train loss 4.797, Val loss 4.824\n",
            "Ep 1 (Step 001530): Train loss 4.870, Val loss 4.799\n",
            "Ep 1 (Step 001535): Train loss 4.532, Val loss 4.809\n",
            "Ep 1 (Step 001540): Train loss 4.827, Val loss 4.824\n",
            "Ep 1 (Step 001545): Train loss 4.684, Val loss 4.817\n",
            "Ep 1 (Step 001550): Train loss 4.612, Val loss 4.829\n",
            "Ep 1 (Step 001555): Train loss 4.741, Val loss 4.822\n",
            "Ep 1 (Step 001560): Train loss 4.515, Val loss 4.820\n",
            "Ep 1 (Step 001565): Train loss 4.777, Val loss 4.811\n",
            "Ep 1 (Step 001570): Train loss 4.441, Val loss 4.827\n",
            "Ep 1 (Step 001575): Train loss 4.348, Val loss 4.825\n",
            "Ep 1 (Step 001580): Train loss 4.702, Val loss 4.821\n",
            "Ep 1 (Step 001585): Train loss 4.613, Val loss 4.822\n",
            "Ep 1 (Step 001590): Train loss 4.657, Val loss 4.794\n",
            "Ep 1 (Step 001595): Train loss 4.829, Val loss 4.790\n",
            "Ep 1 (Step 001600): Train loss 4.540, Val loss 4.792\n",
            "Ep 1 (Step 001605): Train loss 4.889, Val loss 4.791\n",
            "Ep 1 (Step 001610): Train loss 4.644, Val loss 4.799\n",
            "Ep 1 (Step 001615): Train loss 4.835, Val loss 4.817\n",
            "Ep 1 (Step 001620): Train loss 4.680, Val loss 4.843\n",
            "Ep 1 (Step 001625): Train loss 4.770, Val loss 4.843\n",
            "Ep 1 (Step 001630): Train loss 4.854, Val loss 4.840\n",
            "Ep 1 (Step 001635): Train loss 4.724, Val loss 4.814\n",
            "Ep 1 (Step 001640): Train loss 4.742, Val loss 4.832\n",
            "Ep 1 (Step 001645): Train loss 4.485, Val loss 4.841\n",
            "Ep 1 (Step 001650): Train loss 4.593, Val loss 4.820\n",
            "Ep 1 (Step 001655): Train loss 4.858, Val loss 4.822\n",
            "Ep 1 (Step 001660): Train loss 4.485, Val loss 4.791\n",
            "Ep 1 (Step 001665): Train loss 4.936, Val loss 4.789\n",
            "Ep 1 (Step 001670): Train loss 4.595, Val loss 4.789\n",
            "Ep 1 (Step 001675): Train loss 4.917, Val loss 4.808\n",
            "Ep 1 (Step 001680): Train loss 4.615, Val loss 4.802\n",
            "Ep 1 (Step 001685): Train loss 4.720, Val loss 4.795\n",
            "Ep 1 (Step 001690): Train loss 4.493, Val loss 4.789\n",
            "Ep 1 (Step 001695): Train loss 4.825, Val loss 4.797\n",
            "Ep 1 (Step 001700): Train loss 4.535, Val loss 4.809\n",
            "Ep 1 (Step 001705): Train loss 4.616, Val loss 4.821\n",
            "Ep 1 (Step 001710): Train loss 4.604, Val loss 4.810\n",
            "Ep 1 (Step 001715): Train loss 4.558, Val loss 4.811\n",
            "Ep 1 (Step 001720): Train loss 4.718, Val loss 4.827\n",
            "Ep 1 (Step 001725): Train loss 4.684, Val loss 4.834\n",
            "Ep 1 (Step 001730): Train loss 4.703, Val loss 4.834\n",
            "Ep 1 (Step 001735): Train loss 4.636, Val loss 4.833\n",
            "Ep 1 (Step 001740): Train loss 4.476, Val loss 4.822\n",
            "Ep 1 (Step 001745): Train loss 4.731, Val loss 4.807\n",
            "Ep 1 (Step 001750): Train loss 4.840, Val loss 4.811\n",
            "Ep 1 (Step 001755): Train loss 4.727, Val loss 4.783\n",
            "Ep 1 (Step 001760): Train loss 4.670, Val loss 4.767\n",
            "Ep 1 (Step 001765): Train loss 4.504, Val loss 4.785\n",
            "Ep 1 (Step 001770): Train loss 4.875, Val loss 4.794\n",
            "Ep 1 (Step 001775): Train loss 4.530, Val loss 4.773\n",
            "Ep 1 (Step 001780): Train loss 4.739, Val loss 4.778\n",
            "Ep 1 (Step 001785): Train loss 4.541, Val loss 4.781\n",
            "Ep 1 (Step 001790): Train loss 4.555, Val loss 4.798\n",
            "Ep 1 (Step 001795): Train loss 4.564, Val loss 4.792\n",
            "Ep 1 (Step 001800): Train loss 4.782, Val loss 4.802\n",
            "Ep 1 (Step 001805): Train loss 4.533, Val loss 4.830\n",
            "Ep 1 (Step 001810): Train loss 4.680, Val loss 4.817\n",
            "Ep 1 (Step 001815): Train loss 4.547, Val loss 4.811\n",
            "Ep 1 (Step 001820): Train loss 4.912, Val loss 4.821\n",
            "Ep 1 (Step 001825): Train loss 4.416, Val loss 4.810\n",
            "Ep 1 (Step 001830): Train loss 4.475, Val loss 4.823\n",
            "Ep 1 (Step 001835): Train loss 4.638, Val loss 4.808\n",
            "Ep 1 (Step 001840): Train loss 4.451, Val loss 4.808\n",
            "Ep 1 (Step 001845): Train loss 4.627, Val loss 4.830\n",
            "Ep 1 (Step 001850): Train loss 4.675, Val loss 4.817\n",
            "Ep 1 (Step 001855): Train loss 4.708, Val loss 4.795\n",
            "Ep 1 (Step 001860): Train loss 4.585, Val loss 4.811\n",
            "Ep 1 (Step 001865): Train loss 4.508, Val loss 4.819\n",
            "Ep 1 (Step 001870): Train loss 4.918, Val loss 4.801\n",
            "Ep 1 (Step 001875): Train loss 4.554, Val loss 4.785\n",
            "Ep 1 (Step 001880): Train loss 4.758, Val loss 4.803\n",
            "Ep 1 (Step 001885): Train loss 4.537, Val loss 4.809\n",
            "Ep 1 (Step 001890): Train loss 4.561, Val loss 4.799\n",
            "Ep 1 (Step 001895): Train loss 4.797, Val loss 4.835\n",
            "Ep 1 (Step 001900): Train loss 4.708, Val loss 4.818\n",
            "Ep 1 (Step 001905): Train loss 4.673, Val loss 4.786\n",
            "Ep 1 (Step 001910): Train loss 4.557, Val loss 4.786\n",
            "Ep 1 (Step 001915): Train loss 4.817, Val loss 4.799\n",
            "Ep 1 (Step 001920): Train loss 4.615, Val loss 4.787\n",
            "Ep 1 (Step 001925): Train loss 4.836, Val loss 4.786\n",
            "Ep 1 (Step 001930): Train loss 4.742, Val loss 4.786\n",
            "Ep 1 (Step 001935): Train loss 4.904, Val loss 4.820\n",
            "Ep 1 (Step 001940): Train loss 4.507, Val loss 4.814\n",
            "Ep 1 (Step 001945): Train loss 4.810, Val loss 4.800\n",
            "Ep 1 (Step 001950): Train loss 4.722, Val loss 4.807\n",
            "Ep 1 (Step 001955): Train loss 4.663, Val loss 4.790\n",
            "Ep 1 (Step 001960): Train loss 4.828, Val loss 4.784\n",
            "Ep 1 (Step 001965): Train loss 4.757, Val loss 4.766\n",
            "Ep 1 (Step 001970): Train loss 4.654, Val loss 4.786\n",
            "Ep 1 (Step 001975): Train loss 4.373, Val loss 4.782\n",
            "Ep 1 (Step 001980): Train loss 4.634, Val loss 4.792\n",
            "Ep 1 (Step 001985): Train loss 4.757, Val loss 4.818\n",
            "Ep 1 (Step 001990): Train loss 4.445, Val loss 4.803\n",
            "Ep 1 (Step 001995): Train loss 4.962, Val loss 4.810\n",
            "Ep 1 (Step 002000): Train loss 4.525, Val loss 4.813\n",
            "Ep 1 (Step 002005): Train loss 4.856, Val loss 4.807\n",
            "Ep 1 (Step 002010): Train loss 4.561, Val loss 4.814\n",
            "Ep 1 (Step 002015): Train loss 4.698, Val loss 4.806\n",
            "Ep 1 (Step 002020): Train loss 4.649, Val loss 4.802\n",
            "Ep 1 (Step 002025): Train loss 4.540, Val loss 4.805\n",
            "Ep 1 (Step 002030): Train loss 4.533, Val loss 4.800\n",
            "Ep 1 (Step 002035): Train loss 4.779, Val loss 4.801\n",
            "Ep 1 (Step 002040): Train loss 4.715, Val loss 4.792\n",
            "Ep 1 (Step 002045): Train loss 4.373, Val loss 4.812\n",
            "Ep 1 (Step 002050): Train loss 4.635, Val loss 4.814\n",
            "Ep 1 (Step 002055): Train loss 4.553, Val loss 4.801\n",
            "Ep 1 (Step 002060): Train loss 4.738, Val loss 4.781\n",
            "Ep 1 (Step 002065): Train loss 4.635, Val loss 4.789\n",
            "Ep 1 (Step 002070): Train loss 4.570, Val loss 4.805\n",
            "Ep 1 (Step 002075): Train loss 4.642, Val loss 4.777\n",
            "Ep 1 (Step 002080): Train loss 4.882, Val loss 4.760\n",
            "Ep 1 (Step 002085): Train loss 4.641, Val loss 4.773\n",
            "Ep 1 (Step 002090): Train loss 4.206, Val loss 4.773\n",
            "Ep 1 (Step 002095): Train loss 4.569, Val loss 4.785\n",
            "Ep 1 (Step 002100): Train loss 4.572, Val loss 4.802\n",
            "Ep 1 (Step 002105): Train loss 4.635, Val loss 4.819\n",
            "Ep 1 (Step 002110): Train loss 4.540, Val loss 4.816\n",
            "Ep 1 (Step 002115): Train loss 4.580, Val loss 4.815\n",
            "Ep 1 (Step 002120): Train loss 4.614, Val loss 4.845\n",
            "Ep 1 (Step 002125): Train loss 4.474, Val loss 4.857\n",
            "Ep 1 (Step 002130): Train loss 4.486, Val loss 4.821\n",
            "Ep 1 (Step 002135): Train loss 4.636, Val loss 4.819\n",
            "Ep 1 (Step 002140): Train loss 5.120, Val loss 4.822\n",
            "Ep 1 (Step 002145): Train loss 4.573, Val loss 4.811\n",
            "Ep 1 (Step 002150): Train loss 4.596, Val loss 4.795\n",
            "Ep 1 (Step 002155): Train loss 4.647, Val loss 4.786\n",
            "Ep 1 (Step 002160): Train loss 4.327, Val loss 4.777\n",
            "Ep 1 (Step 002165): Train loss 4.654, Val loss 4.788\n",
            "Ep 1 (Step 002170): Train loss 4.328, Val loss 4.778\n",
            "Ep 1 (Step 002175): Train loss 4.223, Val loss 4.770\n",
            "Ep 1 (Step 002180): Train loss 4.673, Val loss 4.786\n",
            "Ep 1 (Step 002185): Train loss 4.527, Val loss 4.775\n",
            "Ep 1 (Step 002190): Train loss 4.517, Val loss 4.803\n",
            "Ep 1 (Step 002195): Train loss 4.512, Val loss 4.789\n",
            "Ep 1 (Step 002200): Train loss 4.647, Val loss 4.799\n",
            "Ep 1 (Step 002205): Train loss 4.619, Val loss 4.796\n",
            "Ep 1 (Step 002210): Train loss 4.508, Val loss 4.785\n",
            "Ep 1 (Step 002215): Train loss 4.715, Val loss 4.775\n",
            "Ep 1 (Step 002220): Train loss 4.562, Val loss 4.777\n",
            "Ep 1 (Step 002225): Train loss 4.385, Val loss 4.778\n",
            "Ep 1 (Step 002230): Train loss 4.756, Val loss 4.774\n",
            "Ep 1 (Step 002235): Train loss 4.611, Val loss 4.786\n",
            "Ep 1 (Step 002240): Train loss 4.759, Val loss 4.790\n",
            "Ep 1 (Step 002245): Train loss 4.572, Val loss 4.773\n",
            "Ep 1 (Step 002250): Train loss 4.530, Val loss 4.772\n",
            "Ep 1 (Step 002255): Train loss 4.483, Val loss 4.784\n",
            "Ep 1 (Step 002260): Train loss 4.493, Val loss 4.777\n",
            "Ep 1 (Step 002265): Train loss 4.438, Val loss 4.786\n",
            "Ep 1 (Step 002270): Train loss 4.244, Val loss 4.796\n",
            "Ep 1 (Step 002275): Train loss 4.612, Val loss 4.786\n",
            "Ep 1 (Step 002280): Train loss 4.565, Val loss 4.772\n",
            "Ep 1 (Step 002285): Train loss 4.644, Val loss 4.762\n",
            "Ep 1 (Step 002290): Train loss 4.687, Val loss 4.782\n",
            "Ep 1 (Step 002295): Train loss 4.474, Val loss 4.778\n",
            "Ep 1 (Step 002300): Train loss 4.830, Val loss 4.772\n",
            "Ep 1 (Step 002305): Train loss 4.577, Val loss 4.752\n",
            "Ep 1 (Step 002310): Train loss 4.621, Val loss 4.729\n",
            "Ep 1 (Step 002315): Train loss 4.404, Val loss 4.734\n",
            "Ep 1 (Step 002320): Train loss 4.501, Val loss 4.719\n",
            "Ep 1 (Step 002325): Train loss 4.785, Val loss 4.718\n",
            "Ep 1 (Step 002330): Train loss 4.265, Val loss 4.731\n",
            "Ep 1 (Step 002335): Train loss 4.810, Val loss 4.736\n",
            "Ep 1 (Step 002340): Train loss 4.665, Val loss 4.742\n",
            "Ep 1 (Step 002345): Train loss 4.685, Val loss 4.733\n",
            "Ep 1 (Step 002350): Train loss 4.474, Val loss 4.720\n",
            "Ep 1 (Step 002355): Train loss 4.702, Val loss 4.721\n",
            "Ep 1 (Step 002360): Train loss 4.183, Val loss 4.736\n",
            "Ep 1 (Step 002365): Train loss 4.424, Val loss 4.729\n",
            "Ep 1 (Step 002370): Train loss 4.567, Val loss 4.726\n",
            "Ep 1 (Step 002375): Train loss 4.536, Val loss 4.714\n",
            "Ep 1 (Step 002380): Train loss 4.505, Val loss 4.696\n",
            "Ep 1 (Step 002385): Train loss 4.452, Val loss 4.695\n",
            "Ep 1 (Step 002390): Train loss 4.735, Val loss 4.686\n",
            "Ep 1 (Step 002395): Train loss 4.388, Val loss 4.683\n",
            "Ep 1 (Step 002400): Train loss 4.522, Val loss 4.683\n",
            "Ep 1 (Step 002405): Train loss 4.359, Val loss 4.703\n",
            "Ep 1 (Step 002410): Train loss 4.460, Val loss 4.721\n",
            "Ep 1 (Step 002415): Train loss 4.525, Val loss 4.722\n",
            "Ep 1 (Step 002420): Train loss 4.133, Val loss 4.712\n",
            "Ep 1 (Step 002425): Train loss 4.220, Val loss 4.720\n",
            "Ep 1 (Step 002430): Train loss 4.339, Val loss 4.711\n",
            "Ep 1 (Step 002435): Train loss 4.470, Val loss 4.698\n",
            "Ep 1 (Step 002440): Train loss 4.410, Val loss 4.692\n",
            "Ep 1 (Step 002445): Train loss 4.374, Val loss 4.697\n",
            "Ep 1 (Step 002450): Train loss 4.548, Val loss 4.685\n",
            "Ep 1 (Step 002455): Train loss 4.327, Val loss 4.672\n",
            "Ep 1 (Step 002460): Train loss 4.474, Val loss 4.676\n",
            "Ep 1 (Step 002465): Train loss 4.300, Val loss 4.696\n",
            "Ep 1 (Step 002470): Train loss 4.683, Val loss 4.673\n",
            "Ep 1 (Step 002475): Train loss 4.510, Val loss 4.661\n",
            "Ep 1 (Step 002480): Train loss 4.277, Val loss 4.648\n",
            "Ep 1 (Step 002485): Train loss 4.454, Val loss 4.666\n",
            "Ep 1 (Step 002490): Train loss 4.605, Val loss 4.677\n",
            "Ep 1 (Step 002495): Train loss 4.517, Val loss 4.663\n",
            "Ep 1 (Step 002500): Train loss 4.504, Val loss 4.657\n",
            "Ep 1 (Step 002505): Train loss 4.601, Val loss 4.661\n",
            "Ep 1 (Step 002510): Train loss 4.556, Val loss 4.658\n",
            "Ep 1 (Step 002515): Train loss 4.398, Val loss 4.667\n",
            "Ep 1 (Step 002520): Train loss 4.254, Val loss 4.670\n",
            "Ep 1 (Step 002525): Train loss 4.457, Val loss 4.685\n",
            "Ep 1 (Step 002530): Train loss 4.021, Val loss 4.656\n",
            "Ep 1 (Step 002535): Train loss 4.533, Val loss 4.658\n",
            "Ep 1 (Step 002540): Train loss 4.710, Val loss 4.675\n",
            "Ep 1 (Step 002545): Train loss 4.451, Val loss 4.674\n",
            "Ep 1 (Step 002550): Train loss 4.706, Val loss 4.676\n",
            "Ep 1 (Step 002555): Train loss 4.629, Val loss 4.680\n",
            "Ep 1 (Step 002560): Train loss 4.750, Val loss 4.682\n",
            "Ep 1 (Step 002565): Train loss 4.495, Val loss 4.681\n",
            "Ep 1 (Step 002570): Train loss 4.529, Val loss 4.694\n",
            "Ep 1 (Step 002575): Train loss 4.699, Val loss 4.689\n",
            "Ep 1 (Step 002580): Train loss 4.459, Val loss 4.664\n",
            "Ep 1 (Step 002585): Train loss 4.246, Val loss 4.663\n",
            "Ep 1 (Step 002590): Train loss 4.314, Val loss 4.685\n",
            "Ep 1 (Step 002595): Train loss 4.498, Val loss 4.671\n",
            "Ep 1 (Step 002600): Train loss 4.258, Val loss 4.658\n",
            "Ep 1 (Step 002605): Train loss 4.389, Val loss 4.653\n",
            "Ep 1 (Step 002610): Train loss 4.601, Val loss 4.658\n",
            "Ep 1 (Step 002615): Train loss 4.390, Val loss 4.651\n",
            "Ep 1 (Step 002620): Train loss 4.462, Val loss 4.655\n",
            "Ep 1 (Step 002625): Train loss 4.415, Val loss 4.643\n",
            "Ep 1 (Step 002630): Train loss 4.267, Val loss 4.618\n",
            "Ep 1 (Step 002635): Train loss 4.126, Val loss 4.625\n",
            "Ep 1 (Step 002640): Train loss 4.462, Val loss 4.632\n",
            "Ep 1 (Step 002645): Train loss 4.488, Val loss 4.645\n",
            "Ep 1 (Step 002650): Train loss 4.579, Val loss 4.651\n",
            "Ep 1 (Step 002655): Train loss 4.218, Val loss 4.660\n",
            "Ep 1 (Step 002660): Train loss 4.369, Val loss 4.662\n",
            "Ep 1 (Step 002665): Train loss 4.382, Val loss 4.655\n",
            "Ep 1 (Step 002670): Train loss 4.574, Val loss 4.652\n",
            "Ep 1 (Step 002675): Train loss 4.418, Val loss 4.670\n",
            "Ep 1 (Step 002680): Train loss 4.668, Val loss 4.667\n",
            "Ep 1 (Step 002685): Train loss 4.628, Val loss 4.672\n",
            "Ep 1 (Step 002690): Train loss 4.309, Val loss 4.677\n",
            "Ep 1 (Step 002695): Train loss 4.155, Val loss 4.691\n",
            "Ep 1 (Step 002700): Train loss 4.729, Val loss 4.683\n",
            "Ep 1 (Step 002705): Train loss 4.518, Val loss 4.659\n",
            "Ep 1 (Step 002710): Train loss 4.700, Val loss 4.652\n",
            "Ep 1 (Step 002715): Train loss 4.332, Val loss 4.673\n",
            "Ep 1 (Step 002720): Train loss 4.662, Val loss 4.683\n",
            "Ep 1 (Step 002725): Train loss 4.249, Val loss 4.676\n",
            "Ep 1 (Step 002730): Train loss 4.501, Val loss 4.654\n",
            "Ep 1 (Step 002735): Train loss 4.364, Val loss 4.658\n",
            "Ep 1 (Step 002740): Train loss 4.612, Val loss 4.672\n",
            "Ep 1 (Step 002745): Train loss 4.325, Val loss 4.661\n",
            "Ep 1 (Step 002750): Train loss 4.504, Val loss 4.660\n",
            "Ep 1 (Step 002755): Train loss 4.490, Val loss 4.653\n",
            "Ep 1 (Step 002760): Train loss 4.188, Val loss 4.638\n",
            "Ep 1 (Step 002765): Train loss 4.547, Val loss 4.633\n",
            "Ep 1 (Step 002770): Train loss 4.278, Val loss 4.646\n",
            "Ep 1 (Step 002775): Train loss 4.446, Val loss 4.660\n",
            "Ep 1 (Step 002780): Train loss 4.579, Val loss 4.649\n",
            "Ep 1 (Step 002785): Train loss 4.505, Val loss 4.643\n",
            "Ep 1 (Step 002790): Train loss 4.609, Val loss 4.661\n",
            "Ep 1 (Step 002795): Train loss 4.565, Val loss 4.655\n",
            "Ep 1 (Step 002800): Train loss 4.199, Val loss 4.660\n",
            "Ep 1 (Step 002805): Train loss 4.245, Val loss 4.661\n",
            "Ep 1 (Step 002810): Train loss 4.468, Val loss 4.660\n",
            "Ep 1 (Step 002815): Train loss 4.216, Val loss 4.654\n",
            "Ep 1 (Step 002820): Train loss 4.488, Val loss 4.646\n",
            "Ep 1 (Step 002825): Train loss 4.356, Val loss 4.632\n",
            "Ep 1 (Step 002830): Train loss 4.425, Val loss 4.632\n",
            "Ep 1 (Step 002835): Train loss 4.629, Val loss 4.626\n",
            "Ep 1 (Step 002840): Train loss 4.659, Val loss 4.626\n",
            "Ep 1 (Step 002845): Train loss 4.445, Val loss 4.614\n",
            "Ep 1 (Step 002850): Train loss 4.567, Val loss 4.612\n",
            "Ep 1 (Step 002855): Train loss 4.795, Val loss 4.628\n",
            "Ep 1 (Step 002860): Train loss 4.403, Val loss 4.621\n",
            "Ep 1 (Step 002865): Train loss 4.560, Val loss 4.611\n",
            "Ep 1 (Step 002870): Train loss 4.488, Val loss 4.616\n",
            "Ep 1 (Step 002875): Train loss 4.170, Val loss 4.623\n",
            "Ep 1 (Step 002880): Train loss 4.143, Val loss 4.642\n",
            "Ep 1 (Step 002885): Train loss 4.207, Val loss 4.658\n",
            "Ep 1 (Step 002890): Train loss 4.507, Val loss 4.648\n",
            "Ep 1 (Step 002895): Train loss 4.230, Val loss 4.665\n",
            "Ep 1 (Step 002900): Train loss 4.046, Val loss 4.677\n",
            "Ep 1 (Step 002905): Train loss 4.415, Val loss 4.673\n",
            "Ep 1 (Step 002910): Train loss 4.341, Val loss 4.673\n",
            "Ep 1 (Step 002915): Train loss 4.311, Val loss 4.673\n",
            "Ep 1 (Step 002920): Train loss 4.554, Val loss 4.685\n",
            "Ep 1 (Step 002925): Train loss 4.401, Val loss 4.675\n",
            "Ep 1 (Step 002930): Train loss 4.429, Val loss 4.653\n",
            "Ep 1 (Step 002935): Train loss 4.600, Val loss 4.646\n",
            "Ep 1 (Step 002940): Train loss 4.659, Val loss 4.650\n",
            "Ep 1 (Step 002945): Train loss 4.410, Val loss 4.666\n",
            "Ep 1 (Step 002950): Train loss 4.513, Val loss 4.653\n",
            "Ep 1 (Step 002955): Train loss 4.131, Val loss 4.633\n",
            "Ep 1 (Step 002960): Train loss 4.400, Val loss 4.619\n",
            "Ep 1 (Step 002965): Train loss 4.585, Val loss 4.625\n",
            "Ep 1 (Step 002970): Train loss 4.686, Val loss 4.650\n",
            "Ep 1 (Step 002975): Train loss 4.304, Val loss 4.648\n",
            "Every effort moves you, And I have been a good, And I’d, And I’d, And I’d, And I’d, And I’d, And I’d\n",
            "Ep 2 (Step 002980): Train loss 4.369, Val loss 4.636\n",
            "Ep 2 (Step 002985): Train loss 4.663, Val loss 4.668\n",
            "Ep 2 (Step 002990): Train loss 4.185, Val loss 4.642\n",
            "Ep 2 (Step 002995): Train loss 4.477, Val loss 4.647\n",
            "Ep 2 (Step 003000): Train loss 4.439, Val loss 4.661\n",
            "Ep 2 (Step 003005): Train loss 4.075, Val loss 4.669\n",
            "Ep 2 (Step 003010): Train loss 4.198, Val loss 4.661\n",
            "Ep 2 (Step 003015): Train loss 4.263, Val loss 4.649\n",
            "Ep 2 (Step 003020): Train loss 4.340, Val loss 4.656\n",
            "Ep 2 (Step 003025): Train loss 4.364, Val loss 4.673\n",
            "Ep 2 (Step 003030): Train loss 4.450, Val loss 4.675\n",
            "Ep 2 (Step 003035): Train loss 4.571, Val loss 4.671\n",
            "Ep 2 (Step 003040): Train loss 4.440, Val loss 4.669\n",
            "Ep 2 (Step 003045): Train loss 4.164, Val loss 4.666\n",
            "Ep 2 (Step 003050): Train loss 4.312, Val loss 4.674\n",
            "Ep 2 (Step 003055): Train loss 4.384, Val loss 4.691\n",
            "Ep 2 (Step 003060): Train loss 4.402, Val loss 4.685\n",
            "Ep 2 (Step 003065): Train loss 4.340, Val loss 4.668\n",
            "Ep 2 (Step 003070): Train loss 4.480, Val loss 4.659\n",
            "Ep 2 (Step 003075): Train loss 4.062, Val loss 4.661\n",
            "Ep 2 (Step 003080): Train loss 4.508, Val loss 4.683\n",
            "Ep 2 (Step 003085): Train loss 4.349, Val loss 4.710\n",
            "Ep 2 (Step 003090): Train loss 4.592, Val loss 4.675\n",
            "Ep 2 (Step 003095): Train loss 4.642, Val loss 4.663\n",
            "Ep 2 (Step 003100): Train loss 4.523, Val loss 4.657\n",
            "Ep 2 (Step 003105): Train loss 4.554, Val loss 4.647\n",
            "Ep 2 (Step 003110): Train loss 4.383, Val loss 4.673\n",
            "Ep 2 (Step 003115): Train loss 4.618, Val loss 4.682\n",
            "Ep 2 (Step 003120): Train loss 4.190, Val loss 4.692\n",
            "Ep 2 (Step 003125): Train loss 4.240, Val loss 4.684\n",
            "Ep 2 (Step 003130): Train loss 4.488, Val loss 4.678\n",
            "Ep 2 (Step 003135): Train loss 4.252, Val loss 4.676\n",
            "Ep 2 (Step 003140): Train loss 4.436, Val loss 4.681\n",
            "Ep 2 (Step 003145): Train loss 4.675, Val loss 4.670\n",
            "Ep 2 (Step 003150): Train loss 4.526, Val loss 4.682\n",
            "Ep 2 (Step 003155): Train loss 4.239, Val loss 4.685\n",
            "Ep 2 (Step 003160): Train loss 4.507, Val loss 4.699\n",
            "Ep 2 (Step 003165): Train loss 4.058, Val loss 4.685\n",
            "Ep 2 (Step 003170): Train loss 4.514, Val loss 4.674\n",
            "Ep 2 (Step 003175): Train loss 4.385, Val loss 4.650\n",
            "Ep 2 (Step 003180): Train loss 4.311, Val loss 4.649\n",
            "Ep 2 (Step 003185): Train loss 4.531, Val loss 4.645\n",
            "Ep 2 (Step 003190): Train loss 4.325, Val loss 4.652\n",
            "Ep 2 (Step 003195): Train loss 4.261, Val loss 4.657\n",
            "Ep 2 (Step 003200): Train loss 4.416, Val loss 4.662\n",
            "Ep 2 (Step 003205): Train loss 4.275, Val loss 4.657\n",
            "Ep 2 (Step 003210): Train loss 4.532, Val loss 4.657\n",
            "Ep 2 (Step 003215): Train loss 4.503, Val loss 4.669\n",
            "Ep 2 (Step 003220): Train loss 4.563, Val loss 4.690\n",
            "Ep 2 (Step 003225): Train loss 4.560, Val loss 4.677\n",
            "Ep 2 (Step 003230): Train loss 4.260, Val loss 4.657\n",
            "Ep 2 (Step 003235): Train loss 4.406, Val loss 4.652\n",
            "Ep 2 (Step 003240): Train loss 4.489, Val loss 4.653\n",
            "Ep 2 (Step 003245): Train loss 4.376, Val loss 4.660\n",
            "Ep 2 (Step 003250): Train loss 4.439, Val loss 4.657\n",
            "Ep 2 (Step 003255): Train loss 4.388, Val loss 4.652\n",
            "Ep 2 (Step 003260): Train loss 4.338, Val loss 4.644\n",
            "Ep 2 (Step 003265): Train loss 4.719, Val loss 4.657\n",
            "Ep 2 (Step 003270): Train loss 4.184, Val loss 4.648\n",
            "Ep 2 (Step 003275): Train loss 4.109, Val loss 4.654\n",
            "Ep 2 (Step 003280): Train loss 4.669, Val loss 4.685\n",
            "Ep 2 (Step 003285): Train loss 4.133, Val loss 4.663\n",
            "Ep 2 (Step 003290): Train loss 4.148, Val loss 4.672\n",
            "Ep 2 (Step 003295): Train loss 4.671, Val loss 4.696\n",
            "Ep 2 (Step 003300): Train loss 4.753, Val loss 4.661\n",
            "Ep 2 (Step 003305): Train loss 4.275, Val loss 4.635\n",
            "Ep 2 (Step 003310): Train loss 4.333, Val loss 4.633\n",
            "Ep 2 (Step 003315): Train loss 4.607, Val loss 4.659\n",
            "Ep 2 (Step 003320): Train loss 4.346, Val loss 4.654\n",
            "Ep 2 (Step 003325): Train loss 4.507, Val loss 4.671\n",
            "Ep 2 (Step 003330): Train loss 4.206, Val loss 4.656\n",
            "Ep 2 (Step 003335): Train loss 4.238, Val loss 4.661\n",
            "Ep 2 (Step 003340): Train loss 4.395, Val loss 4.660\n",
            "Ep 2 (Step 003345): Train loss 4.192, Val loss 4.646\n",
            "Ep 2 (Step 003350): Train loss 4.287, Val loss 4.649\n",
            "Ep 2 (Step 003355): Train loss 4.481, Val loss 4.657\n",
            "Ep 2 (Step 003360): Train loss 4.317, Val loss 4.640\n",
            "Ep 2 (Step 003365): Train loss 4.510, Val loss 4.632\n",
            "Ep 2 (Step 003370): Train loss 4.516, Val loss 4.637\n",
            "Ep 2 (Step 003375): Train loss 4.276, Val loss 4.618\n",
            "Ep 2 (Step 003380): Train loss 4.234, Val loss 4.637\n",
            "Ep 2 (Step 003385): Train loss 4.165, Val loss 4.645\n",
            "Ep 2 (Step 003390): Train loss 4.525, Val loss 4.648\n",
            "Ep 2 (Step 003395): Train loss 4.324, Val loss 4.654\n",
            "Ep 2 (Step 003400): Train loss 4.428, Val loss 4.651\n",
            "Ep 2 (Step 003405): Train loss 4.337, Val loss 4.624\n",
            "Ep 2 (Step 003410): Train loss 4.521, Val loss 4.636\n",
            "Ep 2 (Step 003415): Train loss 4.398, Val loss 4.633\n",
            "Ep 2 (Step 003420): Train loss 4.320, Val loss 4.617\n",
            "Ep 2 (Step 003425): Train loss 4.171, Val loss 4.605\n",
            "Ep 2 (Step 003430): Train loss 4.199, Val loss 4.595\n",
            "Ep 2 (Step 003435): Train loss 4.370, Val loss 4.601\n",
            "Ep 2 (Step 003440): Train loss 3.877, Val loss 4.603\n",
            "Ep 2 (Step 003445): Train loss 4.255, Val loss 4.630\n",
            "Ep 2 (Step 003450): Train loss 4.493, Val loss 4.636\n",
            "Ep 2 (Step 003455): Train loss 4.288, Val loss 4.624\n",
            "Ep 2 (Step 003460): Train loss 4.443, Val loss 4.638\n",
            "Ep 2 (Step 003465): Train loss 4.266, Val loss 4.639\n",
            "Ep 2 (Step 003470): Train loss 4.325, Val loss 4.633\n",
            "Ep 2 (Step 003475): Train loss 4.550, Val loss 4.625\n",
            "Ep 2 (Step 003480): Train loss 4.031, Val loss 4.630\n",
            "Ep 2 (Step 003485): Train loss 4.443, Val loss 4.634\n",
            "Ep 2 (Step 003490): Train loss 4.138, Val loss 4.630\n",
            "Ep 2 (Step 003495): Train loss 4.625, Val loss 4.639\n",
            "Ep 2 (Step 003500): Train loss 4.109, Val loss 4.628\n",
            "Ep 2 (Step 003505): Train loss 4.116, Val loss 4.626\n",
            "Ep 2 (Step 003510): Train loss 4.397, Val loss 4.637\n",
            "Ep 2 (Step 003515): Train loss 4.288, Val loss 4.652\n",
            "Ep 2 (Step 003520): Train loss 4.213, Val loss 4.670\n",
            "Ep 2 (Step 003525): Train loss 4.330, Val loss 4.664\n",
            "Ep 2 (Step 003530): Train loss 4.255, Val loss 4.652\n",
            "Ep 2 (Step 003535): Train loss 4.326, Val loss 4.648\n",
            "Ep 2 (Step 003540): Train loss 4.536, Val loss 4.637\n",
            "Ep 2 (Step 003545): Train loss 4.352, Val loss 4.649\n",
            "Ep 2 (Step 003550): Train loss 4.231, Val loss 4.636\n",
            "Ep 2 (Step 003555): Train loss 4.560, Val loss 4.650\n",
            "Ep 2 (Step 003560): Train loss 4.129, Val loss 4.642\n",
            "Ep 2 (Step 003565): Train loss 4.717, Val loss 4.651\n",
            "Ep 2 (Step 003570): Train loss 4.478, Val loss 4.661\n",
            "Ep 2 (Step 003575): Train loss 4.045, Val loss 4.646\n",
            "Ep 2 (Step 003580): Train loss 4.197, Val loss 4.629\n",
            "Ep 2 (Step 003585): Train loss 4.150, Val loss 4.632\n",
            "Ep 2 (Step 003590): Train loss 4.392, Val loss 4.636\n",
            "Ep 2 (Step 003595): Train loss 4.447, Val loss 4.640\n",
            "Ep 2 (Step 003600): Train loss 4.808, Val loss 4.633\n",
            "Ep 2 (Step 003605): Train loss 4.270, Val loss 4.640\n",
            "Ep 2 (Step 003610): Train loss 4.427, Val loss 4.630\n",
            "Ep 2 (Step 003615): Train loss 4.334, Val loss 4.635\n",
            "Ep 2 (Step 003620): Train loss 4.316, Val loss 4.608\n",
            "Ep 2 (Step 003625): Train loss 4.335, Val loss 4.593\n",
            "Ep 2 (Step 003630): Train loss 4.380, Val loss 4.586\n",
            "Ep 2 (Step 003635): Train loss 4.447, Val loss 4.577\n",
            "Ep 2 (Step 003640): Train loss 4.276, Val loss 4.576\n",
            "Ep 2 (Step 003645): Train loss 4.271, Val loss 4.604\n",
            "Ep 2 (Step 003650): Train loss 4.564, Val loss 4.609\n",
            "Ep 2 (Step 003655): Train loss 4.253, Val loss 4.592\n",
            "Ep 2 (Step 003660): Train loss 4.130, Val loss 4.599\n",
            "Ep 2 (Step 003665): Train loss 4.516, Val loss 4.598\n",
            "Ep 2 (Step 003670): Train loss 4.217, Val loss 4.611\n",
            "Ep 2 (Step 003675): Train loss 4.451, Val loss 4.614\n",
            "Ep 2 (Step 003680): Train loss 4.447, Val loss 4.610\n",
            "Ep 2 (Step 003685): Train loss 4.336, Val loss 4.599\n",
            "Ep 2 (Step 003690): Train loss 4.175, Val loss 4.598\n",
            "Ep 2 (Step 003695): Train loss 4.208, Val loss 4.614\n",
            "Ep 2 (Step 003700): Train loss 4.528, Val loss 4.620\n",
            "Ep 2 (Step 003705): Train loss 4.083, Val loss 4.615\n",
            "Ep 2 (Step 003710): Train loss 4.682, Val loss 4.614\n",
            "Ep 2 (Step 003715): Train loss 4.282, Val loss 4.616\n",
            "Ep 2 (Step 003720): Train loss 4.385, Val loss 4.607\n",
            "Ep 2 (Step 003725): Train loss 4.671, Val loss 4.622\n",
            "Ep 2 (Step 003730): Train loss 4.336, Val loss 4.619\n",
            "Ep 2 (Step 003735): Train loss 4.003, Val loss 4.589\n",
            "Ep 2 (Step 003740): Train loss 4.340, Val loss 4.575\n",
            "Ep 2 (Step 003745): Train loss 4.511, Val loss 4.586\n",
            "Ep 2 (Step 003750): Train loss 4.192, Val loss 4.603\n",
            "Ep 2 (Step 003755): Train loss 4.295, Val loss 4.605\n",
            "Ep 2 (Step 003760): Train loss 4.568, Val loss 4.597\n",
            "Ep 2 (Step 003765): Train loss 4.392, Val loss 4.595\n",
            "Ep 2 (Step 003770): Train loss 4.457, Val loss 4.593\n",
            "Ep 2 (Step 003775): Train loss 4.066, Val loss 4.601\n",
            "Ep 2 (Step 003780): Train loss 4.098, Val loss 4.622\n",
            "Ep 2 (Step 003785): Train loss 4.357, Val loss 4.604\n",
            "Ep 2 (Step 003790): Train loss 4.358, Val loss 4.595\n",
            "Ep 2 (Step 003795): Train loss 4.412, Val loss 4.613\n",
            "Ep 2 (Step 003800): Train loss 4.215, Val loss 4.629\n",
            "Ep 2 (Step 003805): Train loss 4.215, Val loss 4.590\n",
            "Ep 2 (Step 003810): Train loss 4.435, Val loss 4.584\n",
            "Ep 2 (Step 003815): Train loss 4.153, Val loss 4.585\n",
            "Ep 2 (Step 003820): Train loss 4.253, Val loss 4.604\n",
            "Ep 2 (Step 003825): Train loss 4.403, Val loss 4.609\n",
            "Ep 2 (Step 003830): Train loss 4.255, Val loss 4.581\n",
            "Ep 2 (Step 003835): Train loss 4.391, Val loss 4.588\n",
            "Ep 2 (Step 003840): Train loss 4.447, Val loss 4.586\n",
            "Ep 2 (Step 003845): Train loss 4.403, Val loss 4.573\n",
            "Ep 2 (Step 003850): Train loss 4.163, Val loss 4.586\n",
            "Ep 2 (Step 003855): Train loss 4.284, Val loss 4.609\n",
            "Ep 2 (Step 003860): Train loss 4.234, Val loss 4.589\n",
            "Ep 2 (Step 003865): Train loss 4.237, Val loss 4.589\n",
            "Ep 2 (Step 003870): Train loss 4.145, Val loss 4.601\n",
            "Ep 2 (Step 003875): Train loss 4.212, Val loss 4.569\n",
            "Ep 2 (Step 003880): Train loss 4.184, Val loss 4.585\n",
            "Ep 2 (Step 003885): Train loss 4.509, Val loss 4.609\n",
            "Ep 2 (Step 003890): Train loss 4.360, Val loss 4.595\n",
            "Ep 2 (Step 003895): Train loss 4.358, Val loss 4.594\n",
            "Ep 2 (Step 003900): Train loss 4.274, Val loss 4.599\n",
            "Ep 2 (Step 003905): Train loss 4.296, Val loss 4.581\n",
            "Ep 2 (Step 003910): Train loss 4.186, Val loss 4.563\n",
            "Ep 2 (Step 003915): Train loss 4.415, Val loss 4.556\n",
            "Ep 2 (Step 003920): Train loss 4.035, Val loss 4.557\n",
            "Ep 2 (Step 003925): Train loss 4.148, Val loss 4.553\n",
            "Ep 2 (Step 003930): Train loss 4.187, Val loss 4.554\n",
            "Ep 2 (Step 003935): Train loss 4.376, Val loss 4.566\n",
            "Ep 2 (Step 003940): Train loss 4.287, Val loss 4.569\n",
            "Ep 2 (Step 003945): Train loss 3.892, Val loss 4.548\n",
            "Ep 2 (Step 003950): Train loss 4.253, Val loss 4.547\n",
            "Ep 2 (Step 003955): Train loss 4.178, Val loss 4.551\n",
            "Ep 2 (Step 003960): Train loss 3.776, Val loss 4.539\n",
            "Ep 2 (Step 003965): Train loss 4.193, Val loss 4.528\n",
            "Ep 2 (Step 003970): Train loss 4.348, Val loss 4.528\n",
            "Ep 2 (Step 003975): Train loss 4.114, Val loss 4.539\n",
            "Ep 2 (Step 003980): Train loss 4.217, Val loss 4.553\n",
            "Ep 2 (Step 003985): Train loss 3.989, Val loss 4.555\n",
            "Ep 2 (Step 003990): Train loss 4.498, Val loss 4.545\n",
            "Ep 2 (Step 003995): Train loss 4.536, Val loss 4.535\n",
            "Ep 2 (Step 004000): Train loss 3.975, Val loss 4.540\n",
            "Ep 2 (Step 004005): Train loss 4.093, Val loss 4.550\n",
            "Ep 2 (Step 004010): Train loss 4.449, Val loss 4.554\n",
            "Ep 2 (Step 004015): Train loss 4.168, Val loss 4.555\n",
            "Ep 2 (Step 004020): Train loss 4.094, Val loss 4.561\n",
            "Ep 2 (Step 004025): Train loss 4.238, Val loss 4.554\n",
            "Ep 2 (Step 004030): Train loss 4.640, Val loss 4.546\n",
            "Ep 2 (Step 004035): Train loss 4.489, Val loss 4.550\n",
            "Ep 2 (Step 004040): Train loss 4.068, Val loss 4.551\n",
            "Ep 2 (Step 004045): Train loss 4.312, Val loss 4.548\n",
            "Ep 2 (Step 004050): Train loss 4.226, Val loss 4.548\n",
            "Ep 2 (Step 004055): Train loss 4.497, Val loss 4.555\n",
            "Ep 2 (Step 004060): Train loss 4.267, Val loss 4.544\n",
            "Ep 2 (Step 004065): Train loss 4.577, Val loss 4.543\n",
            "Ep 2 (Step 004070): Train loss 4.399, Val loss 4.559\n",
            "Ep 2 (Step 004075): Train loss 4.286, Val loss 4.583\n",
            "Ep 2 (Step 004080): Train loss 4.220, Val loss 4.589\n",
            "Ep 2 (Step 004085): Train loss 4.240, Val loss 4.571\n",
            "Ep 2 (Step 004090): Train loss 4.395, Val loss 4.551\n",
            "Ep 2 (Step 004095): Train loss 4.248, Val loss 4.548\n",
            "Ep 2 (Step 004100): Train loss 4.306, Val loss 4.550\n",
            "Ep 2 (Step 004105): Train loss 4.541, Val loss 4.557\n",
            "Ep 2 (Step 004110): Train loss 4.153, Val loss 4.563\n",
            "Ep 2 (Step 004115): Train loss 4.044, Val loss 4.583\n",
            "Ep 2 (Step 004120): Train loss 4.214, Val loss 4.569\n",
            "Ep 2 (Step 004125): Train loss 4.458, Val loss 4.547\n",
            "Ep 2 (Step 004130): Train loss 4.217, Val loss 4.550\n",
            "Ep 2 (Step 004135): Train loss 4.378, Val loss 4.565\n",
            "Ep 2 (Step 004140): Train loss 3.982, Val loss 4.562\n",
            "Ep 2 (Step 004145): Train loss 4.359, Val loss 4.560\n",
            "Ep 2 (Step 004150): Train loss 4.182, Val loss 4.557\n",
            "Ep 2 (Step 004155): Train loss 4.763, Val loss 4.568\n",
            "Ep 2 (Step 004160): Train loss 3.878, Val loss 4.568\n",
            "Ep 2 (Step 004165): Train loss 4.141, Val loss 4.569\n",
            "Ep 2 (Step 004170): Train loss 4.113, Val loss 4.568\n",
            "Ep 2 (Step 004175): Train loss 4.283, Val loss 4.570\n",
            "Ep 2 (Step 004180): Train loss 4.171, Val loss 4.575\n",
            "Ep 2 (Step 004185): Train loss 4.092, Val loss 4.573\n",
            "Ep 2 (Step 004190): Train loss 4.368, Val loss 4.566\n",
            "Ep 2 (Step 004195): Train loss 4.197, Val loss 4.574\n",
            "Ep 2 (Step 004200): Train loss 4.220, Val loss 4.583\n",
            "Ep 2 (Step 004205): Train loss 4.484, Val loss 4.575\n",
            "Ep 2 (Step 004210): Train loss 4.111, Val loss 4.563\n",
            "Ep 2 (Step 004215): Train loss 4.446, Val loss 4.547\n",
            "Ep 2 (Step 004220): Train loss 4.564, Val loss 4.555\n",
            "Ep 2 (Step 004225): Train loss 4.033, Val loss 4.563\n",
            "Ep 2 (Step 004230): Train loss 4.215, Val loss 4.545\n",
            "Ep 2 (Step 004235): Train loss 4.185, Val loss 4.514\n",
            "Ep 2 (Step 004240): Train loss 4.054, Val loss 4.502\n",
            "Ep 2 (Step 004245): Train loss 4.339, Val loss 4.500\n",
            "Ep 2 (Step 004250): Train loss 4.093, Val loss 4.511\n",
            "Ep 2 (Step 004255): Train loss 4.441, Val loss 4.524\n",
            "Ep 2 (Step 004260): Train loss 4.120, Val loss 4.533\n",
            "Ep 2 (Step 004265): Train loss 4.503, Val loss 4.526\n",
            "Ep 2 (Step 004270): Train loss 4.369, Val loss 4.528\n",
            "Ep 2 (Step 004275): Train loss 4.657, Val loss 4.542\n",
            "Ep 2 (Step 004280): Train loss 4.196, Val loss 4.522\n",
            "Ep 2 (Step 004285): Train loss 4.383, Val loss 4.523\n",
            "Ep 2 (Step 004290): Train loss 4.318, Val loss 4.532\n",
            "Ep 2 (Step 004295): Train loss 4.110, Val loss 4.516\n",
            "Ep 2 (Step 004300): Train loss 4.542, Val loss 4.504\n",
            "Ep 2 (Step 004305): Train loss 3.884, Val loss 4.515\n",
            "Ep 2 (Step 004310): Train loss 4.306, Val loss 4.542\n",
            "Ep 2 (Step 004315): Train loss 4.142, Val loss 4.544\n",
            "Ep 2 (Step 004320): Train loss 4.466, Val loss 4.531\n",
            "Ep 2 (Step 004325): Train loss 4.550, Val loss 4.528\n",
            "Ep 2 (Step 004330): Train loss 3.978, Val loss 4.529\n",
            "Ep 2 (Step 004335): Train loss 4.346, Val loss 4.516\n",
            "Ep 2 (Step 004340): Train loss 4.230, Val loss 4.518\n",
            "Ep 2 (Step 004345): Train loss 4.026, Val loss 4.529\n",
            "Ep 2 (Step 004350): Train loss 4.306, Val loss 4.519\n",
            "Ep 2 (Step 004355): Train loss 4.250, Val loss 4.532\n",
            "Ep 2 (Step 004360): Train loss 4.196, Val loss 4.532\n",
            "Ep 2 (Step 004365): Train loss 4.342, Val loss 4.522\n",
            "Ep 2 (Step 004370): Train loss 4.001, Val loss 4.523\n",
            "Ep 2 (Step 004375): Train loss 4.183, Val loss 4.518\n",
            "Ep 2 (Step 004380): Train loss 4.537, Val loss 4.519\n",
            "Ep 2 (Step 004385): Train loss 4.126, Val loss 4.531\n",
            "Ep 2 (Step 004390): Train loss 4.210, Val loss 4.535\n",
            "Ep 2 (Step 004395): Train loss 4.289, Val loss 4.526\n",
            "Ep 2 (Step 004400): Train loss 4.276, Val loss 4.508\n",
            "Ep 2 (Step 004405): Train loss 4.122, Val loss 4.509\n",
            "Ep 2 (Step 004410): Train loss 4.029, Val loss 4.491\n",
            "Ep 2 (Step 004415): Train loss 4.238, Val loss 4.499\n",
            "Ep 2 (Step 004420): Train loss 4.581, Val loss 4.497\n",
            "Ep 2 (Step 004425): Train loss 4.330, Val loss 4.488\n",
            "Ep 2 (Step 004430): Train loss 4.149, Val loss 4.490\n",
            "Ep 2 (Step 004435): Train loss 4.081, Val loss 4.489\n",
            "Ep 2 (Step 004440): Train loss 4.484, Val loss 4.466\n",
            "Ep 2 (Step 004445): Train loss 3.973, Val loss 4.464\n",
            "Ep 2 (Step 004450): Train loss 4.274, Val loss 4.482\n",
            "Ep 2 (Step 004455): Train loss 4.404, Val loss 4.514\n",
            "Ep 2 (Step 004460): Train loss 4.353, Val loss 4.488\n",
            "Ep 2 (Step 004465): Train loss 4.117, Val loss 4.487\n",
            "Ep 2 (Step 004470): Train loss 4.142, Val loss 4.492\n",
            "Ep 2 (Step 004475): Train loss 4.257, Val loss 4.498\n",
            "Ep 2 (Step 004480): Train loss 4.182, Val loss 4.492\n",
            "Ep 2 (Step 004485): Train loss 4.182, Val loss 4.488\n",
            "Ep 2 (Step 004490): Train loss 4.314, Val loss 4.496\n",
            "Ep 2 (Step 004495): Train loss 4.347, Val loss 4.492\n",
            "Ep 2 (Step 004500): Train loss 3.937, Val loss 4.502\n",
            "Ep 2 (Step 004505): Train loss 4.398, Val loss 4.490\n",
            "Ep 2 (Step 004510): Train loss 4.304, Val loss 4.459\n",
            "Ep 2 (Step 004515): Train loss 4.048, Val loss 4.469\n",
            "Ep 2 (Step 004520): Train loss 4.125, Val loss 4.477\n",
            "Ep 2 (Step 004525): Train loss 4.315, Val loss 4.471\n",
            "Ep 2 (Step 004530): Train loss 4.277, Val loss 4.483\n",
            "Ep 2 (Step 004535): Train loss 4.167, Val loss 4.481\n",
            "Ep 2 (Step 004540): Train loss 4.247, Val loss 4.481\n",
            "Ep 2 (Step 004545): Train loss 4.479, Val loss 4.475\n",
            "Ep 2 (Step 004550): Train loss 4.090, Val loss 4.478\n",
            "Ep 2 (Step 004555): Train loss 4.518, Val loss 4.474\n",
            "Ep 2 (Step 004560): Train loss 4.105, Val loss 4.470\n",
            "Ep 2 (Step 004565): Train loss 4.165, Val loss 4.464\n",
            "Ep 2 (Step 004570): Train loss 3.944, Val loss 4.473\n",
            "Ep 2 (Step 004575): Train loss 4.159, Val loss 4.463\n",
            "Ep 2 (Step 004580): Train loss 3.886, Val loss 4.466\n",
            "Ep 2 (Step 004585): Train loss 4.061, Val loss 4.463\n",
            "Ep 2 (Step 004590): Train loss 4.171, Val loss 4.449\n",
            "Ep 2 (Step 004595): Train loss 3.964, Val loss 4.467\n",
            "Ep 2 (Step 004600): Train loss 3.978, Val loss 4.476\n",
            "Ep 2 (Step 004605): Train loss 4.094, Val loss 4.478\n",
            "Ep 2 (Step 004610): Train loss 4.054, Val loss 4.456\n",
            "Ep 2 (Step 004615): Train loss 4.045, Val loss 4.459\n",
            "Ep 2 (Step 004620): Train loss 4.229, Val loss 4.468\n",
            "Ep 2 (Step 004625): Train loss 3.914, Val loss 4.456\n",
            "Ep 2 (Step 004630): Train loss 4.392, Val loss 4.463\n",
            "Ep 2 (Step 004635): Train loss 4.290, Val loss 4.479\n",
            "Ep 2 (Step 004640): Train loss 4.023, Val loss 4.474\n",
            "Ep 2 (Step 004645): Train loss 4.159, Val loss 4.473\n",
            "Ep 2 (Step 004650): Train loss 4.056, Val loss 4.478\n",
            "Ep 2 (Step 004655): Train loss 4.342, Val loss 4.478\n",
            "Ep 2 (Step 004660): Train loss 4.162, Val loss 4.463\n",
            "Ep 2 (Step 004665): Train loss 4.386, Val loss 4.459\n",
            "Ep 2 (Step 004670): Train loss 4.024, Val loss 4.459\n",
            "Ep 2 (Step 004675): Train loss 4.222, Val loss 4.463\n",
            "Ep 2 (Step 004680): Train loss 4.456, Val loss 4.476\n",
            "Ep 2 (Step 004685): Train loss 4.325, Val loss 4.489\n",
            "Ep 2 (Step 004690): Train loss 4.253, Val loss 4.489\n",
            "Ep 2 (Step 004695): Train loss 4.038, Val loss 4.480\n",
            "Ep 2 (Step 004700): Train loss 4.165, Val loss 4.482\n",
            "Ep 2 (Step 004705): Train loss 4.620, Val loss 4.458\n",
            "Ep 2 (Step 004710): Train loss 4.033, Val loss 4.451\n",
            "Ep 2 (Step 004715): Train loss 4.045, Val loss 4.468\n",
            "Ep 2 (Step 004720): Train loss 4.016, Val loss 4.489\n",
            "Ep 2 (Step 004725): Train loss 3.912, Val loss 4.487\n",
            "Ep 2 (Step 004730): Train loss 4.327, Val loss 4.479\n",
            "Ep 2 (Step 004735): Train loss 4.001, Val loss 4.472\n",
            "Ep 2 (Step 004740): Train loss 4.021, Val loss 4.485\n",
            "Ep 2 (Step 004745): Train loss 4.352, Val loss 4.481\n",
            "Ep 2 (Step 004750): Train loss 4.127, Val loss 4.497\n",
            "Ep 2 (Step 004755): Train loss 4.223, Val loss 4.491\n",
            "Ep 2 (Step 004760): Train loss 4.458, Val loss 4.491\n",
            "Ep 2 (Step 004765): Train loss 4.325, Val loss 4.494\n",
            "Ep 2 (Step 004770): Train loss 4.652, Val loss 4.505\n",
            "Ep 2 (Step 004775): Train loss 4.204, Val loss 4.507\n",
            "Ep 2 (Step 004780): Train loss 4.204, Val loss 4.485\n",
            "Ep 2 (Step 004785): Train loss 4.001, Val loss 4.488\n",
            "Ep 2 (Step 004790): Train loss 3.937, Val loss 4.492\n",
            "Ep 2 (Step 004795): Train loss 4.508, Val loss 4.480\n",
            "Ep 2 (Step 004800): Train loss 4.270, Val loss 4.475\n",
            "Ep 2 (Step 004805): Train loss 4.331, Val loss 4.472\n",
            "Ep 2 (Step 004810): Train loss 4.221, Val loss 4.463\n",
            "Ep 2 (Step 004815): Train loss 3.935, Val loss 4.473\n",
            "Ep 2 (Step 004820): Train loss 4.287, Val loss 4.483\n",
            "Ep 2 (Step 004825): Train loss 4.576, Val loss 4.495\n",
            "Ep 2 (Step 004830): Train loss 4.412, Val loss 4.492\n",
            "Ep 2 (Step 004835): Train loss 4.442, Val loss 4.481\n",
            "Ep 2 (Step 004840): Train loss 4.137, Val loss 4.477\n",
            "Ep 2 (Step 004845): Train loss 4.093, Val loss 4.474\n",
            "Ep 2 (Step 004850): Train loss 4.336, Val loss 4.461\n",
            "Ep 2 (Step 004855): Train loss 4.246, Val loss 4.452\n",
            "Ep 2 (Step 004860): Train loss 4.591, Val loss 4.458\n",
            "Ep 2 (Step 004865): Train loss 3.980, Val loss 4.486\n",
            "Ep 2 (Step 004870): Train loss 4.023, Val loss 4.511\n",
            "Ep 2 (Step 004875): Train loss 4.255, Val loss 4.506\n",
            "Ep 2 (Step 004880): Train loss 4.293, Val loss 4.483\n",
            "Ep 2 (Step 004885): Train loss 4.171, Val loss 4.500\n",
            "Ep 2 (Step 004890): Train loss 3.886, Val loss 4.502\n",
            "Ep 2 (Step 004895): Train loss 4.019, Val loss 4.509\n",
            "Ep 2 (Step 004900): Train loss 3.728, Val loss 4.488\n",
            "Ep 2 (Step 004905): Train loss 4.229, Val loss 4.478\n",
            "Ep 2 (Step 004910): Train loss 4.079, Val loss 4.478\n",
            "Ep 2 (Step 004915): Train loss 4.045, Val loss 4.471\n",
            "Ep 2 (Step 004920): Train loss 4.284, Val loss 4.474\n",
            "Ep 2 (Step 004925): Train loss 4.038, Val loss 4.479\n",
            "Ep 2 (Step 004930): Train loss 4.093, Val loss 4.474\n",
            "Ep 2 (Step 004935): Train loss 3.979, Val loss 4.473\n",
            "Ep 2 (Step 004940): Train loss 4.290, Val loss 4.473\n",
            "Ep 2 (Step 004945): Train loss 4.088, Val loss 4.464\n",
            "Ep 2 (Step 004950): Train loss 4.613, Val loss 4.467\n",
            "Ep 2 (Step 004955): Train loss 4.385, Val loss 4.446\n",
            "Ep 2 (Step 004960): Train loss 4.198, Val loss 4.437\n",
            "Ep 2 (Step 004965): Train loss 4.013, Val loss 4.445\n",
            "Ep 2 (Step 004970): Train loss 4.048, Val loss 4.437\n",
            "Ep 2 (Step 004975): Train loss 4.301, Val loss 4.431\n",
            "Ep 2 (Step 004980): Train loss 4.236, Val loss 4.422\n",
            "Ep 2 (Step 004985): Train loss 4.205, Val loss 4.418\n",
            "Ep 2 (Step 004990): Train loss 3.932, Val loss 4.422\n",
            "Ep 2 (Step 004995): Train loss 4.017, Val loss 4.416\n",
            "Ep 2 (Step 005000): Train loss 4.164, Val loss 4.425\n",
            "Ep 2 (Step 005005): Train loss 4.439, Val loss 4.439\n",
            "Ep 2 (Step 005010): Train loss 4.127, Val loss 4.421\n",
            "Ep 2 (Step 005015): Train loss 3.961, Val loss 4.418\n",
            "Ep 2 (Step 005020): Train loss 4.120, Val loss 4.422\n",
            "Ep 2 (Step 005025): Train loss 3.956, Val loss 4.424\n",
            "Ep 2 (Step 005030): Train loss 4.016, Val loss 4.437\n",
            "Ep 2 (Step 005035): Train loss 4.140, Val loss 4.433\n",
            "Ep 2 (Step 005040): Train loss 4.269, Val loss 4.439\n",
            "Ep 2 (Step 005045): Train loss 4.061, Val loss 4.448\n",
            "Ep 2 (Step 005050): Train loss 4.058, Val loss 4.422\n",
            "Ep 2 (Step 005055): Train loss 3.845, Val loss 4.419\n",
            "Ep 2 (Step 005060): Train loss 4.022, Val loss 4.407\n",
            "Ep 2 (Step 005065): Train loss 3.948, Val loss 4.395\n",
            "Ep 2 (Step 005070): Train loss 4.320, Val loss 4.390\n",
            "Ep 2 (Step 005075): Train loss 3.930, Val loss 4.403\n",
            "Ep 2 (Step 005080): Train loss 3.912, Val loss 4.413\n",
            "Ep 2 (Step 005085): Train loss 4.409, Val loss 4.413\n",
            "Ep 2 (Step 005090): Train loss 4.090, Val loss 4.416\n",
            "Ep 2 (Step 005095): Train loss 4.105, Val loss 4.422\n",
            "Ep 2 (Step 005100): Train loss 4.102, Val loss 4.451\n",
            "Ep 2 (Step 005105): Train loss 4.054, Val loss 4.446\n",
            "Ep 2 (Step 005110): Train loss 4.229, Val loss 4.425\n",
            "Ep 2 (Step 005115): Train loss 4.079, Val loss 4.428\n",
            "Ep 2 (Step 005120): Train loss 4.029, Val loss 4.432\n",
            "Ep 2 (Step 005125): Train loss 4.274, Val loss 4.450\n",
            "Ep 2 (Step 005130): Train loss 3.972, Val loss 4.469\n",
            "Ep 2 (Step 005135): Train loss 4.204, Val loss 4.461\n",
            "Ep 2 (Step 005140): Train loss 4.285, Val loss 4.448\n",
            "Ep 2 (Step 005145): Train loss 4.207, Val loss 4.455\n",
            "Ep 2 (Step 005150): Train loss 4.177, Val loss 4.440\n",
            "Ep 2 (Step 005155): Train loss 4.310, Val loss 4.443\n",
            "Ep 2 (Step 005160): Train loss 4.142, Val loss 4.436\n",
            "Ep 2 (Step 005165): Train loss 4.268, Val loss 4.435\n",
            "Ep 2 (Step 005170): Train loss 3.854, Val loss 4.439\n",
            "Ep 2 (Step 005175): Train loss 4.367, Val loss 4.429\n",
            "Ep 2 (Step 005180): Train loss 4.200, Val loss 4.412\n",
            "Ep 2 (Step 005185): Train loss 3.713, Val loss 4.415\n",
            "Ep 2 (Step 005190): Train loss 4.206, Val loss 4.422\n",
            "Ep 2 (Step 005195): Train loss 4.039, Val loss 4.441\n",
            "Ep 2 (Step 005200): Train loss 4.287, Val loss 4.450\n",
            "Ep 2 (Step 005205): Train loss 4.134, Val loss 4.451\n",
            "Ep 2 (Step 005210): Train loss 4.148, Val loss 4.448\n",
            "Ep 2 (Step 005215): Train loss 4.026, Val loss 4.439\n",
            "Ep 2 (Step 005220): Train loss 3.936, Val loss 4.436\n",
            "Ep 2 (Step 005225): Train loss 4.143, Val loss 4.417\n",
            "Ep 2 (Step 005230): Train loss 4.212, Val loss 4.420\n",
            "Ep 2 (Step 005235): Train loss 4.056, Val loss 4.434\n",
            "Ep 2 (Step 005240): Train loss 4.145, Val loss 4.424\n",
            "Ep 2 (Step 005245): Train loss 3.885, Val loss 4.432\n",
            "Ep 2 (Step 005250): Train loss 4.189, Val loss 4.454\n",
            "Ep 2 (Step 005255): Train loss 4.392, Val loss 4.443\n",
            "Ep 2 (Step 005260): Train loss 4.139, Val loss 4.429\n",
            "Ep 2 (Step 005265): Train loss 4.335, Val loss 4.420\n",
            "Ep 2 (Step 005270): Train loss 3.811, Val loss 4.417\n",
            "Ep 2 (Step 005275): Train loss 4.255, Val loss 4.420\n",
            "Ep 2 (Step 005280): Train loss 4.194, Val loss 4.427\n",
            "Ep 2 (Step 005285): Train loss 3.727, Val loss 4.425\n",
            "Ep 2 (Step 005290): Train loss 4.146, Val loss 4.435\n",
            "Ep 2 (Step 005295): Train loss 3.906, Val loss 4.443\n",
            "Ep 2 (Step 005300): Train loss 3.922, Val loss 4.453\n",
            "Ep 2 (Step 005305): Train loss 4.471, Val loss 4.456\n",
            "Ep 2 (Step 005310): Train loss 4.401, Val loss 4.444\n",
            "Ep 2 (Step 005315): Train loss 4.032, Val loss 4.441\n",
            "Ep 2 (Step 005320): Train loss 3.885, Val loss 4.441\n",
            "Ep 2 (Step 005325): Train loss 3.943, Val loss 4.446\n",
            "Ep 2 (Step 005330): Train loss 4.107, Val loss 4.453\n",
            "Ep 2 (Step 005335): Train loss 4.028, Val loss 4.439\n",
            "Ep 2 (Step 005340): Train loss 4.114, Val loss 4.448\n",
            "Ep 2 (Step 005345): Train loss 4.132, Val loss 4.440\n",
            "Ep 2 (Step 005350): Train loss 4.152, Val loss 4.431\n",
            "Ep 2 (Step 005355): Train loss 4.194, Val loss 4.419\n",
            "Ep 2 (Step 005360): Train loss 4.294, Val loss 4.413\n",
            "Ep 2 (Step 005365): Train loss 3.926, Val loss 4.422\n",
            "Ep 2 (Step 005370): Train loss 4.181, Val loss 4.433\n",
            "Ep 2 (Step 005375): Train loss 4.270, Val loss 4.432\n",
            "Ep 2 (Step 005380): Train loss 4.437, Val loss 4.418\n",
            "Ep 2 (Step 005385): Train loss 4.248, Val loss 4.420\n",
            "Ep 2 (Step 005390): Train loss 4.028, Val loss 4.422\n",
            "Ep 2 (Step 005395): Train loss 4.094, Val loss 4.426\n",
            "Ep 2 (Step 005400): Train loss 4.333, Val loss 4.442\n",
            "Ep 2 (Step 005405): Train loss 4.097, Val loss 4.459\n",
            "Ep 2 (Step 005410): Train loss 4.202, Val loss 4.441\n",
            "Ep 2 (Step 005415): Train loss 4.020, Val loss 4.444\n",
            "Ep 2 (Step 005420): Train loss 3.811, Val loss 4.455\n",
            "Ep 2 (Step 005425): Train loss 4.126, Val loss 4.482\n",
            "Ep 2 (Step 005430): Train loss 4.044, Val loss 4.496\n",
            "Ep 2 (Step 005435): Train loss 4.236, Val loss 4.491\n",
            "Ep 2 (Step 005440): Train loss 3.949, Val loss 4.468\n",
            "Ep 2 (Step 005445): Train loss 3.925, Val loss 4.463\n",
            "Ep 2 (Step 005450): Train loss 4.244, Val loss 4.470\n",
            "Ep 2 (Step 005455): Train loss 3.957, Val loss 4.456\n",
            "Ep 2 (Step 005460): Train loss 3.905, Val loss 4.452\n",
            "Ep 2 (Step 005465): Train loss 4.180, Val loss 4.455\n",
            "Ep 2 (Step 005470): Train loss 4.044, Val loss 4.451\n",
            "Ep 2 (Step 005475): Train loss 4.209, Val loss 4.451\n",
            "Ep 2 (Step 005480): Train loss 4.359, Val loss 4.449\n",
            "Ep 2 (Step 005485): Train loss 4.082, Val loss 4.447\n",
            "Ep 2 (Step 005490): Train loss 4.312, Val loss 4.461\n",
            "Ep 2 (Step 005495): Train loss 3.997, Val loss 4.457\n",
            "Ep 2 (Step 005500): Train loss 4.070, Val loss 4.457\n",
            "Ep 2 (Step 005505): Train loss 4.297, Val loss 4.465\n",
            "Ep 2 (Step 005510): Train loss 3.949, Val loss 4.460\n",
            "Ep 2 (Step 005515): Train loss 4.024, Val loss 4.464\n",
            "Ep 2 (Step 005520): Train loss 4.042, Val loss 4.474\n",
            "Ep 2 (Step 005525): Train loss 3.774, Val loss 4.471\n",
            "Ep 2 (Step 005530): Train loss 4.070, Val loss 4.440\n",
            "Ep 2 (Step 005535): Train loss 3.986, Val loss 4.422\n",
            "Ep 2 (Step 005540): Train loss 4.049, Val loss 4.421\n",
            "Ep 2 (Step 005545): Train loss 3.887, Val loss 4.416\n",
            "Ep 2 (Step 005550): Train loss 4.204, Val loss 4.416\n",
            "Ep 2 (Step 005555): Train loss 4.095, Val loss 4.423\n",
            "Ep 2 (Step 005560): Train loss 3.801, Val loss 4.437\n",
            "Ep 2 (Step 005565): Train loss 4.077, Val loss 4.440\n",
            "Ep 2 (Step 005570): Train loss 4.168, Val loss 4.439\n",
            "Ep 2 (Step 005575): Train loss 4.063, Val loss 4.428\n",
            "Ep 2 (Step 005580): Train loss 3.956, Val loss 4.427\n",
            "Ep 2 (Step 005585): Train loss 4.276, Val loss 4.431\n",
            "Ep 2 (Step 005590): Train loss 3.900, Val loss 4.410\n",
            "Ep 2 (Step 005595): Train loss 4.098, Val loss 4.406\n",
            "Ep 2 (Step 005600): Train loss 4.115, Val loss 4.409\n",
            "Ep 2 (Step 005605): Train loss 4.165, Val loss 4.418\n",
            "Ep 2 (Step 005610): Train loss 4.127, Val loss 4.416\n",
            "Ep 2 (Step 005615): Train loss 4.010, Val loss 4.406\n",
            "Ep 2 (Step 005620): Train loss 4.409, Val loss 4.402\n",
            "Ep 2 (Step 005625): Train loss 4.080, Val loss 4.408\n",
            "Ep 2 (Step 005630): Train loss 3.712, Val loss 4.405\n",
            "Ep 2 (Step 005635): Train loss 3.991, Val loss 4.391\n",
            "Ep 2 (Step 005640): Train loss 4.170, Val loss 4.385\n",
            "Ep 2 (Step 005645): Train loss 4.071, Val loss 4.398\n",
            "Ep 2 (Step 005650): Train loss 4.064, Val loss 4.394\n",
            "Ep 2 (Step 005655): Train loss 3.997, Val loss 4.403\n",
            "Ep 2 (Step 005660): Train loss 4.271, Val loss 4.412\n",
            "Ep 2 (Step 005665): Train loss 3.897, Val loss 4.395\n",
            "Ep 2 (Step 005670): Train loss 4.120, Val loss 4.399\n",
            "Ep 2 (Step 005675): Train loss 4.116, Val loss 4.394\n",
            "Ep 2 (Step 005680): Train loss 4.010, Val loss 4.393\n",
            "Ep 2 (Step 005685): Train loss 4.270, Val loss 4.384\n",
            "Ep 2 (Step 005690): Train loss 3.841, Val loss 4.390\n",
            "Ep 2 (Step 005695): Train loss 4.091, Val loss 4.387\n",
            "Ep 2 (Step 005700): Train loss 4.261, Val loss 4.410\n",
            "Ep 2 (Step 005705): Train loss 3.993, Val loss 4.432\n",
            "Ep 2 (Step 005710): Train loss 4.026, Val loss 4.416\n",
            "Ep 2 (Step 005715): Train loss 4.179, Val loss 4.411\n",
            "Ep 2 (Step 005720): Train loss 3.912, Val loss 4.432\n",
            "Ep 2 (Step 005725): Train loss 3.687, Val loss 4.403\n",
            "Ep 2 (Step 005730): Train loss 4.026, Val loss 4.412\n",
            "Ep 2 (Step 005735): Train loss 4.025, Val loss 4.410\n",
            "Ep 2 (Step 005740): Train loss 4.008, Val loss 4.400\n",
            "Ep 2 (Step 005745): Train loss 3.960, Val loss 4.378\n",
            "Ep 2 (Step 005750): Train loss 3.894, Val loss 4.380\n",
            "Ep 2 (Step 005755): Train loss 4.140, Val loss 4.387\n",
            "Ep 2 (Step 005760): Train loss 4.076, Val loss 4.382\n",
            "Ep 2 (Step 005765): Train loss 3.968, Val loss 4.404\n",
            "Ep 2 (Step 005770): Train loss 4.171, Val loss 4.407\n",
            "Ep 2 (Step 005775): Train loss 3.978, Val loss 4.398\n",
            "Ep 2 (Step 005780): Train loss 4.171, Val loss 4.386\n",
            "Ep 2 (Step 005785): Train loss 4.010, Val loss 4.393\n",
            "Ep 2 (Step 005790): Train loss 3.985, Val loss 4.386\n",
            "Ep 2 (Step 005795): Train loss 3.963, Val loss 4.377\n",
            "Ep 2 (Step 005800): Train loss 4.249, Val loss 4.389\n",
            "Ep 2 (Step 005805): Train loss 3.698, Val loss 4.401\n",
            "Ep 2 (Step 005810): Train loss 4.408, Val loss 4.411\n",
            "Ep 2 (Step 005815): Train loss 4.131, Val loss 4.382\n",
            "Ep 2 (Step 005820): Train loss 3.977, Val loss 4.382\n",
            "Ep 2 (Step 005825): Train loss 3.955, Val loss 4.390\n",
            "Ep 2 (Step 005830): Train loss 3.872, Val loss 4.383\n",
            "Ep 2 (Step 005835): Train loss 4.071, Val loss 4.384\n",
            "Ep 2 (Step 005840): Train loss 4.207, Val loss 4.390\n",
            "Ep 2 (Step 005845): Train loss 4.027, Val loss 4.395\n",
            "Ep 2 (Step 005850): Train loss 4.249, Val loss 4.397\n",
            "Ep 2 (Step 005855): Train loss 3.908, Val loss 4.394\n",
            "Ep 2 (Step 005860): Train loss 4.387, Val loss 4.397\n",
            "Ep 2 (Step 005865): Train loss 4.214, Val loss 4.393\n",
            "Ep 2 (Step 005870): Train loss 4.321, Val loss 4.391\n",
            "Ep 2 (Step 005875): Train loss 4.219, Val loss 4.378\n",
            "Ep 2 (Step 005880): Train loss 4.010, Val loss 4.385\n",
            "Ep 2 (Step 005885): Train loss 4.168, Val loss 4.390\n",
            "Ep 2 (Step 005890): Train loss 4.042, Val loss 4.378\n",
            "Ep 2 (Step 005895): Train loss 4.003, Val loss 4.385\n",
            "Ep 2 (Step 005900): Train loss 3.949, Val loss 4.405\n",
            "Ep 2 (Step 005905): Train loss 4.182, Val loss 4.389\n",
            "Ep 2 (Step 005910): Train loss 3.833, Val loss 4.375\n",
            "Ep 2 (Step 005915): Train loss 4.041, Val loss 4.367\n",
            "Ep 2 (Step 005920): Train loss 4.134, Val loss 4.352\n",
            "Ep 2 (Step 005925): Train loss 3.811, Val loss 4.357\n",
            "Ep 2 (Step 005930): Train loss 4.070, Val loss 4.369\n",
            "Ep 2 (Step 005935): Train loss 3.634, Val loss 4.384\n",
            "Ep 2 (Step 005940): Train loss 3.735, Val loss 4.393\n",
            "Ep 2 (Step 005945): Train loss 4.045, Val loss 4.384\n",
            "Ep 2 (Step 005950): Train loss 4.092, Val loss 4.392\n",
            "Every effort moves you, And with the King’d, And, And, with the King’d, And with the King’d, And, And with the King’d, And with the King�\n",
            "Ep 3 (Step 005955): Train loss 4.032, Val loss 4.405\n",
            "Ep 3 (Step 005960): Train loss 4.051, Val loss 4.403\n",
            "Ep 3 (Step 005965): Train loss 3.983, Val loss 4.400\n",
            "Ep 3 (Step 005970): Train loss 4.343, Val loss 4.388\n",
            "Ep 3 (Step 005975): Train loss 4.076, Val loss 4.405\n",
            "Ep 3 (Step 005980): Train loss 3.975, Val loss 4.412\n",
            "Ep 3 (Step 005985): Train loss 4.255, Val loss 4.420\n",
            "Ep 3 (Step 005990): Train loss 3.805, Val loss 4.405\n",
            "Ep 3 (Step 005995): Train loss 4.063, Val loss 4.376\n",
            "Ep 3 (Step 006000): Train loss 3.607, Val loss 4.366\n",
            "Ep 3 (Step 006005): Train loss 3.934, Val loss 4.373\n",
            "Ep 3 (Step 006010): Train loss 3.761, Val loss 4.402\n",
            "Ep 3 (Step 006015): Train loss 3.765, Val loss 4.386\n",
            "Ep 3 (Step 006020): Train loss 3.816, Val loss 4.373\n",
            "Ep 3 (Step 006025): Train loss 3.863, Val loss 4.361\n",
            "Ep 3 (Step 006030): Train loss 3.989, Val loss 4.401\n",
            "Ep 3 (Step 006035): Train loss 3.877, Val loss 4.388\n",
            "Ep 3 (Step 006040): Train loss 3.976, Val loss 4.368\n",
            "Ep 3 (Step 006045): Train loss 3.965, Val loss 4.377\n",
            "Ep 3 (Step 006050): Train loss 3.968, Val loss 4.387\n",
            "Ep 3 (Step 006055): Train loss 4.271, Val loss 4.396\n",
            "Ep 3 (Step 006060): Train loss 4.038, Val loss 4.403\n",
            "Ep 3 (Step 006065): Train loss 3.719, Val loss 4.391\n",
            "Ep 3 (Step 006070): Train loss 4.039, Val loss 4.379\n",
            "Ep 3 (Step 006075): Train loss 4.215, Val loss 4.394\n",
            "Ep 3 (Step 006080): Train loss 3.966, Val loss 4.399\n",
            "Ep 3 (Step 006085): Train loss 3.811, Val loss 4.413\n",
            "Ep 3 (Step 006090): Train loss 3.931, Val loss 4.405\n",
            "Ep 3 (Step 006095): Train loss 4.149, Val loss 4.402\n",
            "Ep 3 (Step 006100): Train loss 4.105, Val loss 4.399\n",
            "Ep 3 (Step 006105): Train loss 4.003, Val loss 4.387\n",
            "Ep 3 (Step 006110): Train loss 4.074, Val loss 4.357\n",
            "Ep 3 (Step 006115): Train loss 3.968, Val loss 4.376\n",
            "Ep 3 (Step 006120): Train loss 4.002, Val loss 4.384\n",
            "Ep 3 (Step 006125): Train loss 4.031, Val loss 4.374\n",
            "Ep 3 (Step 006130): Train loss 4.102, Val loss 4.377\n",
            "Ep 3 (Step 006135): Train loss 4.107, Val loss 4.377\n",
            "Ep 3 (Step 006140): Train loss 4.007, Val loss 4.380\n",
            "Ep 3 (Step 006145): Train loss 3.870, Val loss 4.378\n",
            "Ep 3 (Step 006150): Train loss 4.302, Val loss 4.344\n",
            "Ep 3 (Step 006155): Train loss 4.172, Val loss 4.346\n",
            "Ep 3 (Step 006160): Train loss 4.077, Val loss 4.362\n",
            "Ep 3 (Step 006165): Train loss 3.897, Val loss 4.370\n",
            "Ep 3 (Step 006170): Train loss 3.907, Val loss 4.365\n",
            "Ep 3 (Step 006175): Train loss 3.795, Val loss 4.377\n",
            "Ep 3 (Step 006180): Train loss 4.200, Val loss 4.372\n",
            "Ep 3 (Step 006185): Train loss 3.888, Val loss 4.377\n",
            "Ep 3 (Step 006190): Train loss 4.037, Val loss 4.382\n",
            "Ep 3 (Step 006195): Train loss 4.262, Val loss 4.382\n",
            "Ep 3 (Step 006200): Train loss 4.055, Val loss 4.370\n",
            "Ep 3 (Step 006205): Train loss 3.886, Val loss 4.387\n",
            "Ep 3 (Step 006210): Train loss 3.884, Val loss 4.399\n",
            "Ep 3 (Step 006215): Train loss 4.027, Val loss 4.396\n",
            "Ep 3 (Step 006220): Train loss 3.928, Val loss 4.393\n",
            "Ep 3 (Step 006225): Train loss 4.121, Val loss 4.378\n",
            "Ep 3 (Step 006230): Train loss 3.971, Val loss 4.379\n",
            "Ep 3 (Step 006235): Train loss 4.005, Val loss 4.368\n",
            "Ep 3 (Step 006240): Train loss 3.840, Val loss 4.348\n",
            "Ep 3 (Step 006245): Train loss 3.842, Val loss 4.351\n",
            "Ep 3 (Step 006250): Train loss 4.003, Val loss 4.353\n",
            "Ep 3 (Step 006255): Train loss 3.876, Val loss 4.365\n",
            "Ep 3 (Step 006260): Train loss 3.957, Val loss 4.388\n",
            "Ep 3 (Step 006265): Train loss 3.864, Val loss 4.358\n",
            "Ep 3 (Step 006270): Train loss 4.005, Val loss 4.360\n",
            "Ep 3 (Step 006275): Train loss 3.883, Val loss 4.352\n",
            "Ep 3 (Step 006280): Train loss 4.130, Val loss 4.363\n",
            "Ep 3 (Step 006285): Train loss 4.024, Val loss 4.372\n",
            "Ep 3 (Step 006290): Train loss 3.918, Val loss 4.380\n",
            "Ep 3 (Step 006295): Train loss 4.039, Val loss 4.355\n",
            "Ep 3 (Step 006300): Train loss 4.226, Val loss 4.346\n",
            "Ep 3 (Step 006305): Train loss 4.073, Val loss 4.357\n",
            "Ep 3 (Step 006310): Train loss 4.016, Val loss 4.377\n",
            "Ep 3 (Step 006315): Train loss 3.953, Val loss 4.384\n",
            "Ep 3 (Step 006320): Train loss 3.838, Val loss 4.373\n",
            "Ep 3 (Step 006325): Train loss 3.982, Val loss 4.370\n",
            "Ep 3 (Step 006330): Train loss 3.857, Val loss 4.367\n",
            "Ep 3 (Step 006335): Train loss 4.189, Val loss 4.359\n",
            "Ep 3 (Step 006340): Train loss 3.785, Val loss 4.360\n",
            "Ep 3 (Step 006345): Train loss 3.875, Val loss 4.358\n",
            "Ep 3 (Step 006350): Train loss 4.152, Val loss 4.348\n",
            "Ep 3 (Step 006355): Train loss 3.835, Val loss 4.357\n",
            "Ep 3 (Step 006360): Train loss 3.749, Val loss 4.377\n",
            "Ep 3 (Step 006365): Train loss 3.829, Val loss 4.374\n",
            "Ep 3 (Step 006370): Train loss 3.787, Val loss 4.345\n",
            "Ep 3 (Step 006375): Train loss 3.855, Val loss 4.334\n",
            "Ep 3 (Step 006380): Train loss 3.699, Val loss 4.340\n",
            "Ep 3 (Step 006385): Train loss 4.083, Val loss 4.354\n",
            "Ep 3 (Step 006390): Train loss 4.045, Val loss 4.357\n",
            "Ep 3 (Step 006395): Train loss 4.090, Val loss 4.349\n",
            "Ep 3 (Step 006400): Train loss 3.718, Val loss 4.372\n",
            "Ep 3 (Step 006405): Train loss 3.768, Val loss 4.397\n",
            "Ep 3 (Step 006410): Train loss 3.848, Val loss 4.382\n",
            "Ep 3 (Step 006415): Train loss 3.886, Val loss 4.377\n",
            "Ep 3 (Step 006420): Train loss 3.854, Val loss 4.382\n",
            "Ep 3 (Step 006425): Train loss 4.035, Val loss 4.388\n",
            "Ep 3 (Step 006430): Train loss 4.220, Val loss 4.363\n",
            "Ep 3 (Step 006435): Train loss 3.945, Val loss 4.360\n",
            "Ep 3 (Step 006440): Train loss 4.068, Val loss 4.374\n",
            "Ep 3 (Step 006445): Train loss 4.100, Val loss 4.371\n",
            "Ep 3 (Step 006450): Train loss 3.866, Val loss 4.368\n",
            "Ep 3 (Step 006455): Train loss 4.069, Val loss 4.373\n",
            "Ep 3 (Step 006460): Train loss 3.820, Val loss 4.362\n",
            "Ep 3 (Step 006465): Train loss 3.798, Val loss 4.377\n",
            "Ep 3 (Step 006470): Train loss 3.736, Val loss 4.380\n",
            "Ep 3 (Step 006475): Train loss 4.257, Val loss 4.365\n",
            "Ep 3 (Step 006480): Train loss 4.023, Val loss 4.365\n",
            "Ep 3 (Step 006485): Train loss 3.872, Val loss 4.370\n",
            "Ep 3 (Step 006490): Train loss 3.826, Val loss 4.363\n",
            "Ep 3 (Step 006495): Train loss 3.933, Val loss 4.361\n",
            "Ep 3 (Step 006500): Train loss 3.948, Val loss 4.375\n",
            "Ep 3 (Step 006505): Train loss 3.717, Val loss 4.401\n",
            "Ep 3 (Step 006510): Train loss 4.161, Val loss 4.392\n",
            "Ep 3 (Step 006515): Train loss 3.921, Val loss 4.379\n",
            "Ep 3 (Step 006520): Train loss 3.863, Val loss 4.381\n",
            "Ep 3 (Step 006525): Train loss 4.178, Val loss 4.385\n",
            "Ep 3 (Step 006530): Train loss 3.915, Val loss 4.393\n",
            "Ep 3 (Step 006535): Train loss 3.928, Val loss 4.383\n",
            "Ep 3 (Step 006540): Train loss 3.989, Val loss 4.376\n",
            "Ep 3 (Step 006545): Train loss 3.500, Val loss 4.373\n",
            "Ep 3 (Step 006550): Train loss 3.776, Val loss 4.374\n",
            "Ep 3 (Step 006555): Train loss 4.071, Val loss 4.384\n",
            "Ep 3 (Step 006560): Train loss 3.890, Val loss 4.373\n",
            "Ep 3 (Step 006565): Train loss 3.566, Val loss 4.367\n",
            "Ep 3 (Step 006570): Train loss 3.742, Val loss 4.362\n",
            "Ep 3 (Step 006575): Train loss 3.878, Val loss 4.363\n",
            "Ep 3 (Step 006580): Train loss 4.167, Val loss 4.381\n",
            "Ep 3 (Step 006585): Train loss 4.250, Val loss 4.379\n",
            "Ep 3 (Step 006590): Train loss 3.638, Val loss 4.376\n",
            "Ep 3 (Step 006595): Train loss 4.051, Val loss 4.371\n",
            "Ep 3 (Step 006600): Train loss 4.120, Val loss 4.358\n",
            "Ep 3 (Step 006605): Train loss 4.096, Val loss 4.357\n",
            "Ep 3 (Step 006610): Train loss 4.097, Val loss 4.363\n",
            "Ep 3 (Step 006615): Train loss 4.028, Val loss 4.373\n",
            "Ep 3 (Step 006620): Train loss 4.214, Val loss 4.356\n",
            "Ep 3 (Step 006625): Train loss 3.982, Val loss 4.343\n",
            "Ep 3 (Step 006630): Train loss 4.026, Val loss 4.357\n",
            "Ep 3 (Step 006635): Train loss 4.117, Val loss 4.366\n",
            "Ep 3 (Step 006640): Train loss 3.887, Val loss 4.382\n",
            "Ep 3 (Step 006645): Train loss 3.994, Val loss 4.337\n",
            "Ep 3 (Step 006650): Train loss 3.878, Val loss 4.322\n",
            "Ep 3 (Step 006655): Train loss 4.309, Val loss 4.323\n",
            "Ep 3 (Step 006660): Train loss 3.920, Val loss 4.330\n",
            "Ep 3 (Step 006665): Train loss 4.105, Val loss 4.342\n",
            "Ep 3 (Step 006670): Train loss 3.738, Val loss 4.344\n",
            "Ep 3 (Step 006675): Train loss 3.899, Val loss 4.330\n",
            "Ep 3 (Step 006680): Train loss 4.055, Val loss 4.340\n",
            "Ep 3 (Step 006685): Train loss 4.105, Val loss 4.342\n",
            "Ep 3 (Step 006690): Train loss 3.498, Val loss 4.346\n",
            "Ep 3 (Step 006695): Train loss 3.935, Val loss 4.355\n",
            "Ep 3 (Step 006700): Train loss 3.738, Val loss 4.353\n",
            "Ep 3 (Step 006705): Train loss 3.891, Val loss 4.345\n",
            "Ep 3 (Step 006710): Train loss 3.782, Val loss 4.327\n",
            "Ep 3 (Step 006715): Train loss 3.822, Val loss 4.322\n",
            "Ep 3 (Step 006720): Train loss 3.893, Val loss 4.342\n",
            "Ep 3 (Step 006725): Train loss 3.855, Val loss 4.373\n",
            "Ep 3 (Step 006730): Train loss 3.790, Val loss 4.370\n",
            "Ep 3 (Step 006735): Train loss 3.933, Val loss 4.361\n",
            "Ep 3 (Step 006740): Train loss 3.942, Val loss 4.363\n",
            "Ep 3 (Step 006745): Train loss 3.769, Val loss 4.362\n",
            "Ep 3 (Step 006750): Train loss 3.752, Val loss 4.372\n",
            "Ep 3 (Step 006755): Train loss 3.551, Val loss 4.381\n",
            "Ep 3 (Step 006760): Train loss 3.720, Val loss 4.366\n",
            "Ep 3 (Step 006765): Train loss 4.041, Val loss 4.365\n",
            "Ep 3 (Step 006770): Train loss 3.891, Val loss 4.350\n",
            "Ep 3 (Step 006775): Train loss 3.767, Val loss 4.352\n",
            "Ep 3 (Step 006780): Train loss 4.017, Val loss 4.354\n",
            "Ep 3 (Step 006785): Train loss 4.012, Val loss 4.368\n",
            "Ep 3 (Step 006790): Train loss 4.052, Val loss 4.375\n",
            "Ep 3 (Step 006795): Train loss 3.996, Val loss 4.362\n",
            "Ep 3 (Step 006800): Train loss 3.772, Val loss 4.363\n",
            "Ep 3 (Step 006805): Train loss 3.657, Val loss 4.372\n",
            "Ep 3 (Step 006810): Train loss 3.841, Val loss 4.389\n",
            "Ep 3 (Step 006815): Train loss 4.058, Val loss 4.362\n",
            "Ep 3 (Step 006820): Train loss 4.220, Val loss 4.349\n",
            "Ep 3 (Step 006825): Train loss 3.845, Val loss 4.348\n",
            "Ep 3 (Step 006830): Train loss 4.040, Val loss 4.354\n",
            "Ep 3 (Step 006835): Train loss 4.025, Val loss 4.360\n",
            "Ep 3 (Step 006840): Train loss 3.843, Val loss 4.347\n",
            "Ep 3 (Step 006845): Train loss 4.015, Val loss 4.336\n",
            "Ep 3 (Step 006850): Train loss 3.926, Val loss 4.338\n",
            "Ep 3 (Step 006855): Train loss 4.032, Val loss 4.357\n",
            "Ep 3 (Step 006860): Train loss 3.644, Val loss 4.365\n",
            "Ep 3 (Step 006865): Train loss 3.948, Val loss 4.371\n",
            "Ep 3 (Step 006870): Train loss 3.899, Val loss 4.370\n",
            "Ep 3 (Step 006875): Train loss 3.692, Val loss 4.362\n",
            "Ep 3 (Step 006880): Train loss 3.585, Val loss 4.353\n",
            "Ep 3 (Step 006885): Train loss 4.085, Val loss 4.338\n",
            "Ep 3 (Step 006890): Train loss 4.021, Val loss 4.346\n",
            "Ep 3 (Step 006895): Train loss 3.996, Val loss 4.349\n",
            "Ep 3 (Step 006900): Train loss 3.933, Val loss 4.351\n",
            "Ep 3 (Step 006905): Train loss 4.068, Val loss 4.346\n",
            "Ep 3 (Step 006910): Train loss 3.999, Val loss 4.338\n",
            "Ep 3 (Step 006915): Train loss 4.039, Val loss 4.334\n",
            "Ep 3 (Step 006920): Train loss 4.022, Val loss 4.340\n",
            "Ep 3 (Step 006925): Train loss 3.918, Val loss 4.348\n",
            "Ep 3 (Step 006930): Train loss 3.964, Val loss 4.370\n",
            "Ep 3 (Step 006935): Train loss 4.144, Val loss 4.361\n",
            "Ep 3 (Step 006940): Train loss 3.939, Val loss 4.357\n",
            "Ep 3 (Step 006945): Train loss 3.812, Val loss 4.341\n",
            "Ep 3 (Step 006950): Train loss 3.742, Val loss 4.330\n",
            "Ep 3 (Step 006955): Train loss 3.990, Val loss 4.335\n",
            "Ep 3 (Step 006960): Train loss 3.807, Val loss 4.343\n",
            "Ep 3 (Step 006965): Train loss 4.153, Val loss 4.359\n",
            "Ep 3 (Step 006970): Train loss 3.786, Val loss 4.365\n",
            "Ep 3 (Step 006975): Train loss 4.122, Val loss 4.359\n",
            "Ep 3 (Step 006980): Train loss 3.962, Val loss 4.352\n",
            "Ep 3 (Step 006985): Train loss 3.970, Val loss 4.345\n",
            "Ep 3 (Step 006990): Train loss 3.982, Val loss 4.357\n",
            "Ep 3 (Step 006995): Train loss 3.964, Val loss 4.365\n",
            "Ep 3 (Step 007000): Train loss 3.643, Val loss 4.348\n",
            "Ep 3 (Step 007005): Train loss 3.419, Val loss 4.340\n",
            "Ep 3 (Step 007010): Train loss 4.152, Val loss 4.346\n",
            "Ep 3 (Step 007015): Train loss 3.988, Val loss 4.360\n",
            "Ep 3 (Step 007020): Train loss 4.003, Val loss 4.362\n",
            "Ep 3 (Step 007025): Train loss 4.016, Val loss 4.356\n",
            "Ep 3 (Step 007030): Train loss 3.455, Val loss 4.356\n",
            "Ep 3 (Step 007035): Train loss 3.775, Val loss 4.352\n",
            "Ep 3 (Step 007040): Train loss 3.723, Val loss 4.356\n",
            "Ep 3 (Step 007045): Train loss 3.864, Val loss 4.366\n",
            "Ep 3 (Step 007050): Train loss 4.125, Val loss 4.354\n",
            "Ep 3 (Step 007055): Train loss 4.170, Val loss 4.342\n",
            "Ep 3 (Step 007060): Train loss 4.018, Val loss 4.352\n",
            "Ep 3 (Step 007065): Train loss 3.900, Val loss 4.351\n",
            "Ep 3 (Step 007070): Train loss 3.756, Val loss 4.359\n",
            "Ep 3 (Step 007075): Train loss 4.051, Val loss 4.349\n",
            "Ep 3 (Step 007080): Train loss 4.093, Val loss 4.355\n",
            "Ep 3 (Step 007085): Train loss 3.633, Val loss 4.356\n",
            "Ep 3 (Step 007090): Train loss 3.931, Val loss 4.352\n",
            "Ep 3 (Step 007095): Train loss 3.888, Val loss 4.354\n",
            "Ep 3 (Step 007100): Train loss 3.870, Val loss 4.344\n",
            "Ep 3 (Step 007105): Train loss 3.956, Val loss 4.355\n",
            "Ep 3 (Step 007110): Train loss 3.821, Val loss 4.372\n",
            "Ep 3 (Step 007115): Train loss 3.830, Val loss 4.363\n",
            "Ep 3 (Step 007120): Train loss 4.056, Val loss 4.360\n",
            "Ep 3 (Step 007125): Train loss 3.742, Val loss 4.347\n",
            "Ep 3 (Step 007130): Train loss 3.645, Val loss 4.336\n",
            "Ep 3 (Step 007135): Train loss 3.757, Val loss 4.334\n",
            "Ep 3 (Step 007140): Train loss 4.007, Val loss 4.350\n",
            "Ep 3 (Step 007145): Train loss 3.756, Val loss 4.340\n",
            "Ep 3 (Step 007150): Train loss 4.133, Val loss 4.332\n",
            "Ep 3 (Step 007155): Train loss 4.016, Val loss 4.327\n",
            "Ep 3 (Step 007160): Train loss 3.927, Val loss 4.349\n",
            "Ep 3 (Step 007165): Train loss 4.104, Val loss 4.345\n",
            "Ep 3 (Step 007170): Train loss 3.697, Val loss 4.327\n",
            "Ep 3 (Step 007175): Train loss 4.031, Val loss 4.320\n",
            "Ep 3 (Step 007180): Train loss 3.732, Val loss 4.311\n",
            "Ep 3 (Step 007185): Train loss 3.700, Val loss 4.322\n",
            "Ep 3 (Step 007190): Train loss 3.895, Val loss 4.315\n",
            "Ep 3 (Step 007195): Train loss 4.038, Val loss 4.323\n",
            "Ep 3 (Step 007200): Train loss 3.744, Val loss 4.335\n",
            "Ep 3 (Step 007205): Train loss 4.005, Val loss 4.346\n",
            "Ep 3 (Step 007210): Train loss 3.867, Val loss 4.348\n",
            "Ep 3 (Step 007215): Train loss 3.943, Val loss 4.326\n",
            "Ep 3 (Step 007220): Train loss 4.008, Val loss 4.335\n",
            "Ep 3 (Step 007225): Train loss 3.925, Val loss 4.343\n",
            "Ep 3 (Step 007230): Train loss 3.867, Val loss 4.346\n",
            "Ep 3 (Step 007235): Train loss 3.958, Val loss 4.351\n",
            "Ep 3 (Step 007240): Train loss 3.852, Val loss 4.333\n",
            "Ep 3 (Step 007245): Train loss 3.778, Val loss 4.317\n",
            "Ep 3 (Step 007250): Train loss 3.774, Val loss 4.315\n",
            "Ep 3 (Step 007255): Train loss 3.999, Val loss 4.315\n",
            "Ep 3 (Step 007260): Train loss 4.096, Val loss 4.306\n",
            "Ep 3 (Step 007265): Train loss 3.890, Val loss 4.314\n",
            "Ep 3 (Step 007270): Train loss 3.977, Val loss 4.312\n",
            "Ep 3 (Step 007275): Train loss 3.753, Val loss 4.327\n",
            "Ep 3 (Step 007280): Train loss 3.678, Val loss 4.319\n",
            "Ep 3 (Step 007285): Train loss 3.811, Val loss 4.318\n",
            "Ep 3 (Step 007290): Train loss 3.903, Val loss 4.309\n",
            "Ep 3 (Step 007295): Train loss 4.017, Val loss 4.324\n",
            "Ep 3 (Step 007300): Train loss 3.804, Val loss 4.334\n",
            "Ep 3 (Step 007305): Train loss 3.912, Val loss 4.325\n",
            "Ep 3 (Step 007310): Train loss 3.840, Val loss 4.317\n",
            "Ep 3 (Step 007315): Train loss 3.796, Val loss 4.313\n",
            "Ep 3 (Step 007320): Train loss 4.120, Val loss 4.307\n",
            "Ep 3 (Step 007325): Train loss 3.503, Val loss 4.308\n",
            "Ep 3 (Step 007330): Train loss 4.034, Val loss 4.291\n",
            "Ep 3 (Step 007335): Train loss 3.924, Val loss 4.297\n",
            "Ep 3 (Step 007340): Train loss 3.839, Val loss 4.308\n",
            "Ep 3 (Step 007345): Train loss 4.237, Val loss 4.327\n",
            "Ep 3 (Step 007350): Train loss 3.793, Val loss 4.324\n",
            "Ep 3 (Step 007355): Train loss 3.427, Val loss 4.315\n",
            "Ep 3 (Step 007360): Train loss 4.008, Val loss 4.310\n",
            "Ep 3 (Step 007365): Train loss 3.733, Val loss 4.309\n",
            "Ep 3 (Step 007370): Train loss 3.874, Val loss 4.318\n",
            "Ep 3 (Step 007375): Train loss 3.923, Val loss 4.313\n",
            "Ep 3 (Step 007380): Train loss 3.856, Val loss 4.305\n",
            "Ep 3 (Step 007385): Train loss 3.800, Val loss 4.311\n",
            "Ep 3 (Step 007390): Train loss 3.760, Val loss 4.323\n",
            "Ep 3 (Step 007395): Train loss 3.935, Val loss 4.322\n",
            "Ep 3 (Step 007400): Train loss 3.782, Val loss 4.316\n",
            "Ep 3 (Step 007405): Train loss 3.923, Val loss 4.314\n",
            "Ep 3 (Step 007410): Train loss 4.060, Val loss 4.332\n",
            "Ep 3 (Step 007415): Train loss 3.806, Val loss 4.339\n",
            "Ep 3 (Step 007420): Train loss 4.019, Val loss 4.318\n",
            "Ep 3 (Step 007425): Train loss 3.936, Val loss 4.318\n",
            "Ep 3 (Step 007430): Train loss 3.818, Val loss 4.320\n",
            "Ep 3 (Step 007435): Train loss 4.023, Val loss 4.334\n",
            "Ep 3 (Step 007440): Train loss 3.985, Val loss 4.342\n",
            "Ep 3 (Step 007445): Train loss 3.677, Val loss 4.335\n",
            "Ep 3 (Step 007450): Train loss 3.793, Val loss 4.311\n",
            "Ep 3 (Step 007455): Train loss 3.838, Val loss 4.313\n",
            "Ep 3 (Step 007460): Train loss 4.065, Val loss 4.312\n",
            "Ep 3 (Step 007465): Train loss 3.928, Val loss 4.323\n",
            "Ep 3 (Step 007470): Train loss 3.871, Val loss 4.311\n",
            "Ep 3 (Step 007475): Train loss 3.917, Val loss 4.310\n",
            "Ep 3 (Step 007480): Train loss 3.840, Val loss 4.310\n",
            "Ep 3 (Step 007485): Train loss 3.939, Val loss 4.310\n",
            "Ep 3 (Step 007490): Train loss 4.137, Val loss 4.320\n",
            "Ep 3 (Step 007495): Train loss 3.770, Val loss 4.320\n",
            "Ep 3 (Step 007500): Train loss 3.614, Val loss 4.304\n",
            "Ep 3 (Step 007505): Train loss 4.064, Val loss 4.307\n",
            "Ep 3 (Step 007510): Train loss 3.649, Val loss 4.314\n",
            "Ep 3 (Step 007515): Train loss 3.953, Val loss 4.329\n",
            "Ep 3 (Step 007520): Train loss 3.834, Val loss 4.327\n",
            "Ep 3 (Step 007525): Train loss 3.922, Val loss 4.314\n",
            "Ep 3 (Step 007530): Train loss 3.845, Val loss 4.313\n",
            "Ep 3 (Step 007535): Train loss 4.044, Val loss 4.323\n",
            "Ep 3 (Step 007540): Train loss 3.903, Val loss 4.341\n",
            "Ep 3 (Step 007545): Train loss 3.390, Val loss 4.342\n",
            "Ep 3 (Step 007550): Train loss 4.235, Val loss 4.343\n",
            "Ep 3 (Step 007555): Train loss 4.063, Val loss 4.366\n",
            "Ep 3 (Step 007560): Train loss 4.110, Val loss 4.347\n",
            "Ep 3 (Step 007565): Train loss 4.121, Val loss 4.325\n",
            "Ep 3 (Step 007570): Train loss 3.554, Val loss 4.316\n",
            "Ep 3 (Step 007575): Train loss 3.866, Val loss 4.334\n",
            "Ep 3 (Step 007580): Train loss 4.183, Val loss 4.348\n",
            "Ep 3 (Step 007585): Train loss 4.032, Val loss 4.339\n",
            "Ep 3 (Step 007590): Train loss 4.011, Val loss 4.332\n",
            "Ep 3 (Step 007595): Train loss 4.162, Val loss 4.332\n",
            "Ep 3 (Step 007600): Train loss 3.828, Val loss 4.312\n",
            "Ep 3 (Step 007605): Train loss 3.799, Val loss 4.311\n",
            "Ep 3 (Step 007610): Train loss 3.841, Val loss 4.305\n",
            "Ep 3 (Step 007615): Train loss 3.925, Val loss 4.310\n",
            "Ep 3 (Step 007620): Train loss 3.797, Val loss 4.308\n",
            "Ep 3 (Step 007625): Train loss 3.804, Val loss 4.311\n",
            "Ep 3 (Step 007630): Train loss 3.814, Val loss 4.313\n",
            "Ep 3 (Step 007635): Train loss 4.056, Val loss 4.309\n",
            "Ep 3 (Step 007640): Train loss 3.721, Val loss 4.319\n",
            "Ep 3 (Step 007645): Train loss 3.874, Val loss 4.337\n",
            "Ep 3 (Step 007650): Train loss 3.827, Val loss 4.356\n",
            "Ep 3 (Step 007655): Train loss 3.921, Val loss 4.341\n",
            "Ep 3 (Step 007660): Train loss 3.670, Val loss 4.327\n",
            "Ep 3 (Step 007665): Train loss 3.681, Val loss 4.336\n",
            "Ep 3 (Step 007670): Train loss 4.008, Val loss 4.334\n",
            "Ep 3 (Step 007675): Train loss 3.666, Val loss 4.330\n",
            "Ep 3 (Step 007680): Train loss 3.810, Val loss 4.328\n",
            "Ep 3 (Step 007685): Train loss 3.742, Val loss 4.323\n",
            "Ep 3 (Step 007690): Train loss 3.635, Val loss 4.319\n",
            "Ep 3 (Step 007695): Train loss 3.964, Val loss 4.341\n",
            "Ep 3 (Step 007700): Train loss 3.750, Val loss 4.334\n",
            "Ep 3 (Step 007705): Train loss 3.904, Val loss 4.318\n",
            "Ep 3 (Step 007710): Train loss 3.922, Val loss 4.315\n",
            "Ep 3 (Step 007715): Train loss 4.064, Val loss 4.328\n",
            "Ep 3 (Step 007720): Train loss 3.972, Val loss 4.336\n",
            "Ep 3 (Step 007725): Train loss 3.696, Val loss 4.325\n",
            "Ep 3 (Step 007730): Train loss 3.834, Val loss 4.324\n",
            "Ep 3 (Step 007735): Train loss 3.857, Val loss 4.332\n",
            "Ep 3 (Step 007740): Train loss 3.839, Val loss 4.325\n",
            "Ep 3 (Step 007745): Train loss 3.841, Val loss 4.327\n",
            "Ep 3 (Step 007750): Train loss 3.650, Val loss 4.350\n",
            "Ep 3 (Step 007755): Train loss 3.743, Val loss 4.340\n",
            "Ep 3 (Step 007760): Train loss 3.848, Val loss 4.331\n",
            "Ep 3 (Step 007765): Train loss 3.998, Val loss 4.327\n",
            "Ep 3 (Step 007770): Train loss 3.484, Val loss 4.334\n",
            "Ep 3 (Step 007775): Train loss 4.055, Val loss 4.341\n",
            "Ep 3 (Step 007780): Train loss 4.004, Val loss 4.339\n",
            "Ep 3 (Step 007785): Train loss 3.921, Val loss 4.335\n",
            "Ep 3 (Step 007790): Train loss 3.919, Val loss 4.335\n",
            "Ep 3 (Step 007795): Train loss 3.673, Val loss 4.322\n",
            "Ep 3 (Step 007800): Train loss 3.844, Val loss 4.316\n",
            "Ep 3 (Step 007805): Train loss 3.659, Val loss 4.302\n",
            "Ep 3 (Step 007810): Train loss 3.586, Val loss 4.310\n",
            "Ep 3 (Step 007815): Train loss 3.617, Val loss 4.313\n",
            "Ep 3 (Step 007820): Train loss 3.767, Val loss 4.325\n",
            "Ep 3 (Step 007825): Train loss 3.987, Val loss 4.328\n",
            "Ep 3 (Step 007830): Train loss 3.757, Val loss 4.331\n",
            "Ep 3 (Step 007835): Train loss 3.717, Val loss 4.327\n",
            "Ep 3 (Step 007840): Train loss 3.705, Val loss 4.330\n",
            "Ep 3 (Step 007845): Train loss 3.969, Val loss 4.325\n",
            "Ep 3 (Step 007850): Train loss 3.954, Val loss 4.343\n",
            "Ep 3 (Step 007855): Train loss 3.831, Val loss 4.322\n",
            "Ep 3 (Step 007860): Train loss 3.792, Val loss 4.313\n",
            "Ep 3 (Step 007865): Train loss 3.886, Val loss 4.320\n",
            "Ep 3 (Step 007870): Train loss 3.633, Val loss 4.324\n",
            "Ep 3 (Step 007875): Train loss 3.726, Val loss 4.339\n",
            "Ep 3 (Step 007880): Train loss 3.748, Val loss 4.342\n",
            "Ep 3 (Step 007885): Train loss 4.024, Val loss 4.332\n",
            "Ep 3 (Step 007890): Train loss 3.772, Val loss 4.332\n",
            "Ep 3 (Step 007895): Train loss 3.686, Val loss 4.340\n",
            "Ep 3 (Step 007900): Train loss 3.781, Val loss 4.348\n",
            "Ep 3 (Step 007905): Train loss 3.760, Val loss 4.358\n",
            "Ep 3 (Step 007910): Train loss 3.858, Val loss 4.374\n",
            "Ep 3 (Step 007915): Train loss 3.739, Val loss 4.350\n",
            "Ep 3 (Step 007920): Train loss 3.936, Val loss 4.330\n",
            "Ep 3 (Step 007925): Train loss 4.086, Val loss 4.318\n",
            "Ep 3 (Step 007930): Train loss 3.778, Val loss 4.299\n",
            "Ep 3 (Step 007935): Train loss 3.523, Val loss 4.301\n",
            "Ep 3 (Step 007940): Train loss 3.912, Val loss 4.319\n",
            "Ep 3 (Step 007945): Train loss 3.937, Val loss 4.317\n",
            "Ep 3 (Step 007950): Train loss 3.714, Val loss 4.305\n",
            "Ep 3 (Step 007955): Train loss 3.836, Val loss 4.299\n",
            "Ep 3 (Step 007960): Train loss 4.090, Val loss 4.297\n",
            "Ep 3 (Step 007965): Train loss 4.070, Val loss 4.318\n",
            "Ep 3 (Step 007970): Train loss 3.654, Val loss 4.319\n",
            "Ep 3 (Step 007975): Train loss 3.753, Val loss 4.316\n",
            "Ep 3 (Step 007980): Train loss 4.023, Val loss 4.312\n",
            "Ep 3 (Step 007985): Train loss 3.742, Val loss 4.311\n",
            "Ep 3 (Step 007990): Train loss 3.956, Val loss 4.316\n",
            "Ep 3 (Step 007995): Train loss 4.004, Val loss 4.332\n",
            "Ep 3 (Step 008000): Train loss 3.567, Val loss 4.307\n",
            "Ep 3 (Step 008005): Train loss 4.063, Val loss 4.290\n",
            "Ep 3 (Step 008010): Train loss 3.840, Val loss 4.308\n",
            "Ep 3 (Step 008015): Train loss 3.839, Val loss 4.305\n",
            "Ep 3 (Step 008020): Train loss 3.962, Val loss 4.312\n",
            "Ep 3 (Step 008025): Train loss 3.590, Val loss 4.320\n",
            "Ep 3 (Step 008030): Train loss 3.576, Val loss 4.328\n",
            "Ep 3 (Step 008035): Train loss 4.019, Val loss 4.326\n",
            "Ep 3 (Step 008040): Train loss 3.632, Val loss 4.319\n",
            "Ep 3 (Step 008045): Train loss 3.703, Val loss 4.296\n",
            "Ep 3 (Step 008050): Train loss 4.035, Val loss 4.296\n",
            "Ep 3 (Step 008055): Train loss 3.726, Val loss 4.306\n",
            "Ep 3 (Step 008060): Train loss 3.750, Val loss 4.307\n",
            "Ep 3 (Step 008065): Train loss 3.972, Val loss 4.315\n",
            "Ep 3 (Step 008070): Train loss 3.799, Val loss 4.319\n",
            "Ep 3 (Step 008075): Train loss 3.466, Val loss 4.318\n",
            "Ep 3 (Step 008080): Train loss 3.711, Val loss 4.324\n",
            "Ep 3 (Step 008085): Train loss 3.848, Val loss 4.327\n",
            "Ep 3 (Step 008090): Train loss 3.975, Val loss 4.320\n",
            "Ep 3 (Step 008095): Train loss 4.073, Val loss 4.316\n",
            "Ep 3 (Step 008100): Train loss 3.533, Val loss 4.309\n",
            "Ep 3 (Step 008105): Train loss 3.852, Val loss 4.298\n",
            "Ep 3 (Step 008110): Train loss 3.957, Val loss 4.287\n",
            "Ep 3 (Step 008115): Train loss 3.508, Val loss 4.280\n",
            "Ep 3 (Step 008120): Train loss 3.836, Val loss 4.287\n",
            "Ep 3 (Step 008125): Train loss 3.788, Val loss 4.285\n",
            "Ep 3 (Step 008130): Train loss 3.732, Val loss 4.281\n",
            "Ep 3 (Step 008135): Train loss 3.845, Val loss 4.282\n",
            "Ep 3 (Step 008140): Train loss 3.853, Val loss 4.284\n",
            "Ep 3 (Step 008145): Train loss 3.823, Val loss 4.284\n",
            "Ep 3 (Step 008150): Train loss 3.825, Val loss 4.302\n",
            "Ep 3 (Step 008155): Train loss 3.802, Val loss 4.292\n",
            "Ep 3 (Step 008160): Train loss 3.590, Val loss 4.283\n",
            "Ep 3 (Step 008165): Train loss 4.022, Val loss 4.284\n",
            "Ep 3 (Step 008170): Train loss 3.734, Val loss 4.293\n",
            "Ep 3 (Step 008175): Train loss 3.582, Val loss 4.281\n",
            "Ep 3 (Step 008180): Train loss 3.917, Val loss 4.280\n",
            "Ep 3 (Step 008185): Train loss 4.075, Val loss 4.269\n",
            "Ep 3 (Step 008190): Train loss 3.904, Val loss 4.260\n",
            "Ep 3 (Step 008195): Train loss 3.634, Val loss 4.265\n",
            "Ep 3 (Step 008200): Train loss 4.004, Val loss 4.274\n",
            "Ep 3 (Step 008205): Train loss 3.702, Val loss 4.287\n",
            "Ep 3 (Step 008210): Train loss 3.715, Val loss 4.304\n",
            "Ep 3 (Step 008215): Train loss 3.814, Val loss 4.302\n",
            "Ep 3 (Step 008220): Train loss 3.810, Val loss 4.310\n",
            "Ep 3 (Step 008225): Train loss 3.855, Val loss 4.319\n",
            "Ep 3 (Step 008230): Train loss 3.770, Val loss 4.296\n",
            "Ep 3 (Step 008235): Train loss 3.672, Val loss 4.277\n",
            "Ep 3 (Step 008240): Train loss 3.951, Val loss 4.283\n",
            "Ep 3 (Step 008245): Train loss 3.707, Val loss 4.289\n",
            "Ep 3 (Step 008250): Train loss 4.081, Val loss 4.303\n",
            "Ep 3 (Step 008255): Train loss 3.439, Val loss 4.297\n",
            "Ep 3 (Step 008260): Train loss 3.948, Val loss 4.296\n",
            "Ep 3 (Step 008265): Train loss 3.796, Val loss 4.303\n",
            "Ep 3 (Step 008270): Train loss 3.694, Val loss 4.300\n",
            "Ep 3 (Step 008275): Train loss 4.037, Val loss 4.301\n",
            "Ep 3 (Step 008280): Train loss 3.777, Val loss 4.299\n",
            "Ep 3 (Step 008285): Train loss 3.744, Val loss 4.294\n",
            "Ep 3 (Step 008290): Train loss 3.742, Val loss 4.295\n",
            "Ep 3 (Step 008295): Train loss 3.632, Val loss 4.297\n",
            "Ep 3 (Step 008300): Train loss 3.767, Val loss 4.303\n",
            "Ep 3 (Step 008305): Train loss 3.581, Val loss 4.298\n",
            "Ep 3 (Step 008310): Train loss 3.891, Val loss 4.305\n",
            "Ep 3 (Step 008315): Train loss 3.607, Val loss 4.300\n",
            "Ep 3 (Step 008320): Train loss 3.609, Val loss 4.296\n",
            "Ep 3 (Step 008325): Train loss 3.687, Val loss 4.281\n",
            "Ep 3 (Step 008330): Train loss 3.866, Val loss 4.272\n",
            "Ep 3 (Step 008335): Train loss 3.805, Val loss 4.281\n",
            "Ep 3 (Step 008340): Train loss 3.720, Val loss 4.283\n",
            "Ep 3 (Step 008345): Train loss 4.152, Val loss 4.285\n",
            "Ep 3 (Step 008350): Train loss 4.042, Val loss 4.284\n",
            "Ep 3 (Step 008355): Train loss 3.679, Val loss 4.272\n",
            "Ep 3 (Step 008360): Train loss 3.816, Val loss 4.265\n",
            "Ep 3 (Step 008365): Train loss 3.921, Val loss 4.268\n",
            "Ep 3 (Step 008370): Train loss 3.638, Val loss 4.279\n",
            "Ep 3 (Step 008375): Train loss 3.741, Val loss 4.273\n",
            "Ep 3 (Step 008380): Train loss 3.722, Val loss 4.270\n",
            "Ep 3 (Step 008385): Train loss 3.774, Val loss 4.271\n",
            "Ep 3 (Step 008390): Train loss 3.875, Val loss 4.265\n",
            "Ep 3 (Step 008395): Train loss 3.623, Val loss 4.251\n",
            "Ep 3 (Step 008400): Train loss 3.604, Val loss 4.267\n",
            "Ep 3 (Step 008405): Train loss 3.869, Val loss 4.260\n",
            "Ep 3 (Step 008410): Train loss 3.920, Val loss 4.263\n",
            "Ep 3 (Step 008415): Train loss 3.794, Val loss 4.278\n",
            "Ep 3 (Step 008420): Train loss 3.551, Val loss 4.278\n",
            "Ep 3 (Step 008425): Train loss 3.884, Val loss 4.291\n",
            "Ep 3 (Step 008430): Train loss 3.549, Val loss 4.304\n",
            "Ep 3 (Step 008435): Train loss 4.026, Val loss 4.301\n",
            "Ep 3 (Step 008440): Train loss 3.655, Val loss 4.297\n",
            "Ep 3 (Step 008445): Train loss 3.710, Val loss 4.309\n",
            "Ep 3 (Step 008450): Train loss 3.700, Val loss 4.309\n",
            "Ep 3 (Step 008455): Train loss 3.746, Val loss 4.312\n",
            "Ep 3 (Step 008460): Train loss 4.026, Val loss 4.314\n",
            "Ep 3 (Step 008465): Train loss 3.791, Val loss 4.297\n",
            "Ep 3 (Step 008470): Train loss 3.833, Val loss 4.282\n",
            "Ep 3 (Step 008475): Train loss 3.693, Val loss 4.286\n",
            "Ep 3 (Step 008480): Train loss 3.493, Val loss 4.287\n",
            "Ep 3 (Step 008485): Train loss 3.656, Val loss 4.294\n",
            "Ep 3 (Step 008490): Train loss 4.003, Val loss 4.304\n",
            "Ep 3 (Step 008495): Train loss 3.901, Val loss 4.318\n",
            "Ep 3 (Step 008500): Train loss 3.897, Val loss 4.310\n",
            "Ep 3 (Step 008505): Train loss 3.707, Val loss 4.306\n",
            "Ep 3 (Step 008510): Train loss 3.273, Val loss 4.312\n",
            "Ep 3 (Step 008515): Train loss 3.697, Val loss 4.317\n",
            "Ep 3 (Step 008520): Train loss 3.783, Val loss 4.313\n",
            "Ep 3 (Step 008525): Train loss 3.474, Val loss 4.305\n",
            "Ep 3 (Step 008530): Train loss 3.851, Val loss 4.311\n",
            "Ep 3 (Step 008535): Train loss 3.564, Val loss 4.330\n",
            "Ep 3 (Step 008540): Train loss 3.827, Val loss 4.325\n",
            "Ep 3 (Step 008545): Train loss 3.754, Val loss 4.310\n",
            "Ep 3 (Step 008550): Train loss 3.965, Val loss 4.311\n",
            "Ep 3 (Step 008555): Train loss 3.781, Val loss 4.315\n",
            "Ep 3 (Step 008560): Train loss 3.799, Val loss 4.332\n",
            "Ep 3 (Step 008565): Train loss 3.721, Val loss 4.325\n",
            "Ep 3 (Step 008570): Train loss 3.386, Val loss 4.320\n",
            "Ep 3 (Step 008575): Train loss 3.719, Val loss 4.317\n",
            "Ep 3 (Step 008580): Train loss 3.950, Val loss 4.321\n",
            "Ep 3 (Step 008585): Train loss 4.059, Val loss 4.314\n",
            "Ep 3 (Step 008590): Train loss 3.651, Val loss 4.296\n",
            "Ep 3 (Step 008595): Train loss 3.863, Val loss 4.296\n",
            "Ep 3 (Step 008600): Train loss 3.766, Val loss 4.291\n",
            "Ep 3 (Step 008605): Train loss 3.782, Val loss 4.287\n",
            "Ep 3 (Step 008610): Train loss 3.801, Val loss 4.295\n",
            "Ep 3 (Step 008615): Train loss 3.863, Val loss 4.299\n",
            "Ep 3 (Step 008620): Train loss 3.907, Val loss 4.283\n",
            "Ep 3 (Step 008625): Train loss 3.492, Val loss 4.289\n",
            "Ep 3 (Step 008630): Train loss 4.043, Val loss 4.293\n",
            "Ep 3 (Step 008635): Train loss 3.886, Val loss 4.281\n",
            "Ep 3 (Step 008640): Train loss 3.841, Val loss 4.282\n",
            "Ep 3 (Step 008645): Train loss 3.749, Val loss 4.293\n",
            "Ep 3 (Step 008650): Train loss 4.012, Val loss 4.293\n",
            "Ep 3 (Step 008655): Train loss 3.882, Val loss 4.292\n",
            "Ep 3 (Step 008660): Train loss 3.950, Val loss 4.292\n",
            "Ep 3 (Step 008665): Train loss 3.856, Val loss 4.311\n",
            "Ep 3 (Step 008670): Train loss 3.801, Val loss 4.317\n",
            "Ep 3 (Step 008675): Train loss 3.532, Val loss 4.301\n",
            "Ep 3 (Step 008680): Train loss 3.800, Val loss 4.291\n",
            "Ep 3 (Step 008685): Train loss 3.740, Val loss 4.276\n",
            "Ep 3 (Step 008690): Train loss 3.742, Val loss 4.277\n",
            "Ep 3 (Step 008695): Train loss 3.809, Val loss 4.281\n",
            "Ep 3 (Step 008700): Train loss 3.790, Val loss 4.276\n",
            "Ep 3 (Step 008705): Train loss 3.888, Val loss 4.267\n",
            "Ep 3 (Step 008710): Train loss 3.896, Val loss 4.265\n",
            "Ep 3 (Step 008715): Train loss 4.038, Val loss 4.276\n",
            "Ep 3 (Step 008720): Train loss 3.728, Val loss 4.302\n",
            "Ep 3 (Step 008725): Train loss 3.813, Val loss 4.293\n",
            "Ep 3 (Step 008730): Train loss 3.683, Val loss 4.271\n",
            "Ep 3 (Step 008735): Train loss 3.701, Val loss 4.271\n",
            "Ep 3 (Step 008740): Train loss 3.794, Val loss 4.289\n",
            "Ep 3 (Step 008745): Train loss 3.662, Val loss 4.309\n",
            "Ep 3 (Step 008750): Train loss 3.844, Val loss 4.289\n",
            "Ep 3 (Step 008755): Train loss 3.727, Val loss 4.267\n",
            "Ep 3 (Step 008760): Train loss 3.693, Val loss 4.256\n",
            "Ep 3 (Step 008765): Train loss 3.738, Val loss 4.248\n",
            "Ep 3 (Step 008770): Train loss 3.676, Val loss 4.252\n",
            "Ep 3 (Step 008775): Train loss 3.626, Val loss 4.265\n",
            "Ep 3 (Step 008780): Train loss 3.779, Val loss 4.276\n",
            "Ep 3 (Step 008785): Train loss 3.486, Val loss 4.263\n",
            "Ep 3 (Step 008790): Train loss 3.921, Val loss 4.251\n",
            "Ep 3 (Step 008795): Train loss 3.916, Val loss 4.248\n",
            "Ep 3 (Step 008800): Train loss 3.749, Val loss 4.275\n",
            "Ep 3 (Step 008805): Train loss 3.821, Val loss 4.257\n",
            "Ep 3 (Step 008810): Train loss 3.834, Val loss 4.245\n",
            "Ep 3 (Step 008815): Train loss 3.743, Val loss 4.247\n",
            "Ep 3 (Step 008820): Train loss 3.633, Val loss 4.239\n",
            "Ep 3 (Step 008825): Train loss 3.815, Val loss 4.243\n",
            "Ep 3 (Step 008830): Train loss 3.542, Val loss 4.244\n",
            "Ep 3 (Step 008835): Train loss 4.073, Val loss 4.235\n",
            "Ep 3 (Step 008840): Train loss 3.958, Val loss 4.234\n",
            "Ep 3 (Step 008845): Train loss 3.712, Val loss 4.238\n",
            "Ep 3 (Step 008850): Train loss 3.561, Val loss 4.225\n",
            "Ep 3 (Step 008855): Train loss 3.826, Val loss 4.236\n",
            "Ep 3 (Step 008860): Train loss 3.434, Val loss 4.228\n",
            "Ep 3 (Step 008865): Train loss 3.889, Val loss 4.229\n",
            "Ep 3 (Step 008870): Train loss 3.672, Val loss 4.238\n",
            "Ep 3 (Step 008875): Train loss 3.899, Val loss 4.254\n",
            "Ep 3 (Step 008880): Train loss 3.493, Val loss 4.231\n",
            "Ep 3 (Step 008885): Train loss 3.853, Val loss 4.236\n",
            "Ep 3 (Step 008890): Train loss 3.875, Val loss 4.245\n",
            "Ep 3 (Step 008895): Train loss 3.969, Val loss 4.257\n",
            "Ep 3 (Step 008900): Train loss 3.776, Val loss 4.254\n",
            "Ep 3 (Step 008905): Train loss 3.585, Val loss 4.241\n",
            "Ep 3 (Step 008910): Train loss 3.581, Val loss 4.240\n",
            "Ep 3 (Step 008915): Train loss 3.377, Val loss 4.247\n",
            "Ep 3 (Step 008920): Train loss 3.677, Val loss 4.251\n",
            "Ep 3 (Step 008925): Train loss 3.584, Val loss 4.244\n",
            "Ep 3 (Step 008930): Train loss 3.786, Val loss 4.248\n",
            "Every effort moves you, And, I am a man, And I am a little, And I am a man, And I am a little, And I am a man’ll be a little.  [_Exeunt._\n",
            "Ep 4 (Step 008935): Train loss 3.643, Val loss 4.239\n",
            "Ep 4 (Step 008940): Train loss 3.891, Val loss 4.243\n",
            "Ep 4 (Step 008945): Train loss 3.664, Val loss 4.241\n",
            "Ep 4 (Step 008950): Train loss 3.874, Val loss 4.235\n",
            "Ep 4 (Step 008955): Train loss 3.632, Val loss 4.250\n",
            "Ep 4 (Step 008960): Train loss 3.955, Val loss 4.266\n",
            "Ep 4 (Step 008965): Train loss 3.917, Val loss 4.276\n",
            "Ep 4 (Step 008970): Train loss 3.647, Val loss 4.259\n",
            "Ep 4 (Step 008975): Train loss 3.816, Val loss 4.261\n",
            "Ep 4 (Step 008980): Train loss 4.027, Val loss 4.282\n",
            "Ep 4 (Step 008985): Train loss 3.640, Val loss 4.301\n",
            "Ep 4 (Step 008990): Train loss 3.588, Val loss 4.279\n",
            "Ep 4 (Step 008995): Train loss 3.722, Val loss 4.274\n",
            "Ep 4 (Step 009000): Train loss 3.540, Val loss 4.259\n",
            "Ep 4 (Step 009005): Train loss 3.644, Val loss 4.268\n",
            "Ep 4 (Step 009010): Train loss 3.770, Val loss 4.283\n",
            "Ep 4 (Step 009015): Train loss 3.721, Val loss 4.286\n",
            "Ep 4 (Step 009020): Train loss 3.961, Val loss 4.300\n",
            "Ep 4 (Step 009025): Train loss 3.830, Val loss 4.283\n",
            "Ep 4 (Step 009030): Train loss 3.595, Val loss 4.265\n",
            "Ep 4 (Step 009035): Train loss 3.566, Val loss 4.261\n",
            "Ep 4 (Step 009040): Train loss 3.732, Val loss 4.256\n",
            "Ep 4 (Step 009045): Train loss 4.012, Val loss 4.254\n",
            "Ep 4 (Step 009050): Train loss 3.457, Val loss 4.267\n",
            "Ep 4 (Step 009055): Train loss 3.270, Val loss 4.291\n",
            "Ep 4 (Step 009060): Train loss 3.885, Val loss 4.285\n",
            "Ep 4 (Step 009065): Train loss 3.844, Val loss 4.269\n",
            "Ep 4 (Step 009070): Train loss 3.578, Val loss 4.252\n",
            "Ep 4 (Step 009075): Train loss 3.867, Val loss 4.246\n",
            "Ep 4 (Step 009080): Train loss 3.707, Val loss 4.250\n",
            "Ep 4 (Step 009085): Train loss 3.765, Val loss 4.243\n",
            "Ep 4 (Step 009090): Train loss 3.784, Val loss 4.254\n",
            "Ep 4 (Step 009095): Train loss 3.745, Val loss 4.257\n",
            "Ep 4 (Step 009100): Train loss 3.717, Val loss 4.262\n",
            "Ep 4 (Step 009105): Train loss 3.869, Val loss 4.282\n",
            "Ep 4 (Step 009110): Train loss 3.751, Val loss 4.273\n",
            "Ep 4 (Step 009115): Train loss 3.515, Val loss 4.268\n",
            "Ep 4 (Step 009120): Train loss 3.851, Val loss 4.256\n",
            "Ep 4 (Step 009125): Train loss 3.534, Val loss 4.263\n",
            "Ep 4 (Step 009130): Train loss 3.717, Val loss 4.272\n",
            "Ep 4 (Step 009135): Train loss 3.677, Val loss 4.257\n",
            "Ep 4 (Step 009140): Train loss 3.889, Val loss 4.256\n",
            "Ep 4 (Step 009145): Train loss 3.723, Val loss 4.267\n",
            "Ep 4 (Step 009150): Train loss 3.569, Val loss 4.276\n",
            "Ep 4 (Step 009155): Train loss 3.822, Val loss 4.268\n",
            "Ep 4 (Step 009160): Train loss 3.567, Val loss 4.265\n",
            "Ep 4 (Step 009165): Train loss 3.911, Val loss 4.274\n",
            "Ep 4 (Step 009170): Train loss 3.416, Val loss 4.289\n",
            "Ep 4 (Step 009175): Train loss 3.637, Val loss 4.279\n",
            "Ep 4 (Step 009180): Train loss 3.401, Val loss 4.271\n",
            "Ep 4 (Step 009185): Train loss 3.679, Val loss 4.288\n",
            "Ep 4 (Step 009190): Train loss 3.835, Val loss 4.313\n",
            "Ep 4 (Step 009195): Train loss 3.460, Val loss 4.298\n",
            "Ep 4 (Step 009200): Train loss 3.657, Val loss 4.292\n",
            "Ep 4 (Step 009205): Train loss 3.521, Val loss 4.309\n",
            "Ep 4 (Step 009210): Train loss 3.470, Val loss 4.316\n",
            "Ep 4 (Step 009215): Train loss 3.568, Val loss 4.317\n",
            "Ep 4 (Step 009220): Train loss 3.789, Val loss 4.304\n",
            "Ep 4 (Step 009225): Train loss 3.589, Val loss 4.293\n",
            "Ep 4 (Step 009230): Train loss 3.797, Val loss 4.285\n",
            "Ep 4 (Step 009235): Train loss 3.695, Val loss 4.279\n",
            "Ep 4 (Step 009240): Train loss 3.567, Val loss 4.270\n",
            "Ep 4 (Step 009245): Train loss 3.296, Val loss 4.267\n",
            "Ep 4 (Step 009250): Train loss 3.690, Val loss 4.267\n",
            "Ep 4 (Step 009255): Train loss 3.797, Val loss 4.290\n",
            "Ep 4 (Step 009260): Train loss 3.972, Val loss 4.318\n",
            "Ep 4 (Step 009265): Train loss 3.558, Val loss 4.299\n",
            "Ep 4 (Step 009270): Train loss 3.500, Val loss 4.290\n",
            "Ep 4 (Step 009275): Train loss 3.458, Val loss 4.305\n",
            "Ep 4 (Step 009280): Train loss 3.511, Val loss 4.304\n",
            "Ep 4 (Step 009285): Train loss 3.863, Val loss 4.297\n",
            "Ep 4 (Step 009290): Train loss 3.805, Val loss 4.304\n",
            "Ep 4 (Step 009295): Train loss 3.895, Val loss 4.301\n",
            "Ep 4 (Step 009300): Train loss 3.884, Val loss 4.289\n",
            "Ep 4 (Step 009305): Train loss 3.853, Val loss 4.285\n",
            "Ep 4 (Step 009310): Train loss 3.752, Val loss 4.282\n",
            "Ep 4 (Step 009315): Train loss 3.630, Val loss 4.300\n",
            "Ep 4 (Step 009320): Train loss 3.798, Val loss 4.270\n",
            "Ep 4 (Step 009325): Train loss 3.449, Val loss 4.267\n",
            "Ep 4 (Step 009330): Train loss 3.547, Val loss 4.272\n",
            "Ep 4 (Step 009335): Train loss 3.641, Val loss 4.286\n",
            "Ep 4 (Step 009340): Train loss 3.772, Val loss 4.280\n",
            "Ep 4 (Step 009345): Train loss 3.989, Val loss 4.289\n",
            "Ep 4 (Step 009350): Train loss 3.839, Val loss 4.283\n",
            "Ep 4 (Step 009355): Train loss 3.292, Val loss 4.266\n",
            "Ep 4 (Step 009360): Train loss 3.329, Val loss 4.271\n",
            "Ep 4 (Step 009365): Train loss 3.790, Val loss 4.284\n",
            "Ep 4 (Step 009370): Train loss 3.624, Val loss 4.291\n",
            "Ep 4 (Step 009375): Train loss 3.858, Val loss 4.286\n",
            "Ep 4 (Step 009380): Train loss 3.728, Val loss 4.281\n",
            "Ep 4 (Step 009385): Train loss 3.654, Val loss 4.279\n",
            "Ep 4 (Step 009390): Train loss 4.191, Val loss 4.284\n",
            "Ep 4 (Step 009395): Train loss 3.658, Val loss 4.282\n",
            "Ep 4 (Step 009400): Train loss 3.550, Val loss 4.298\n",
            "Ep 4 (Step 009405): Train loss 3.620, Val loss 4.302\n",
            "Ep 4 (Step 009410): Train loss 4.026, Val loss 4.272\n",
            "Ep 4 (Step 009415): Train loss 3.792, Val loss 4.263\n",
            "Ep 4 (Step 009420): Train loss 3.807, Val loss 4.270\n",
            "Ep 4 (Step 009425): Train loss 3.764, Val loss 4.285\n",
            "Ep 4 (Step 009430): Train loss 3.758, Val loss 4.277\n",
            "Ep 4 (Step 009435): Train loss 3.518, Val loss 4.274\n",
            "Ep 4 (Step 009440): Train loss 3.760, Val loss 4.256\n",
            "Ep 4 (Step 009445): Train loss 3.800, Val loss 4.264\n",
            "Ep 4 (Step 009450): Train loss 3.662, Val loss 4.252\n",
            "Ep 4 (Step 009455): Train loss 3.438, Val loss 4.248\n",
            "Ep 4 (Step 009460): Train loss 3.909, Val loss 4.245\n",
            "Ep 4 (Step 009465): Train loss 4.022, Val loss 4.236\n",
            "Ep 4 (Step 009470): Train loss 3.679, Val loss 4.243\n",
            "Ep 4 (Step 009475): Train loss 3.839, Val loss 4.252\n",
            "Ep 4 (Step 009480): Train loss 3.735, Val loss 4.257\n",
            "Ep 4 (Step 009485): Train loss 3.503, Val loss 4.247\n",
            "Ep 4 (Step 009490): Train loss 3.666, Val loss 4.253\n",
            "Ep 4 (Step 009495): Train loss 3.638, Val loss 4.254\n",
            "Ep 4 (Step 009500): Train loss 3.433, Val loss 4.262\n",
            "Ep 4 (Step 009505): Train loss 3.421, Val loss 4.248\n",
            "Ep 4 (Step 009510): Train loss 4.026, Val loss 4.247\n",
            "Ep 4 (Step 009515): Train loss 3.833, Val loss 4.252\n",
            "Ep 4 (Step 009520): Train loss 3.622, Val loss 4.264\n",
            "Ep 4 (Step 009525): Train loss 3.667, Val loss 4.278\n",
            "Ep 4 (Step 009530): Train loss 3.509, Val loss 4.279\n",
            "Ep 4 (Step 009535): Train loss 3.857, Val loss 4.261\n",
            "Ep 4 (Step 009540): Train loss 3.422, Val loss 4.251\n",
            "Ep 4 (Step 009545): Train loss 3.474, Val loss 4.259\n",
            "Ep 4 (Step 009550): Train loss 3.553, Val loss 4.271\n",
            "Ep 4 (Step 009555): Train loss 3.856, Val loss 4.271\n",
            "Ep 4 (Step 009560): Train loss 3.419, Val loss 4.261\n",
            "Ep 4 (Step 009565): Train loss 4.220, Val loss 4.260\n",
            "Ep 4 (Step 009570): Train loss 4.028, Val loss 4.259\n",
            "Ep 4 (Step 009575): Train loss 3.650, Val loss 4.253\n",
            "Ep 4 (Step 009580): Train loss 3.838, Val loss 4.263\n",
            "Ep 4 (Step 009585): Train loss 3.609, Val loss 4.254\n",
            "Ep 4 (Step 009590): Train loss 3.751, Val loss 4.246\n",
            "Ep 4 (Step 009595): Train loss 3.840, Val loss 4.249\n",
            "Ep 4 (Step 009600): Train loss 3.529, Val loss 4.247\n",
            "Ep 4 (Step 009605): Train loss 3.577, Val loss 4.242\n",
            "Ep 4 (Step 009610): Train loss 3.649, Val loss 4.247\n",
            "Ep 4 (Step 009615): Train loss 3.878, Val loss 4.245\n",
            "Ep 4 (Step 009620): Train loss 3.392, Val loss 4.256\n",
            "Ep 4 (Step 009625): Train loss 3.655, Val loss 4.256\n",
            "Ep 4 (Step 009630): Train loss 3.559, Val loss 4.265\n",
            "Ep 4 (Step 009635): Train loss 3.542, Val loss 4.259\n",
            "Ep 4 (Step 009640): Train loss 3.763, Val loss 4.257\n",
            "Ep 4 (Step 009645): Train loss 3.693, Val loss 4.250\n",
            "Ep 4 (Step 009650): Train loss 3.449, Val loss 4.242\n",
            "Ep 4 (Step 009655): Train loss 3.530, Val loss 4.242\n",
            "Ep 4 (Step 009660): Train loss 3.944, Val loss 4.240\n",
            "Ep 4 (Step 009665): Train loss 3.232, Val loss 4.241\n",
            "Ep 4 (Step 009670): Train loss 3.714, Val loss 4.247\n",
            "Ep 4 (Step 009675): Train loss 3.634, Val loss 4.254\n",
            "Ep 4 (Step 009680): Train loss 3.414, Val loss 4.235\n",
            "Ep 4 (Step 009685): Train loss 3.362, Val loss 4.221\n",
            "Ep 4 (Step 009690): Train loss 3.811, Val loss 4.231\n",
            "Ep 4 (Step 009695): Train loss 3.356, Val loss 4.253\n",
            "Ep 4 (Step 009700): Train loss 3.647, Val loss 4.261\n",
            "Ep 4 (Step 009705): Train loss 3.993, Val loss 4.245\n",
            "Ep 4 (Step 009710): Train loss 3.386, Val loss 4.245\n",
            "Ep 4 (Step 009715): Train loss 3.524, Val loss 4.234\n",
            "Ep 4 (Step 009720): Train loss 3.955, Val loss 4.224\n",
            "Ep 4 (Step 009725): Train loss 3.487, Val loss 4.227\n",
            "Ep 4 (Step 009730): Train loss 3.597, Val loss 4.260\n",
            "Ep 4 (Step 009735): Train loss 3.470, Val loss 4.254\n",
            "Ep 4 (Step 009740): Train loss 3.765, Val loss 4.248\n",
            "Ep 4 (Step 009745): Train loss 3.720, Val loss 4.237\n",
            "Ep 4 (Step 009750): Train loss 3.519, Val loss 4.233\n",
            "Ep 4 (Step 009755): Train loss 3.733, Val loss 4.225\n",
            "Ep 4 (Step 009760): Train loss 3.654, Val loss 4.226\n",
            "Ep 4 (Step 009765): Train loss 3.596, Val loss 4.223\n",
            "Ep 4 (Step 009770): Train loss 3.562, Val loss 4.228\n",
            "Ep 4 (Step 009775): Train loss 3.537, Val loss 4.237\n",
            "Ep 4 (Step 009780): Train loss 3.749, Val loss 4.239\n",
            "Ep 4 (Step 009785): Train loss 3.981, Val loss 4.239\n",
            "Ep 4 (Step 009790): Train loss 3.691, Val loss 4.242\n",
            "Ep 4 (Step 009795): Train loss 3.737, Val loss 4.250\n",
            "Ep 4 (Step 009800): Train loss 3.524, Val loss 4.243\n",
            "Ep 4 (Step 009805): Train loss 3.629, Val loss 4.236\n",
            "Ep 4 (Step 009810): Train loss 3.768, Val loss 4.239\n",
            "Ep 4 (Step 009815): Train loss 3.509, Val loss 4.250\n",
            "Ep 4 (Step 009820): Train loss 3.471, Val loss 4.273\n",
            "Ep 4 (Step 009825): Train loss 3.805, Val loss 4.266\n",
            "Ep 4 (Step 009830): Train loss 3.903, Val loss 4.243\n",
            "Ep 4 (Step 009835): Train loss 3.671, Val loss 4.232\n",
            "Ep 4 (Step 009840): Train loss 3.819, Val loss 4.238\n",
            "Ep 4 (Step 009845): Train loss 3.842, Val loss 4.233\n",
            "Ep 4 (Step 009850): Train loss 3.853, Val loss 4.230\n",
            "Ep 4 (Step 009855): Train loss 3.551, Val loss 4.239\n",
            "Ep 4 (Step 009860): Train loss 3.553, Val loss 4.248\n",
            "Ep 4 (Step 009865): Train loss 3.888, Val loss 4.246\n",
            "Ep 4 (Step 009870): Train loss 3.724, Val loss 4.258\n",
            "Ep 4 (Step 009875): Train loss 3.824, Val loss 4.239\n",
            "Ep 4 (Step 009880): Train loss 3.661, Val loss 4.230\n",
            "Ep 4 (Step 009885): Train loss 3.830, Val loss 4.242\n",
            "Ep 4 (Step 009890): Train loss 3.418, Val loss 4.255\n",
            "Ep 4 (Step 009895): Train loss 3.805, Val loss 4.258\n",
            "Ep 4 (Step 009900): Train loss 3.447, Val loss 4.242\n",
            "Ep 4 (Step 009905): Train loss 3.807, Val loss 4.258\n",
            "Ep 4 (Step 009910): Train loss 3.818, Val loss 4.280\n",
            "Ep 4 (Step 009915): Train loss 3.492, Val loss 4.264\n",
            "Ep 4 (Step 009920): Train loss 3.904, Val loss 4.242\n",
            "Ep 4 (Step 009925): Train loss 3.650, Val loss 4.257\n",
            "Ep 4 (Step 009930): Train loss 3.773, Val loss 4.277\n",
            "Ep 4 (Step 009935): Train loss 3.562, Val loss 4.286\n",
            "Ep 4 (Step 009940): Train loss 3.577, Val loss 4.270\n",
            "Ep 4 (Step 009945): Train loss 3.189, Val loss 4.260\n",
            "Ep 4 (Step 009950): Train loss 3.607, Val loss 4.268\n",
            "Ep 4 (Step 009955): Train loss 3.877, Val loss 4.282\n",
            "Ep 4 (Step 009960): Train loss 3.677, Val loss 4.271\n",
            "Ep 4 (Step 009965): Train loss 3.831, Val loss 4.253\n",
            "Ep 4 (Step 009970): Train loss 3.557, Val loss 4.246\n",
            "Ep 4 (Step 009975): Train loss 3.826, Val loss 4.236\n",
            "Ep 4 (Step 009980): Train loss 3.788, Val loss 4.236\n",
            "Ep 4 (Step 009985): Train loss 3.680, Val loss 4.253\n",
            "Ep 4 (Step 009990): Train loss 3.670, Val loss 4.251\n",
            "Ep 4 (Step 009995): Train loss 3.464, Val loss 4.238\n",
            "Ep 4 (Step 010000): Train loss 3.828, Val loss 4.239\n",
            "Ep 4 (Step 010005): Train loss 3.346, Val loss 4.249\n",
            "Ep 4 (Step 010010): Train loss 3.743, Val loss 4.267\n",
            "Ep 4 (Step 010015): Train loss 3.642, Val loss 4.263\n",
            "Ep 4 (Step 010020): Train loss 3.517, Val loss 4.256\n",
            "Ep 4 (Step 010025): Train loss 3.697, Val loss 4.239\n",
            "Ep 4 (Step 010030): Train loss 3.376, Val loss 4.222\n",
            "Ep 4 (Step 010035): Train loss 3.702, Val loss 4.228\n",
            "Ep 4 (Step 010040): Train loss 3.605, Val loss 4.243\n",
            "Ep 4 (Step 010045): Train loss 3.518, Val loss 4.262\n",
            "Ep 4 (Step 010050): Train loss 3.442, Val loss 4.270\n",
            "Ep 4 (Step 010055): Train loss 3.630, Val loss 4.268\n",
            "Ep 4 (Step 010060): Train loss 3.417, Val loss 4.255\n",
            "Ep 4 (Step 010065): Train loss 3.756, Val loss 4.264\n",
            "Ep 4 (Step 010070): Train loss 3.676, Val loss 4.276\n",
            "Ep 4 (Step 010075): Train loss 3.839, Val loss 4.261\n",
            "Ep 4 (Step 010080): Train loss 3.320, Val loss 4.263\n",
            "Ep 4 (Step 010085): Train loss 4.009, Val loss 4.279\n",
            "Ep 4 (Step 010090): Train loss 3.954, Val loss 4.268\n",
            "Ep 4 (Step 010095): Train loss 3.975, Val loss 4.256\n",
            "Ep 4 (Step 010100): Train loss 3.909, Val loss 4.256\n",
            "Ep 4 (Step 010105): Train loss 3.471, Val loss 4.264\n",
            "Ep 4 (Step 010110): Train loss 3.548, Val loss 4.270\n",
            "Ep 4 (Step 010115): Train loss 3.642, Val loss 4.264\n",
            "Ep 4 (Step 010120): Train loss 3.425, Val loss 4.249\n",
            "Ep 4 (Step 010125): Train loss 3.585, Val loss 4.250\n",
            "Ep 4 (Step 010130): Train loss 3.724, Val loss 4.250\n",
            "Ep 4 (Step 010135): Train loss 3.516, Val loss 4.246\n",
            "Ep 4 (Step 010140): Train loss 3.565, Val loss 4.258\n",
            "Ep 4 (Step 010145): Train loss 3.588, Val loss 4.247\n",
            "Ep 4 (Step 010150): Train loss 3.796, Val loss 4.235\n",
            "Ep 4 (Step 010155): Train loss 3.553, Val loss 4.249\n",
            "Ep 4 (Step 010160): Train loss 3.619, Val loss 4.243\n",
            "Ep 4 (Step 010165): Train loss 3.642, Val loss 4.232\n",
            "Ep 4 (Step 010170): Train loss 3.499, Val loss 4.236\n",
            "Ep 4 (Step 010175): Train loss 3.415, Val loss 4.245\n",
            "Ep 4 (Step 010180): Train loss 3.208, Val loss 4.247\n",
            "Ep 4 (Step 010185): Train loss 3.904, Val loss 4.236\n",
            "Ep 4 (Step 010190): Train loss 3.941, Val loss 4.229\n",
            "Ep 4 (Step 010195): Train loss 3.636, Val loss 4.231\n",
            "Ep 4 (Step 010200): Train loss 3.974, Val loss 4.229\n",
            "Ep 4 (Step 010205): Train loss 3.698, Val loss 4.232\n",
            "Ep 4 (Step 010210): Train loss 3.979, Val loss 4.237\n",
            "Ep 4 (Step 010215): Train loss 3.568, Val loss 4.244\n",
            "Ep 4 (Step 010220): Train loss 3.614, Val loss 4.236\n",
            "Ep 4 (Step 010225): Train loss 3.673, Val loss 4.235\n",
            "Ep 4 (Step 010230): Train loss 3.647, Val loss 4.249\n",
            "Ep 4 (Step 010235): Train loss 3.798, Val loss 4.270\n",
            "Ep 4 (Step 010240): Train loss 3.745, Val loss 4.269\n",
            "Ep 4 (Step 010245): Train loss 3.448, Val loss 4.258\n",
            "Ep 4 (Step 010250): Train loss 3.952, Val loss 4.242\n",
            "Ep 4 (Step 010255): Train loss 3.697, Val loss 4.239\n",
            "Ep 4 (Step 010260): Train loss 3.587, Val loss 4.271\n",
            "Ep 4 (Step 010265): Train loss 3.781, Val loss 4.274\n",
            "Ep 4 (Step 010270): Train loss 3.406, Val loss 4.258\n",
            "Ep 4 (Step 010275): Train loss 3.980, Val loss 4.251\n",
            "Ep 4 (Step 010280): Train loss 3.763, Val loss 4.247\n",
            "Ep 4 (Step 010285): Train loss 3.693, Val loss 4.234\n",
            "Ep 4 (Step 010290): Train loss 3.671, Val loss 4.226\n",
            "Ep 4 (Step 010295): Train loss 3.644, Val loss 4.232\n",
            "Ep 4 (Step 010300): Train loss 3.688, Val loss 4.239\n",
            "Ep 4 (Step 010305): Train loss 3.379, Val loss 4.250\n",
            "Ep 4 (Step 010310): Train loss 3.333, Val loss 4.234\n",
            "Ep 4 (Step 010315): Train loss 3.702, Val loss 4.230\n",
            "Ep 4 (Step 010320): Train loss 3.589, Val loss 4.241\n",
            "Ep 4 (Step 010325): Train loss 3.643, Val loss 4.249\n",
            "Ep 4 (Step 010330): Train loss 3.579, Val loss 4.249\n",
            "Ep 4 (Step 010335): Train loss 3.917, Val loss 4.225\n",
            "Ep 4 (Step 010340): Train loss 3.788, Val loss 4.218\n",
            "Ep 4 (Step 010345): Train loss 3.652, Val loss 4.223\n",
            "Ep 4 (Step 010350): Train loss 3.645, Val loss 4.228\n",
            "Ep 4 (Step 010355): Train loss 3.709, Val loss 4.225\n",
            "Ep 4 (Step 010360): Train loss 3.972, Val loss 4.216\n",
            "Ep 4 (Step 010365): Train loss 3.762, Val loss 4.212\n",
            "Ep 4 (Step 010370): Train loss 3.567, Val loss 4.223\n",
            "Ep 4 (Step 010375): Train loss 3.458, Val loss 4.236\n",
            "Ep 4 (Step 010380): Train loss 3.670, Val loss 4.240\n",
            "Ep 4 (Step 010385): Train loss 3.583, Val loss 4.237\n",
            "Ep 4 (Step 010390): Train loss 3.366, Val loss 4.240\n",
            "Ep 4 (Step 010395): Train loss 3.682, Val loss 4.255\n",
            "Ep 4 (Step 010400): Train loss 3.488, Val loss 4.249\n",
            "Ep 4 (Step 010405): Train loss 3.669, Val loss 4.236\n",
            "Ep 4 (Step 010410): Train loss 3.505, Val loss 4.225\n",
            "Ep 4 (Step 010415): Train loss 3.231, Val loss 4.226\n",
            "Ep 4 (Step 010420): Train loss 3.652, Val loss 4.221\n",
            "Ep 4 (Step 010425): Train loss 3.513, Val loss 4.234\n",
            "Ep 4 (Step 010430): Train loss 3.433, Val loss 4.238\n",
            "Ep 4 (Step 010435): Train loss 3.720, Val loss 4.247\n",
            "Ep 4 (Step 010440): Train loss 3.744, Val loss 4.247\n",
            "Ep 4 (Step 010445): Train loss 3.416, Val loss 4.236\n",
            "Ep 4 (Step 010450): Train loss 3.527, Val loss 4.227\n",
            "Ep 4 (Step 010455): Train loss 3.657, Val loss 4.205\n",
            "Ep 4 (Step 010460): Train loss 3.600, Val loss 4.206\n",
            "Ep 4 (Step 010465): Train loss 3.432, Val loss 4.209\n",
            "Ep 4 (Step 010470): Train loss 3.630, Val loss 4.211\n",
            "Ep 4 (Step 010475): Train loss 3.645, Val loss 4.211\n",
            "Ep 4 (Step 010480): Train loss 3.562, Val loss 4.204\n",
            "Ep 4 (Step 010485): Train loss 3.825, Val loss 4.190\n",
            "Ep 4 (Step 010490): Train loss 3.705, Val loss 4.188\n",
            "Ep 4 (Step 010495): Train loss 3.367, Val loss 4.189\n",
            "Ep 4 (Step 010500): Train loss 3.429, Val loss 4.195\n",
            "Ep 4 (Step 010505): Train loss 3.982, Val loss 4.204\n",
            "Ep 4 (Step 010510): Train loss 3.711, Val loss 4.200\n",
            "Ep 4 (Step 010515): Train loss 3.472, Val loss 4.208\n",
            "Ep 4 (Step 010520): Train loss 3.532, Val loss 4.239\n",
            "Ep 4 (Step 010525): Train loss 3.273, Val loss 4.234\n",
            "Ep 4 (Step 010530): Train loss 3.810, Val loss 4.201\n",
            "Ep 4 (Step 010535): Train loss 3.806, Val loss 4.187\n",
            "Ep 4 (Step 010540): Train loss 3.773, Val loss 4.183\n",
            "Ep 4 (Step 010545): Train loss 3.522, Val loss 4.197\n",
            "Ep 4 (Step 010550): Train loss 3.742, Val loss 4.208\n",
            "Ep 4 (Step 010555): Train loss 3.062, Val loss 4.207\n",
            "Ep 4 (Step 010560): Train loss 3.672, Val loss 4.220\n",
            "Ep 4 (Step 010565): Train loss 3.587, Val loss 4.214\n",
            "Ep 4 (Step 010570): Train loss 3.740, Val loss 4.204\n",
            "Ep 4 (Step 010575): Train loss 3.227, Val loss 4.208\n",
            "Ep 4 (Step 010580): Train loss 3.658, Val loss 4.192\n",
            "Ep 4 (Step 010585): Train loss 3.655, Val loss 4.181\n",
            "Ep 4 (Step 010590): Train loss 3.391, Val loss 4.192\n",
            "Ep 4 (Step 010595): Train loss 3.618, Val loss 4.209\n",
            "Ep 4 (Step 010600): Train loss 3.670, Val loss 4.202\n",
            "Ep 4 (Step 010605): Train loss 3.584, Val loss 4.195\n",
            "Ep 4 (Step 010610): Train loss 3.800, Val loss 4.209\n",
            "Ep 4 (Step 010615): Train loss 3.749, Val loss 4.208\n",
            "Ep 4 (Step 010620): Train loss 3.574, Val loss 4.201\n",
            "Ep 4 (Step 010625): Train loss 3.600, Val loss 4.207\n",
            "Ep 4 (Step 010630): Train loss 3.542, Val loss 4.217\n",
            "Ep 4 (Step 010635): Train loss 3.671, Val loss 4.202\n",
            "Ep 4 (Step 010640): Train loss 3.535, Val loss 4.199\n",
            "Ep 4 (Step 010645): Train loss 3.345, Val loss 4.210\n",
            "Ep 4 (Step 010650): Train loss 3.823, Val loss 4.218\n",
            "Ep 4 (Step 010655): Train loss 3.263, Val loss 4.218\n",
            "Ep 4 (Step 010660): Train loss 3.283, Val loss 4.219\n",
            "Ep 4 (Step 010665): Train loss 3.848, Val loss 4.206\n",
            "Ep 4 (Step 010670): Train loss 3.475, Val loss 4.220\n",
            "Ep 4 (Step 010675): Train loss 3.535, Val loss 4.219\n",
            "Ep 4 (Step 010680): Train loss 3.599, Val loss 4.213\n",
            "Ep 4 (Step 010685): Train loss 3.561, Val loss 4.207\n",
            "Ep 4 (Step 010690): Train loss 3.225, Val loss 4.210\n",
            "Ep 4 (Step 010695): Train loss 3.723, Val loss 4.194\n",
            "Ep 4 (Step 010700): Train loss 3.902, Val loss 4.194\n",
            "Ep 4 (Step 010705): Train loss 3.636, Val loss 4.208\n",
            "Ep 4 (Step 010710): Train loss 3.438, Val loss 4.199\n",
            "Ep 4 (Step 010715): Train loss 3.452, Val loss 4.193\n",
            "Ep 4 (Step 010720): Train loss 3.524, Val loss 4.200\n",
            "Ep 4 (Step 010725): Train loss 3.660, Val loss 4.226\n",
            "Ep 4 (Step 010730): Train loss 3.286, Val loss 4.205\n",
            "Ep 4 (Step 010735): Train loss 3.540, Val loss 4.196\n",
            "Ep 4 (Step 010740): Train loss 3.457, Val loss 4.190\n",
            "Ep 4 (Step 010745): Train loss 3.732, Val loss 4.192\n",
            "Ep 4 (Step 010750): Train loss 3.550, Val loss 4.199\n",
            "Ep 4 (Step 010755): Train loss 3.655, Val loss 4.193\n",
            "Ep 4 (Step 010760): Train loss 3.851, Val loss 4.196\n",
            "Ep 4 (Step 010765): Train loss 3.437, Val loss 4.186\n",
            "Ep 4 (Step 010770): Train loss 3.732, Val loss 4.186\n",
            "Ep 4 (Step 010775): Train loss 3.477, Val loss 4.173\n",
            "Ep 4 (Step 010780): Train loss 3.793, Val loss 4.165\n",
            "Ep 4 (Step 010785): Train loss 3.792, Val loss 4.179\n",
            "Ep 4 (Step 010790): Train loss 3.764, Val loss 4.199\n",
            "Ep 4 (Step 010795): Train loss 3.284, Val loss 4.182\n",
            "Ep 4 (Step 010800): Train loss 3.330, Val loss 4.180\n",
            "Ep 4 (Step 010805): Train loss 3.214, Val loss 4.193\n",
            "Ep 4 (Step 010810): Train loss 3.880, Val loss 4.190\n",
            "Ep 4 (Step 010815): Train loss 3.589, Val loss 4.170\n",
            "Ep 4 (Step 010820): Train loss 3.882, Val loss 4.169\n",
            "Ep 4 (Step 010825): Train loss 3.689, Val loss 4.165\n",
            "Ep 4 (Step 010830): Train loss 3.486, Val loss 4.170\n",
            "Ep 4 (Step 010835): Train loss 3.658, Val loss 4.182\n",
            "Ep 4 (Step 010840): Train loss 3.419, Val loss 4.181\n",
            "Ep 4 (Step 010845): Train loss 3.578, Val loss 4.178\n",
            "Ep 4 (Step 010850): Train loss 3.906, Val loss 4.173\n",
            "Ep 4 (Step 010855): Train loss 3.363, Val loss 4.172\n",
            "Ep 4 (Step 010860): Train loss 3.330, Val loss 4.177\n",
            "Ep 4 (Step 010865): Train loss 3.750, Val loss 4.179\n",
            "Ep 4 (Step 010870): Train loss 3.167, Val loss 4.177\n",
            "Ep 4 (Step 010875): Train loss 3.859, Val loss 4.195\n",
            "Ep 4 (Step 010880): Train loss 3.403, Val loss 4.175\n",
            "Ep 4 (Step 010885): Train loss 3.471, Val loss 4.167\n",
            "Ep 4 (Step 010890): Train loss 3.454, Val loss 4.171\n",
            "Ep 4 (Step 010895): Train loss 3.495, Val loss 4.179\n",
            "Ep 4 (Step 010900): Train loss 3.584, Val loss 4.179\n",
            "Ep 4 (Step 010905): Train loss 3.643, Val loss 4.160\n",
            "Ep 4 (Step 010910): Train loss 3.431, Val loss 4.152\n",
            "Ep 4 (Step 010915): Train loss 3.412, Val loss 4.168\n",
            "Ep 4 (Step 010920): Train loss 3.344, Val loss 4.174\n",
            "Ep 4 (Step 010925): Train loss 3.925, Val loss 4.179\n",
            "Ep 4 (Step 010930): Train loss 3.275, Val loss 4.179\n",
            "Ep 4 (Step 010935): Train loss 3.695, Val loss 4.164\n",
            "Ep 4 (Step 010940): Train loss 3.708, Val loss 4.153\n",
            "Ep 4 (Step 010945): Train loss 3.785, Val loss 4.172\n",
            "Ep 4 (Step 010950): Train loss 3.923, Val loss 4.175\n",
            "Ep 4 (Step 010955): Train loss 3.771, Val loss 4.179\n",
            "Ep 4 (Step 010960): Train loss 3.886, Val loss 4.177\n",
            "Ep 4 (Step 010965): Train loss 3.724, Val loss 4.178\n",
            "Ep 4 (Step 010970): Train loss 3.463, Val loss 4.159\n",
            "Ep 4 (Step 010975): Train loss 3.627, Val loss 4.157\n",
            "Ep 4 (Step 010980): Train loss 3.391, Val loss 4.168\n",
            "Ep 4 (Step 010985): Train loss 3.659, Val loss 4.164\n",
            "Ep 4 (Step 010990): Train loss 3.353, Val loss 4.143\n",
            "Ep 4 (Step 010995): Train loss 3.519, Val loss 4.148\n",
            "Ep 4 (Step 011000): Train loss 3.585, Val loss 4.176\n",
            "Ep 4 (Step 011005): Train loss 3.541, Val loss 4.174\n",
            "Ep 4 (Step 011010): Train loss 3.473, Val loss 4.159\n",
            "Ep 4 (Step 011015): Train loss 3.701, Val loss 4.154\n",
            "Ep 4 (Step 011020): Train loss 3.532, Val loss 4.162\n",
            "Ep 4 (Step 011025): Train loss 3.754, Val loss 4.164\n",
            "Ep 4 (Step 011030): Train loss 3.511, Val loss 4.161\n",
            "Ep 4 (Step 011035): Train loss 3.600, Val loss 4.173\n",
            "Ep 4 (Step 011040): Train loss 3.798, Val loss 4.174\n",
            "Ep 4 (Step 011045): Train loss 3.416, Val loss 4.155\n",
            "Ep 4 (Step 011050): Train loss 3.598, Val loss 4.146\n",
            "Ep 4 (Step 011055): Train loss 3.461, Val loss 4.155\n",
            "Ep 4 (Step 011060): Train loss 3.668, Val loss 4.165\n",
            "Ep 4 (Step 011065): Train loss 3.527, Val loss 4.148\n",
            "Ep 4 (Step 011070): Train loss 3.689, Val loss 4.145\n",
            "Ep 4 (Step 011075): Train loss 3.668, Val loss 4.157\n",
            "Ep 4 (Step 011080): Train loss 3.637, Val loss 4.166\n",
            "Ep 4 (Step 011085): Train loss 3.280, Val loss 4.162\n",
            "Ep 4 (Step 011090): Train loss 3.793, Val loss 4.163\n",
            "Ep 4 (Step 011095): Train loss 3.423, Val loss 4.171\n",
            "Ep 4 (Step 011100): Train loss 3.603, Val loss 4.171\n",
            "Ep 4 (Step 011105): Train loss 3.326, Val loss 4.179\n",
            "Ep 4 (Step 011110): Train loss 3.394, Val loss 4.184\n",
            "Ep 4 (Step 011115): Train loss 3.318, Val loss 4.178\n",
            "Ep 4 (Step 011120): Train loss 3.679, Val loss 4.177\n",
            "Ep 4 (Step 011125): Train loss 3.313, Val loss 4.162\n",
            "Ep 4 (Step 011130): Train loss 3.594, Val loss 4.164\n",
            "Ep 4 (Step 011135): Train loss 3.361, Val loss 4.182\n",
            "Ep 4 (Step 011140): Train loss 3.644, Val loss 4.185\n",
            "Ep 4 (Step 011145): Train loss 3.179, Val loss 4.164\n",
            "Ep 4 (Step 011150): Train loss 3.507, Val loss 4.154\n",
            "Ep 4 (Step 011155): Train loss 3.741, Val loss 4.166\n",
            "Ep 4 (Step 011160): Train loss 2.981, Val loss 4.170\n",
            "Ep 4 (Step 011165): Train loss 3.397, Val loss 4.162\n",
            "Ep 4 (Step 011170): Train loss 3.583, Val loss 4.153\n",
            "Ep 4 (Step 011175): Train loss 3.571, Val loss 4.150\n",
            "Ep 4 (Step 011180): Train loss 3.553, Val loss 4.156\n",
            "Ep 4 (Step 011185): Train loss 3.532, Val loss 4.162\n",
            "Ep 4 (Step 011190): Train loss 3.619, Val loss 4.166\n",
            "Ep 4 (Step 011195): Train loss 3.651, Val loss 4.166\n",
            "Ep 4 (Step 011200): Train loss 3.488, Val loss 4.167\n",
            "Ep 4 (Step 011205): Train loss 3.483, Val loss 4.152\n",
            "Ep 4 (Step 011210): Train loss 3.611, Val loss 4.142\n",
            "Ep 4 (Step 011215): Train loss 3.513, Val loss 4.143\n",
            "Ep 4 (Step 011220): Train loss 3.527, Val loss 4.142\n",
            "Ep 4 (Step 011225): Train loss 3.609, Val loss 4.153\n",
            "Ep 4 (Step 011230): Train loss 3.348, Val loss 4.162\n",
            "Ep 4 (Step 011235): Train loss 3.566, Val loss 4.176\n",
            "Ep 4 (Step 011240): Train loss 3.694, Val loss 4.185\n",
            "Ep 4 (Step 011245): Train loss 3.409, Val loss 4.166\n",
            "Ep 4 (Step 011250): Train loss 3.933, Val loss 4.139\n",
            "Ep 4 (Step 011255): Train loss 3.480, Val loss 4.158\n",
            "Ep 4 (Step 011260): Train loss 3.876, Val loss 4.186\n",
            "Ep 4 (Step 011265): Train loss 3.435, Val loss 4.208\n",
            "Ep 4 (Step 011270): Train loss 3.618, Val loss 4.203\n",
            "Ep 4 (Step 011275): Train loss 3.441, Val loss 4.192\n",
            "Ep 4 (Step 011280): Train loss 3.849, Val loss 4.176\n",
            "Ep 4 (Step 011285): Train loss 3.701, Val loss 4.177\n",
            "Ep 4 (Step 011290): Train loss 3.632, Val loss 4.175\n",
            "Ep 4 (Step 011295): Train loss 3.721, Val loss 4.177\n",
            "Ep 4 (Step 011300): Train loss 3.814, Val loss 4.195\n",
            "Ep 4 (Step 011305): Train loss 3.381, Val loss 4.184\n",
            "Ep 4 (Step 011310): Train loss 3.734, Val loss 4.177\n",
            "Ep 4 (Step 011315): Train loss 3.695, Val loss 4.173\n",
            "Ep 4 (Step 011320): Train loss 3.542, Val loss 4.179\n",
            "Ep 4 (Step 011325): Train loss 3.376, Val loss 4.190\n",
            "Ep 4 (Step 011330): Train loss 3.319, Val loss 4.199\n",
            "Ep 4 (Step 011335): Train loss 3.719, Val loss 4.188\n",
            "Ep 4 (Step 011340): Train loss 3.472, Val loss 4.182\n",
            "Ep 4 (Step 011345): Train loss 3.511, Val loss 4.184\n",
            "Ep 4 (Step 011350): Train loss 3.571, Val loss 4.180\n",
            "Ep 4 (Step 011355): Train loss 3.415, Val loss 4.192\n",
            "Ep 4 (Step 011360): Train loss 3.690, Val loss 4.193\n",
            "Ep 4 (Step 011365): Train loss 3.564, Val loss 4.203\n",
            "Ep 4 (Step 011370): Train loss 3.456, Val loss 4.201\n",
            "Ep 4 (Step 011375): Train loss 3.531, Val loss 4.180\n",
            "Ep 4 (Step 011380): Train loss 3.608, Val loss 4.177\n",
            "Ep 4 (Step 011385): Train loss 3.358, Val loss 4.183\n",
            "Ep 4 (Step 011390): Train loss 3.372, Val loss 4.184\n",
            "Ep 4 (Step 011395): Train loss 3.748, Val loss 4.185\n",
            "Ep 4 (Step 011400): Train loss 3.472, Val loss 4.162\n",
            "Ep 4 (Step 011405): Train loss 3.514, Val loss 4.148\n",
            "Ep 4 (Step 011410): Train loss 3.502, Val loss 4.145\n",
            "Ep 4 (Step 011415): Train loss 3.601, Val loss 4.158\n",
            "Ep 4 (Step 011420): Train loss 3.669, Val loss 4.156\n",
            "Ep 4 (Step 011425): Train loss 3.658, Val loss 4.149\n",
            "Ep 4 (Step 011430): Train loss 3.614, Val loss 4.150\n",
            "Ep 4 (Step 011435): Train loss 3.623, Val loss 4.155\n",
            "Ep 4 (Step 011440): Train loss 3.230, Val loss 4.166\n",
            "Ep 4 (Step 011445): Train loss 3.600, Val loss 4.166\n",
            "Ep 4 (Step 011450): Train loss 3.564, Val loss 4.153\n",
            "Ep 4 (Step 011455): Train loss 3.445, Val loss 4.152\n",
            "Ep 4 (Step 011460): Train loss 3.525, Val loss 4.159\n",
            "Ep 4 (Step 011465): Train loss 3.563, Val loss 4.152\n",
            "Ep 4 (Step 011470): Train loss 3.520, Val loss 4.144\n",
            "Ep 4 (Step 011475): Train loss 3.324, Val loss 4.143\n",
            "Ep 4 (Step 011480): Train loss 3.503, Val loss 4.146\n",
            "Ep 4 (Step 011485): Train loss 3.748, Val loss 4.166\n",
            "Ep 4 (Step 011490): Train loss 3.557, Val loss 4.170\n",
            "Ep 4 (Step 011495): Train loss 3.510, Val loss 4.166\n",
            "Ep 4 (Step 011500): Train loss 3.018, Val loss 4.154\n",
            "Ep 4 (Step 011505): Train loss 3.460, Val loss 4.149\n",
            "Ep 4 (Step 011510): Train loss 3.756, Val loss 4.159\n",
            "Ep 4 (Step 011515): Train loss 3.558, Val loss 4.159\n",
            "Ep 4 (Step 011520): Train loss 3.525, Val loss 4.167\n",
            "Ep 4 (Step 011525): Train loss 3.428, Val loss 4.156\n",
            "Ep 4 (Step 011530): Train loss 3.753, Val loss 4.149\n",
            "Ep 4 (Step 011535): Train loss 3.618, Val loss 4.155\n",
            "Ep 4 (Step 011540): Train loss 3.547, Val loss 4.156\n",
            "Ep 4 (Step 011545): Train loss 3.226, Val loss 4.167\n",
            "Ep 4 (Step 011550): Train loss 3.564, Val loss 4.158\n",
            "Ep 4 (Step 011555): Train loss 3.682, Val loss 4.149\n",
            "Ep 4 (Step 011560): Train loss 3.535, Val loss 4.143\n",
            "Ep 4 (Step 011565): Train loss 3.392, Val loss 4.146\n",
            "Ep 4 (Step 011570): Train loss 3.199, Val loss 4.152\n",
            "Ep 4 (Step 011575): Train loss 3.581, Val loss 4.163\n",
            "Ep 4 (Step 011580): Train loss 3.534, Val loss 4.175\n",
            "Ep 4 (Step 011585): Train loss 3.563, Val loss 4.179\n",
            "Ep 4 (Step 011590): Train loss 3.670, Val loss 4.173\n",
            "Ep 4 (Step 011595): Train loss 3.679, Val loss 4.154\n",
            "Ep 4 (Step 011600): Train loss 3.337, Val loss 4.144\n",
            "Ep 4 (Step 011605): Train loss 3.319, Val loss 4.157\n",
            "Ep 4 (Step 011610): Train loss 3.600, Val loss 4.165\n",
            "Ep 4 (Step 011615): Train loss 3.315, Val loss 4.172\n",
            "Ep 4 (Step 011620): Train loss 3.565, Val loss 4.163\n",
            "Ep 4 (Step 011625): Train loss 3.724, Val loss 4.146\n",
            "Ep 4 (Step 011630): Train loss 3.910, Val loss 4.144\n",
            "Ep 4 (Step 011635): Train loss 3.598, Val loss 4.141\n",
            "Ep 4 (Step 011640): Train loss 3.486, Val loss 4.151\n",
            "Ep 4 (Step 011645): Train loss 3.602, Val loss 4.151\n",
            "Ep 4 (Step 011650): Train loss 3.522, Val loss 4.145\n",
            "Ep 4 (Step 011655): Train loss 3.674, Val loss 4.145\n",
            "Ep 4 (Step 011660): Train loss 3.626, Val loss 4.162\n",
            "Ep 4 (Step 011665): Train loss 3.700, Val loss 4.162\n",
            "Ep 4 (Step 011670): Train loss 3.934, Val loss 4.145\n",
            "Ep 4 (Step 011675): Train loss 3.325, Val loss 4.135\n",
            "Ep 4 (Step 011680): Train loss 3.548, Val loss 4.139\n",
            "Ep 4 (Step 011685): Train loss 3.886, Val loss 4.145\n",
            "Ep 4 (Step 011690): Train loss 3.500, Val loss 4.143\n",
            "Ep 4 (Step 011695): Train loss 3.455, Val loss 4.146\n",
            "Ep 4 (Step 011700): Train loss 3.466, Val loss 4.143\n",
            "Ep 4 (Step 011705): Train loss 3.585, Val loss 4.138\n",
            "Ep 4 (Step 011710): Train loss 3.590, Val loss 4.136\n",
            "Ep 4 (Step 011715): Train loss 3.412, Val loss 4.140\n",
            "Ep 4 (Step 011720): Train loss 3.591, Val loss 4.158\n",
            "Ep 4 (Step 011725): Train loss 3.666, Val loss 4.177\n",
            "Ep 4 (Step 011730): Train loss 3.575, Val loss 4.156\n",
            "Ep 4 (Step 011735): Train loss 3.504, Val loss 4.141\n",
            "Ep 4 (Step 011740): Train loss 3.208, Val loss 4.135\n",
            "Ep 4 (Step 011745): Train loss 3.283, Val loss 4.139\n",
            "Ep 4 (Step 011750): Train loss 3.468, Val loss 4.139\n",
            "Ep 4 (Step 011755): Train loss 3.644, Val loss 4.134\n",
            "Ep 4 (Step 011760): Train loss 3.729, Val loss 4.127\n",
            "Ep 4 (Step 011765): Train loss 3.390, Val loss 4.140\n",
            "Ep 4 (Step 011770): Train loss 3.545, Val loss 4.146\n",
            "Ep 4 (Step 011775): Train loss 3.761, Val loss 4.156\n",
            "Ep 4 (Step 011780): Train loss 3.304, Val loss 4.153\n",
            "Ep 4 (Step 011785): Train loss 3.430, Val loss 4.143\n",
            "Ep 4 (Step 011790): Train loss 3.445, Val loss 4.150\n",
            "Ep 4 (Step 011795): Train loss 3.512, Val loss 4.145\n",
            "Ep 4 (Step 011800): Train loss 3.544, Val loss 4.141\n",
            "Ep 4 (Step 011805): Train loss 3.420, Val loss 4.149\n",
            "Ep 4 (Step 011810): Train loss 3.755, Val loss 4.150\n",
            "Ep 4 (Step 011815): Train loss 3.369, Val loss 4.150\n",
            "Ep 4 (Step 011820): Train loss 3.734, Val loss 4.150\n",
            "Ep 4 (Step 011825): Train loss 3.045, Val loss 4.148\n",
            "Ep 4 (Step 011830): Train loss 3.459, Val loss 4.147\n",
            "Ep 4 (Step 011835): Train loss 3.735, Val loss 4.146\n",
            "Ep 4 (Step 011840): Train loss 3.384, Val loss 4.140\n",
            "Ep 4 (Step 011845): Train loss 3.413, Val loss 4.141\n",
            "Ep 4 (Step 011850): Train loss 3.354, Val loss 4.144\n",
            "Ep 4 (Step 011855): Train loss 3.602, Val loss 4.140\n",
            "Ep 4 (Step 011860): Train loss 3.414, Val loss 4.147\n",
            "Ep 4 (Step 011865): Train loss 3.293, Val loss 4.141\n",
            "Ep 4 (Step 011870): Train loss 3.423, Val loss 4.127\n",
            "Ep 4 (Step 011875): Train loss 3.628, Val loss 4.129\n",
            "Ep 4 (Step 011880): Train loss 3.700, Val loss 4.137\n",
            "Ep 4 (Step 011885): Train loss 3.470, Val loss 4.134\n",
            "Ep 4 (Step 011890): Train loss 3.493, Val loss 4.127\n",
            "Ep 4 (Step 011895): Train loss 3.430, Val loss 4.116\n",
            "Ep 4 (Step 011900): Train loss 3.528, Val loss 4.122\n",
            "Ep 4 (Step 011905): Train loss 3.281, Val loss 4.123\n",
            "Every effort moves you, And, as I am, And I am glad to be, and I am, And I am a man’s son, And I am a man’s son, And, as I am a man\n",
            "Ep 5 (Step 011910): Train loss 3.194, Val loss 4.112\n",
            "Ep 5 (Step 011915): Train loss 3.185, Val loss 4.101\n",
            "Ep 5 (Step 011920): Train loss 3.672, Val loss 4.115\n",
            "Ep 5 (Step 011925): Train loss 3.514, Val loss 4.127\n",
            "Ep 5 (Step 011930): Train loss 3.452, Val loss 4.137\n",
            "Ep 5 (Step 011935): Train loss 3.438, Val loss 4.136\n",
            "Ep 5 (Step 011940): Train loss 3.519, Val loss 4.136\n",
            "Ep 5 (Step 011945): Train loss 3.750, Val loss 4.149\n",
            "Ep 5 (Step 011950): Train loss 3.475, Val loss 4.144\n",
            "Ep 5 (Step 011955): Train loss 3.669, Val loss 4.141\n",
            "Ep 5 (Step 011960): Train loss 3.275, Val loss 4.135\n",
            "Ep 5 (Step 011965): Train loss 3.366, Val loss 4.135\n",
            "Ep 5 (Step 011970): Train loss 3.406, Val loss 4.129\n",
            "Ep 5 (Step 011975): Train loss 3.564, Val loss 4.143\n",
            "Ep 5 (Step 011980): Train loss 3.296, Val loss 4.130\n",
            "Ep 5 (Step 011985): Train loss 3.519, Val loss 4.132\n",
            "Ep 5 (Step 011990): Train loss 3.266, Val loss 4.152\n",
            "Ep 5 (Step 011995): Train loss 3.499, Val loss 4.152\n",
            "Ep 5 (Step 012000): Train loss 3.295, Val loss 4.155\n",
            "Ep 5 (Step 012005): Train loss 3.919, Val loss 4.158\n",
            "Ep 5 (Step 012010): Train loss 3.468, Val loss 4.157\n",
            "Ep 5 (Step 012015): Train loss 3.446, Val loss 4.166\n",
            "Ep 5 (Step 012020): Train loss 3.463, Val loss 4.159\n",
            "Ep 5 (Step 012025): Train loss 3.444, Val loss 4.173\n",
            "Ep 5 (Step 012030): Train loss 3.273, Val loss 4.182\n",
            "Ep 5 (Step 012035): Train loss 3.522, Val loss 4.173\n",
            "Ep 5 (Step 012040): Train loss 3.236, Val loss 4.171\n",
            "Ep 5 (Step 012045): Train loss 3.716, Val loss 4.178\n",
            "Ep 5 (Step 012050): Train loss 3.626, Val loss 4.179\n",
            "Ep 5 (Step 012055): Train loss 3.380, Val loss 4.162\n",
            "Ep 5 (Step 012060): Train loss 3.520, Val loss 4.148\n",
            "Ep 5 (Step 012065): Train loss 3.415, Val loss 4.158\n",
            "Ep 5 (Step 012070): Train loss 3.335, Val loss 4.167\n",
            "Ep 5 (Step 012075): Train loss 3.495, Val loss 4.171\n",
            "Ep 5 (Step 012080): Train loss 3.649, Val loss 4.160\n",
            "Ep 5 (Step 012085): Train loss 3.204, Val loss 4.161\n",
            "Ep 5 (Step 012090): Train loss 3.646, Val loss 4.161\n",
            "Ep 5 (Step 012095): Train loss 3.735, Val loss 4.153\n",
            "Ep 5 (Step 012100): Train loss 3.214, Val loss 4.155\n",
            "Ep 5 (Step 012105): Train loss 3.425, Val loss 4.169\n",
            "Ep 5 (Step 012110): Train loss 3.309, Val loss 4.179\n",
            "Ep 5 (Step 012115): Train loss 3.624, Val loss 4.170\n",
            "Ep 5 (Step 012120): Train loss 3.519, Val loss 4.160\n",
            "Ep 5 (Step 012125): Train loss 3.566, Val loss 4.165\n",
            "Ep 5 (Step 012130): Train loss 3.346, Val loss 4.169\n",
            "Ep 5 (Step 012135): Train loss 3.767, Val loss 4.168\n",
            "Ep 5 (Step 012140): Train loss 3.443, Val loss 4.160\n",
            "Ep 5 (Step 012145): Train loss 3.353, Val loss 4.164\n",
            "Ep 5 (Step 012150): Train loss 3.614, Val loss 4.168\n",
            "Ep 5 (Step 012155): Train loss 3.581, Val loss 4.169\n",
            "Ep 5 (Step 012160): Train loss 3.425, Val loss 4.168\n",
            "Ep 5 (Step 012165): Train loss 3.423, Val loss 4.174\n",
            "Ep 5 (Step 012170): Train loss 3.295, Val loss 4.168\n",
            "Ep 5 (Step 012175): Train loss 3.419, Val loss 4.165\n",
            "Ep 5 (Step 012180): Train loss 3.512, Val loss 4.170\n",
            "Ep 5 (Step 012185): Train loss 3.553, Val loss 4.177\n",
            "Ep 5 (Step 012190): Train loss 3.287, Val loss 4.163\n",
            "Ep 5 (Step 012195): Train loss 3.643, Val loss 4.152\n",
            "Ep 5 (Step 012200): Train loss 3.171, Val loss 4.159\n",
            "Ep 5 (Step 012205): Train loss 3.344, Val loss 4.161\n",
            "Ep 5 (Step 012210): Train loss 3.598, Val loss 4.160\n",
            "Ep 5 (Step 012215): Train loss 3.331, Val loss 4.166\n",
            "Ep 5 (Step 012220): Train loss 3.697, Val loss 4.190\n",
            "Ep 5 (Step 012225): Train loss 3.316, Val loss 4.203\n",
            "Ep 5 (Step 012230): Train loss 3.640, Val loss 4.200\n",
            "Ep 5 (Step 012235): Train loss 3.533, Val loss 4.195\n",
            "Ep 5 (Step 012240): Train loss 3.430, Val loss 4.190\n",
            "Ep 5 (Step 012245): Train loss 3.575, Val loss 4.193\n",
            "Ep 5 (Step 012250): Train loss 3.317, Val loss 4.197\n",
            "Ep 5 (Step 012255): Train loss 3.522, Val loss 4.198\n",
            "Ep 5 (Step 012260): Train loss 3.423, Val loss 4.204\n",
            "Ep 5 (Step 012265): Train loss 3.357, Val loss 4.203\n",
            "Ep 5 (Step 012270): Train loss 3.693, Val loss 4.194\n",
            "Ep 5 (Step 012275): Train loss 3.546, Val loss 4.200\n",
            "Ep 5 (Step 012280): Train loss 3.210, Val loss 4.201\n",
            "Ep 5 (Step 012285): Train loss 3.514, Val loss 4.203\n",
            "Ep 5 (Step 012290): Train loss 3.254, Val loss 4.216\n",
            "Ep 5 (Step 012295): Train loss 3.378, Val loss 4.217\n",
            "Ep 5 (Step 012300): Train loss 3.649, Val loss 4.215\n",
            "Ep 5 (Step 012305): Train loss 3.345, Val loss 4.207\n",
            "Ep 5 (Step 012310): Train loss 3.311, Val loss 4.203\n",
            "Ep 5 (Step 012315): Train loss 3.475, Val loss 4.193\n",
            "Ep 5 (Step 012320): Train loss 3.418, Val loss 4.196\n",
            "Ep 5 (Step 012325): Train loss 3.638, Val loss 4.199\n",
            "Ep 5 (Step 012330): Train loss 3.229, Val loss 4.207\n",
            "Ep 5 (Step 012335): Train loss 3.230, Val loss 4.199\n",
            "Ep 5 (Step 012340): Train loss 3.425, Val loss 4.180\n",
            "Ep 5 (Step 012345): Train loss 3.479, Val loss 4.185\n",
            "Ep 5 (Step 012350): Train loss 3.346, Val loss 4.198\n",
            "Ep 5 (Step 012355): Train loss 3.502, Val loss 4.194\n",
            "Ep 5 (Step 012360): Train loss 3.413, Val loss 4.204\n",
            "Ep 5 (Step 012365): Train loss 3.144, Val loss 4.213\n",
            "Ep 5 (Step 012370): Train loss 3.515, Val loss 4.217\n",
            "Ep 5 (Step 012375): Train loss 3.508, Val loss 4.212\n",
            "Ep 5 (Step 012380): Train loss 3.436, Val loss 4.216\n",
            "Ep 5 (Step 012385): Train loss 3.098, Val loss 4.224\n",
            "Ep 5 (Step 012390): Train loss 3.264, Val loss 4.232\n",
            "Ep 5 (Step 012395): Train loss 3.496, Val loss 4.230\n",
            "Ep 5 (Step 012400): Train loss 3.136, Val loss 4.223\n",
            "Ep 5 (Step 012405): Train loss 3.478, Val loss 4.210\n",
            "Ep 5 (Step 012410): Train loss 3.520, Val loss 4.198\n",
            "Ep 5 (Step 012415): Train loss 3.622, Val loss 4.215\n",
            "Ep 5 (Step 012420): Train loss 3.525, Val loss 4.231\n",
            "Ep 5 (Step 012425): Train loss 3.400, Val loss 4.224\n",
            "Ep 5 (Step 012430): Train loss 3.261, Val loss 4.217\n",
            "Ep 5 (Step 012435): Train loss 3.653, Val loss 4.224\n",
            "Ep 5 (Step 012440): Train loss 3.413, Val loss 4.221\n",
            "Ep 5 (Step 012445): Train loss 3.610, Val loss 4.223\n",
            "Ep 5 (Step 012450): Train loss 3.703, Val loss 4.213\n",
            "Ep 5 (Step 012455): Train loss 3.586, Val loss 4.203\n",
            "Ep 5 (Step 012460): Train loss 3.426, Val loss 4.187\n",
            "Ep 5 (Step 012465): Train loss 3.436, Val loss 4.188\n",
            "Ep 5 (Step 012470): Train loss 3.510, Val loss 4.192\n",
            "Ep 5 (Step 012475): Train loss 3.467, Val loss 4.187\n",
            "Ep 5 (Step 012480): Train loss 3.364, Val loss 4.179\n",
            "Ep 5 (Step 012485): Train loss 3.374, Val loss 4.180\n",
            "Ep 5 (Step 012490): Train loss 3.284, Val loss 4.187\n",
            "Ep 5 (Step 012495): Train loss 3.572, Val loss 4.179\n",
            "Ep 5 (Step 012500): Train loss 3.641, Val loss 4.180\n",
            "Ep 5 (Step 012505): Train loss 3.704, Val loss 4.184\n",
            "Ep 5 (Step 012510): Train loss 3.302, Val loss 4.187\n",
            "Ep 5 (Step 012515): Train loss 3.869, Val loss 4.215\n",
            "Ep 5 (Step 012520): Train loss 3.365, Val loss 4.207\n",
            "Ep 5 (Step 012525): Train loss 3.409, Val loss 4.200\n",
            "Ep 5 (Step 012530): Train loss 3.130, Val loss 4.211\n",
            "Ep 5 (Step 012535): Train loss 3.350, Val loss 4.200\n",
            "Ep 5 (Step 012540): Train loss 3.456, Val loss 4.197\n",
            "Ep 5 (Step 012545): Train loss 3.470, Val loss 4.198\n",
            "Ep 5 (Step 012550): Train loss 3.531, Val loss 4.185\n",
            "Ep 5 (Step 012555): Train loss 3.464, Val loss 4.178\n",
            "Ep 5 (Step 012560): Train loss 3.590, Val loss 4.193\n",
            "Ep 5 (Step 012565): Train loss 3.449, Val loss 4.200\n",
            "Ep 5 (Step 012570): Train loss 3.682, Val loss 4.206\n",
            "Ep 5 (Step 012575): Train loss 3.687, Val loss 4.194\n",
            "Ep 5 (Step 012580): Train loss 3.538, Val loss 4.192\n",
            "Ep 5 (Step 012585): Train loss 3.301, Val loss 4.177\n",
            "Ep 5 (Step 012590): Train loss 3.497, Val loss 4.180\n",
            "Ep 5 (Step 012595): Train loss 3.424, Val loss 4.178\n",
            "Ep 5 (Step 012600): Train loss 3.365, Val loss 4.181\n",
            "Ep 5 (Step 012605): Train loss 3.505, Val loss 4.196\n",
            "Ep 5 (Step 012610): Train loss 3.465, Val loss 4.191\n",
            "Ep 5 (Step 012615): Train loss 3.351, Val loss 4.193\n",
            "Ep 5 (Step 012620): Train loss 3.643, Val loss 4.202\n",
            "Ep 5 (Step 012625): Train loss 3.766, Val loss 4.205\n",
            "Ep 5 (Step 012630): Train loss 3.242, Val loss 4.216\n",
            "Ep 5 (Step 012635): Train loss 3.674, Val loss 4.206\n",
            "Ep 5 (Step 012640): Train loss 3.566, Val loss 4.196\n",
            "Ep 5 (Step 012645): Train loss 3.399, Val loss 4.187\n",
            "Ep 5 (Step 012650): Train loss 3.404, Val loss 4.189\n",
            "Ep 5 (Step 012655): Train loss 3.293, Val loss 4.194\n",
            "Ep 5 (Step 012660): Train loss 3.427, Val loss 4.199\n",
            "Ep 5 (Step 012665): Train loss 3.308, Val loss 4.198\n",
            "Ep 5 (Step 012670): Train loss 3.595, Val loss 4.182\n",
            "Ep 5 (Step 012675): Train loss 3.296, Val loss 4.180\n",
            "Ep 5 (Step 012680): Train loss 3.208, Val loss 4.167\n",
            "Ep 5 (Step 012685): Train loss 3.497, Val loss 4.169\n",
            "Ep 5 (Step 012690): Train loss 3.451, Val loss 4.186\n",
            "Ep 5 (Step 012695): Train loss 3.137, Val loss 4.193\n",
            "Ep 5 (Step 012700): Train loss 3.369, Val loss 4.182\n",
            "Ep 5 (Step 012705): Train loss 3.138, Val loss 4.186\n",
            "Ep 5 (Step 012710): Train loss 3.416, Val loss 4.201\n",
            "Ep 5 (Step 012715): Train loss 3.423, Val loss 4.186\n",
            "Ep 5 (Step 012720): Train loss 3.635, Val loss 4.158\n",
            "Ep 5 (Step 012725): Train loss 3.569, Val loss 4.149\n",
            "Ep 5 (Step 012730): Train loss 3.407, Val loss 4.148\n",
            "Ep 5 (Step 012735): Train loss 3.204, Val loss 4.164\n",
            "Ep 5 (Step 012740): Train loss 3.361, Val loss 4.168\n",
            "Ep 5 (Step 012745): Train loss 3.401, Val loss 4.158\n",
            "Ep 5 (Step 012750): Train loss 3.015, Val loss 4.154\n",
            "Ep 5 (Step 012755): Train loss 3.339, Val loss 4.163\n",
            "Ep 5 (Step 012760): Train loss 3.381, Val loss 4.169\n",
            "Ep 5 (Step 012765): Train loss 3.453, Val loss 4.166\n",
            "Ep 5 (Step 012770): Train loss 3.480, Val loss 4.169\n",
            "Ep 5 (Step 012775): Train loss 3.429, Val loss 4.180\n",
            "Ep 5 (Step 012780): Train loss 3.821, Val loss 4.181\n",
            "Ep 5 (Step 012785): Train loss 3.277, Val loss 4.185\n",
            "Ep 5 (Step 012790): Train loss 3.217, Val loss 4.174\n",
            "Ep 5 (Step 012795): Train loss 3.332, Val loss 4.169\n",
            "Ep 5 (Step 012800): Train loss 3.267, Val loss 4.184\n",
            "Ep 5 (Step 012805): Train loss 3.384, Val loss 4.184\n",
            "Ep 5 (Step 012810): Train loss 3.323, Val loss 4.174\n",
            "Ep 5 (Step 012815): Train loss 3.124, Val loss 4.172\n",
            "Ep 5 (Step 012820): Train loss 3.274, Val loss 4.172\n",
            "Ep 5 (Step 012825): Train loss 3.539, Val loss 4.173\n",
            "Ep 5 (Step 012830): Train loss 3.450, Val loss 4.177\n",
            "Ep 5 (Step 012835): Train loss 3.311, Val loss 4.190\n",
            "Ep 5 (Step 012840): Train loss 3.292, Val loss 4.190\n",
            "Ep 5 (Step 012845): Train loss 3.512, Val loss 4.188\n",
            "Ep 5 (Step 012850): Train loss 3.534, Val loss 4.181\n",
            "Ep 5 (Step 012855): Train loss 3.388, Val loss 4.170\n",
            "Ep 5 (Step 012860): Train loss 3.175, Val loss 4.158\n",
            "Ep 5 (Step 012865): Train loss 3.244, Val loss 4.167\n",
            "Ep 5 (Step 012870): Train loss 3.588, Val loss 4.161\n",
            "Ep 5 (Step 012875): Train loss 3.285, Val loss 4.164\n",
            "Ep 5 (Step 012880): Train loss 3.264, Val loss 4.165\n",
            "Ep 5 (Step 012885): Train loss 3.543, Val loss 4.161\n",
            "Ep 5 (Step 012890): Train loss 3.462, Val loss 4.175\n",
            "Ep 5 (Step 012895): Train loss 3.597, Val loss 4.171\n",
            "Ep 5 (Step 012900): Train loss 3.271, Val loss 4.161\n",
            "Ep 5 (Step 012905): Train loss 3.401, Val loss 4.155\n",
            "Ep 5 (Step 012910): Train loss 3.314, Val loss 4.147\n",
            "Ep 5 (Step 012915): Train loss 3.313, Val loss 4.149\n",
            "Ep 5 (Step 012920): Train loss 3.156, Val loss 4.164\n",
            "Ep 5 (Step 012925): Train loss 3.437, Val loss 4.181\n",
            "Ep 5 (Step 012930): Train loss 3.361, Val loss 4.169\n",
            "Ep 5 (Step 012935): Train loss 3.447, Val loss 4.158\n",
            "Ep 5 (Step 012940): Train loss 3.458, Val loss 4.153\n",
            "Ep 5 (Step 012945): Train loss 3.288, Val loss 4.149\n",
            "Ep 5 (Step 012950): Train loss 3.472, Val loss 4.138\n",
            "Ep 5 (Step 012955): Train loss 3.275, Val loss 4.127\n",
            "Ep 5 (Step 012960): Train loss 3.601, Val loss 4.121\n",
            "Ep 5 (Step 012965): Train loss 3.127, Val loss 4.118\n",
            "Ep 5 (Step 012970): Train loss 3.303, Val loss 4.129\n",
            "Ep 5 (Step 012975): Train loss 3.369, Val loss 4.134\n",
            "Ep 5 (Step 012980): Train loss 3.276, Val loss 4.139\n",
            "Ep 5 (Step 012985): Train loss 3.346, Val loss 4.114\n",
            "Ep 5 (Step 012990): Train loss 3.444, Val loss 4.122\n",
            "Ep 5 (Step 012995): Train loss 3.646, Val loss 4.127\n",
            "Ep 5 (Step 013000): Train loss 3.577, Val loss 4.135\n",
            "Ep 5 (Step 013005): Train loss 3.269, Val loss 4.147\n",
            "Ep 5 (Step 013010): Train loss 3.217, Val loss 4.138\n",
            "Ep 5 (Step 013015): Train loss 3.099, Val loss 4.132\n",
            "Ep 5 (Step 013020): Train loss 3.597, Val loss 4.126\n",
            "Ep 5 (Step 013025): Train loss 3.619, Val loss 4.124\n",
            "Ep 5 (Step 013030): Train loss 3.039, Val loss 4.129\n",
            "Ep 5 (Step 013035): Train loss 3.204, Val loss 4.126\n",
            "Ep 5 (Step 013040): Train loss 3.180, Val loss 4.124\n",
            "Ep 5 (Step 013045): Train loss 3.429, Val loss 4.123\n",
            "Ep 5 (Step 013050): Train loss 3.308, Val loss 4.128\n",
            "Ep 5 (Step 013055): Train loss 2.964, Val loss 4.136\n",
            "Ep 5 (Step 013060): Train loss 3.342, Val loss 4.144\n",
            "Ep 5 (Step 013065): Train loss 3.481, Val loss 4.153\n",
            "Ep 5 (Step 013070): Train loss 3.115, Val loss 4.144\n",
            "Ep 5 (Step 013075): Train loss 3.027, Val loss 4.136\n",
            "Ep 5 (Step 013080): Train loss 3.297, Val loss 4.142\n",
            "Ep 5 (Step 013085): Train loss 3.457, Val loss 4.142\n",
            "Ep 5 (Step 013090): Train loss 3.221, Val loss 4.143\n",
            "Ep 5 (Step 013095): Train loss 3.339, Val loss 4.148\n",
            "Ep 5 (Step 013100): Train loss 3.507, Val loss 4.152\n",
            "Ep 5 (Step 013105): Train loss 3.154, Val loss 4.144\n",
            "Ep 5 (Step 013110): Train loss 2.952, Val loss 4.130\n",
            "Ep 5 (Step 013115): Train loss 3.481, Val loss 4.131\n",
            "Ep 5 (Step 013120): Train loss 3.167, Val loss 4.136\n",
            "Ep 5 (Step 013125): Train loss 3.235, Val loss 4.142\n",
            "Ep 5 (Step 013130): Train loss 3.256, Val loss 4.149\n",
            "Ep 5 (Step 013135): Train loss 3.152, Val loss 4.147\n",
            "Ep 5 (Step 013140): Train loss 3.235, Val loss 4.150\n",
            "Ep 5 (Step 013145): Train loss 3.394, Val loss 4.153\n",
            "Ep 5 (Step 013150): Train loss 3.684, Val loss 4.157\n",
            "Ep 5 (Step 013155): Train loss 3.118, Val loss 4.157\n",
            "Ep 5 (Step 013160): Train loss 3.696, Val loss 4.160\n",
            "Ep 5 (Step 013165): Train loss 3.257, Val loss 4.154\n",
            "Ep 5 (Step 013170): Train loss 3.587, Val loss 4.151\n",
            "Ep 5 (Step 013175): Train loss 3.350, Val loss 4.136\n",
            "Ep 5 (Step 013180): Train loss 3.441, Val loss 4.134\n",
            "Ep 5 (Step 013185): Train loss 3.340, Val loss 4.132\n",
            "Ep 5 (Step 013190): Train loss 3.570, Val loss 4.132\n",
            "Ep 5 (Step 013195): Train loss 3.522, Val loss 4.118\n",
            "Ep 5 (Step 013200): Train loss 3.494, Val loss 4.123\n",
            "Ep 5 (Step 013205): Train loss 3.236, Val loss 4.130\n",
            "Ep 5 (Step 013210): Train loss 3.189, Val loss 4.144\n",
            "Ep 5 (Step 013215): Train loss 3.481, Val loss 4.149\n",
            "Ep 5 (Step 013220): Train loss 3.439, Val loss 4.140\n",
            "Ep 5 (Step 013225): Train loss 3.606, Val loss 4.141\n",
            "Ep 5 (Step 013230): Train loss 3.202, Val loss 4.140\n",
            "Ep 5 (Step 013235): Train loss 3.409, Val loss 4.139\n",
            "Ep 5 (Step 013240): Train loss 3.677, Val loss 4.134\n",
            "Ep 5 (Step 013245): Train loss 3.107, Val loss 4.139\n",
            "Ep 5 (Step 013250): Train loss 3.760, Val loss 4.148\n",
            "Ep 5 (Step 013255): Train loss 3.435, Val loss 4.133\n",
            "Ep 5 (Step 013260): Train loss 3.308, Val loss 4.117\n",
            "Ep 5 (Step 013265): Train loss 3.363, Val loss 4.110\n",
            "Ep 5 (Step 013270): Train loss 3.426, Val loss 4.121\n",
            "Ep 5 (Step 013275): Train loss 3.345, Val loss 4.139\n",
            "Ep 5 (Step 013280): Train loss 3.275, Val loss 4.146\n",
            "Ep 5 (Step 013285): Train loss 3.307, Val loss 4.150\n",
            "Ep 5 (Step 013290): Train loss 3.485, Val loss 4.153\n",
            "Ep 5 (Step 013295): Train loss 3.117, Val loss 4.155\n",
            "Ep 5 (Step 013300): Train loss 3.453, Val loss 4.153\n",
            "Ep 5 (Step 013305): Train loss 3.385, Val loss 4.157\n",
            "Ep 5 (Step 013310): Train loss 3.197, Val loss 4.145\n",
            "Ep 5 (Step 013315): Train loss 3.503, Val loss 4.137\n",
            "Ep 5 (Step 013320): Train loss 3.324, Val loss 4.144\n",
            "Ep 5 (Step 013325): Train loss 3.471, Val loss 4.147\n",
            "Ep 5 (Step 013330): Train loss 3.377, Val loss 4.138\n",
            "Ep 5 (Step 013335): Train loss 3.485, Val loss 4.135\n",
            "Ep 5 (Step 013340): Train loss 3.389, Val loss 4.133\n",
            "Ep 5 (Step 013345): Train loss 3.204, Val loss 4.138\n",
            "Ep 5 (Step 013350): Train loss 3.373, Val loss 4.145\n",
            "Ep 5 (Step 013355): Train loss 3.437, Val loss 4.132\n",
            "Ep 5 (Step 013360): Train loss 3.543, Val loss 4.125\n",
            "Ep 5 (Step 013365): Train loss 3.336, Val loss 4.126\n",
            "Ep 5 (Step 013370): Train loss 3.646, Val loss 4.130\n",
            "Ep 5 (Step 013375): Train loss 3.057, Val loss 4.129\n",
            "Ep 5 (Step 013380): Train loss 3.415, Val loss 4.135\n",
            "Ep 5 (Step 013385): Train loss 3.263, Val loss 4.115\n",
            "Ep 5 (Step 013390): Train loss 3.173, Val loss 4.120\n",
            "Ep 5 (Step 013395): Train loss 3.421, Val loss 4.141\n",
            "Ep 5 (Step 013400): Train loss 3.080, Val loss 4.143\n",
            "Ep 5 (Step 013405): Train loss 3.130, Val loss 4.134\n",
            "Ep 5 (Step 013410): Train loss 3.534, Val loss 4.141\n",
            "Ep 5 (Step 013415): Train loss 3.675, Val loss 4.136\n",
            "Ep 5 (Step 013420): Train loss 3.463, Val loss 4.136\n",
            "Ep 5 (Step 013425): Train loss 3.255, Val loss 4.140\n",
            "Ep 5 (Step 013430): Train loss 3.097, Val loss 4.143\n",
            "Ep 5 (Step 013435): Train loss 3.188, Val loss 4.144\n",
            "Ep 5 (Step 013440): Train loss 3.136, Val loss 4.140\n",
            "Ep 5 (Step 013445): Train loss 3.477, Val loss 4.137\n",
            "Ep 5 (Step 013450): Train loss 3.536, Val loss 4.126\n",
            "Ep 5 (Step 013455): Train loss 3.572, Val loss 4.124\n",
            "Ep 5 (Step 013460): Train loss 3.278, Val loss 4.129\n",
            "Ep 5 (Step 013465): Train loss 3.313, Val loss 4.149\n",
            "Ep 5 (Step 013470): Train loss 3.340, Val loss 4.155\n",
            "Ep 5 (Step 013475): Train loss 3.069, Val loss 4.133\n",
            "Ep 5 (Step 013480): Train loss 3.250, Val loss 4.132\n",
            "Ep 5 (Step 013485): Train loss 3.218, Val loss 4.133\n",
            "Ep 5 (Step 013490): Train loss 3.536, Val loss 4.125\n",
            "Ep 5 (Step 013495): Train loss 3.394, Val loss 4.121\n",
            "Ep 5 (Step 013500): Train loss 3.631, Val loss 4.123\n",
            "Ep 5 (Step 013505): Train loss 3.448, Val loss 4.111\n",
            "Ep 5 (Step 013510): Train loss 3.345, Val loss 4.102\n",
            "Ep 5 (Step 013515): Train loss 3.689, Val loss 4.101\n",
            "Ep 5 (Step 013520): Train loss 3.383, Val loss 4.104\n",
            "Ep 5 (Step 013525): Train loss 3.398, Val loss 4.114\n",
            "Ep 5 (Step 013530): Train loss 3.309, Val loss 4.111\n",
            "Ep 5 (Step 013535): Train loss 3.503, Val loss 4.102\n",
            "Ep 5 (Step 013540): Train loss 3.582, Val loss 4.100\n",
            "Ep 5 (Step 013545): Train loss 3.238, Val loss 4.110\n",
            "Ep 5 (Step 013550): Train loss 3.455, Val loss 4.106\n",
            "Ep 5 (Step 013555): Train loss 3.604, Val loss 4.094\n",
            "Ep 5 (Step 013560): Train loss 3.591, Val loss 4.087\n",
            "Ep 5 (Step 013565): Train loss 3.571, Val loss 4.092\n",
            "Ep 5 (Step 013570): Train loss 3.094, Val loss 4.094\n",
            "Ep 5 (Step 013575): Train loss 3.409, Val loss 4.092\n",
            "Ep 5 (Step 013580): Train loss 3.408, Val loss 4.091\n",
            "Ep 5 (Step 013585): Train loss 3.655, Val loss 4.100\n",
            "Ep 5 (Step 013590): Train loss 3.477, Val loss 4.109\n",
            "Ep 5 (Step 013595): Train loss 3.424, Val loss 4.107\n",
            "Ep 5 (Step 013600): Train loss 3.292, Val loss 4.095\n",
            "Ep 5 (Step 013605): Train loss 3.599, Val loss 4.095\n",
            "Ep 5 (Step 013610): Train loss 3.434, Val loss 4.088\n",
            "Ep 5 (Step 013615): Train loss 3.102, Val loss 4.106\n",
            "Ep 5 (Step 013620): Train loss 3.391, Val loss 4.118\n",
            "Ep 5 (Step 013625): Train loss 3.289, Val loss 4.101\n",
            "Ep 5 (Step 013630): Train loss 3.171, Val loss 4.099\n",
            "Ep 5 (Step 013635): Train loss 3.568, Val loss 4.101\n",
            "Ep 5 (Step 013640): Train loss 3.606, Val loss 4.106\n",
            "Ep 5 (Step 013645): Train loss 3.220, Val loss 4.092\n",
            "Ep 5 (Step 013650): Train loss 3.392, Val loss 4.094\n",
            "Ep 5 (Step 013655): Train loss 3.240, Val loss 4.106\n",
            "Ep 5 (Step 013660): Train loss 3.459, Val loss 4.099\n",
            "Ep 5 (Step 013665): Train loss 3.369, Val loss 4.098\n",
            "Ep 5 (Step 013670): Train loss 3.325, Val loss 4.105\n",
            "Ep 5 (Step 013675): Train loss 3.073, Val loss 4.100\n",
            "Ep 5 (Step 013680): Train loss 3.565, Val loss 4.097\n",
            "Ep 5 (Step 013685): Train loss 3.164, Val loss 4.099\n",
            "Ep 5 (Step 013690): Train loss 3.410, Val loss 4.103\n",
            "Ep 5 (Step 013695): Train loss 3.283, Val loss 4.113\n",
            "Ep 5 (Step 013700): Train loss 3.297, Val loss 4.107\n",
            "Ep 5 (Step 013705): Train loss 3.226, Val loss 4.102\n",
            "Ep 5 (Step 013710): Train loss 3.278, Val loss 4.116\n",
            "Ep 5 (Step 013715): Train loss 3.264, Val loss 4.123\n",
            "Ep 5 (Step 013720): Train loss 3.500, Val loss 4.128\n",
            "Ep 5 (Step 013725): Train loss 3.558, Val loss 4.119\n",
            "Ep 5 (Step 013730): Train loss 3.215, Val loss 4.110\n",
            "Ep 5 (Step 013735): Train loss 3.380, Val loss 4.108\n",
            "Ep 5 (Step 013740): Train loss 3.649, Val loss 4.106\n",
            "Ep 5 (Step 013745): Train loss 3.266, Val loss 4.103\n",
            "Ep 5 (Step 013750): Train loss 3.290, Val loss 4.127\n",
            "Ep 5 (Step 013755): Train loss 3.346, Val loss 4.135\n",
            "Ep 5 (Step 013760): Train loss 3.409, Val loss 4.117\n",
            "Ep 5 (Step 013765): Train loss 3.124, Val loss 4.095\n",
            "Ep 5 (Step 013770): Train loss 3.447, Val loss 4.099\n",
            "Ep 5 (Step 013775): Train loss 3.153, Val loss 4.126\n",
            "Ep 5 (Step 013780): Train loss 2.947, Val loss 4.146\n",
            "Ep 5 (Step 013785): Train loss 3.199, Val loss 4.135\n",
            "Ep 5 (Step 013790): Train loss 3.531, Val loss 4.132\n",
            "Ep 5 (Step 013795): Train loss 3.055, Val loss 4.129\n",
            "Ep 5 (Step 013800): Train loss 3.243, Val loss 4.134\n",
            "Ep 5 (Step 013805): Train loss 3.040, Val loss 4.129\n",
            "Ep 5 (Step 013810): Train loss 3.444, Val loss 4.121\n",
            "Ep 5 (Step 013815): Train loss 3.261, Val loss 4.114\n",
            "Ep 5 (Step 013820): Train loss 3.051, Val loss 4.105\n",
            "Ep 5 (Step 013825): Train loss 3.566, Val loss 4.107\n",
            "Ep 5 (Step 013830): Train loss 3.204, Val loss 4.111\n",
            "Ep 5 (Step 013835): Train loss 3.216, Val loss 4.110\n",
            "Ep 5 (Step 013840): Train loss 3.054, Val loss 4.119\n",
            "Ep 5 (Step 013845): Train loss 3.533, Val loss 4.129\n",
            "Ep 5 (Step 013850): Train loss 3.637, Val loss 4.135\n",
            "Ep 5 (Step 013855): Train loss 3.501, Val loss 4.133\n",
            "Ep 5 (Step 013860): Train loss 3.538, Val loss 4.122\n",
            "Ep 5 (Step 013865): Train loss 3.180, Val loss 4.124\n",
            "Ep 5 (Step 013870): Train loss 3.405, Val loss 4.137\n",
            "Ep 5 (Step 013875): Train loss 3.119, Val loss 4.124\n",
            "Ep 5 (Step 013880): Train loss 3.303, Val loss 4.124\n",
            "Ep 5 (Step 013885): Train loss 3.043, Val loss 4.124\n",
            "Ep 5 (Step 013890): Train loss 3.341, Val loss 4.123\n",
            "Ep 5 (Step 013895): Train loss 3.422, Val loss 4.123\n",
            "Ep 5 (Step 013900): Train loss 3.378, Val loss 4.139\n",
            "Ep 5 (Step 013905): Train loss 3.272, Val loss 4.132\n",
            "Ep 5 (Step 013910): Train loss 3.368, Val loss 4.124\n",
            "Ep 5 (Step 013915): Train loss 3.398, Val loss 4.123\n",
            "Ep 5 (Step 013920): Train loss 3.525, Val loss 4.119\n",
            "Ep 5 (Step 013925): Train loss 3.073, Val loss 4.101\n",
            "Ep 5 (Step 013930): Train loss 3.332, Val loss 4.108\n",
            "Ep 5 (Step 013935): Train loss 3.131, Val loss 4.113\n",
            "Ep 5 (Step 013940): Train loss 3.270, Val loss 4.134\n",
            "Ep 5 (Step 013945): Train loss 3.463, Val loss 4.118\n",
            "Ep 5 (Step 013950): Train loss 3.467, Val loss 4.094\n",
            "Ep 5 (Step 013955): Train loss 3.100, Val loss 4.101\n",
            "Ep 5 (Step 013960): Train loss 3.133, Val loss 4.120\n",
            "Ep 5 (Step 013965): Train loss 3.301, Val loss 4.112\n",
            "Ep 5 (Step 013970): Train loss 3.533, Val loss 4.113\n",
            "Ep 5 (Step 013975): Train loss 3.183, Val loss 4.127\n",
            "Ep 5 (Step 013980): Train loss 3.535, Val loss 4.159\n",
            "Ep 5 (Step 013985): Train loss 3.241, Val loss 4.143\n",
            "Ep 5 (Step 013990): Train loss 3.561, Val loss 4.121\n",
            "Ep 5 (Step 013995): Train loss 3.077, Val loss 4.121\n",
            "Ep 5 (Step 014000): Train loss 3.309, Val loss 4.120\n",
            "Ep 5 (Step 014005): Train loss 3.358, Val loss 4.104\n",
            "Ep 5 (Step 014010): Train loss 3.471, Val loss 4.106\n",
            "Ep 5 (Step 014015): Train loss 3.158, Val loss 4.100\n",
            "Ep 5 (Step 014020): Train loss 3.571, Val loss 4.099\n",
            "Ep 5 (Step 014025): Train loss 3.386, Val loss 4.107\n",
            "Ep 5 (Step 014030): Train loss 3.210, Val loss 4.123\n",
            "Ep 5 (Step 014035): Train loss 3.365, Val loss 4.115\n",
            "Ep 5 (Step 014040): Train loss 3.487, Val loss 4.110\n",
            "Ep 5 (Step 014045): Train loss 3.007, Val loss 4.122\n",
            "Ep 5 (Step 014050): Train loss 3.645, Val loss 4.135\n",
            "Ep 5 (Step 014055): Train loss 3.204, Val loss 4.136\n",
            "Ep 5 (Step 014060): Train loss 3.230, Val loss 4.111\n",
            "Ep 5 (Step 014065): Train loss 3.307, Val loss 4.113\n",
            "Ep 5 (Step 014070): Train loss 3.491, Val loss 4.116\n",
            "Ep 5 (Step 014075): Train loss 3.522, Val loss 4.130\n",
            "Ep 5 (Step 014080): Train loss 3.358, Val loss 4.132\n",
            "Ep 5 (Step 014085): Train loss 3.268, Val loss 4.123\n",
            "Ep 5 (Step 014090): Train loss 3.112, Val loss 4.127\n",
            "Ep 5 (Step 014095): Train loss 3.141, Val loss 4.125\n",
            "Ep 5 (Step 014100): Train loss 3.421, Val loss 4.127\n",
            "Ep 5 (Step 014105): Train loss 3.314, Val loss 4.125\n",
            "Ep 5 (Step 014110): Train loss 3.045, Val loss 4.127\n",
            "Ep 5 (Step 014115): Train loss 3.430, Val loss 4.129\n",
            "Ep 5 (Step 014120): Train loss 3.480, Val loss 4.135\n",
            "Ep 5 (Step 014125): Train loss 3.586, Val loss 4.116\n",
            "Ep 5 (Step 014130): Train loss 3.355, Val loss 4.104\n",
            "Ep 5 (Step 014135): Train loss 3.425, Val loss 4.120\n",
            "Ep 5 (Step 014140): Train loss 3.548, Val loss 4.134\n",
            "Ep 5 (Step 014145): Train loss 3.406, Val loss 4.125\n",
            "Ep 5 (Step 014150): Train loss 3.020, Val loss 4.114\n",
            "Ep 5 (Step 014155): Train loss 3.192, Val loss 4.115\n",
            "Ep 5 (Step 014160): Train loss 3.401, Val loss 4.126\n",
            "Ep 5 (Step 014165): Train loss 3.444, Val loss 4.126\n",
            "Ep 5 (Step 014170): Train loss 3.157, Val loss 4.122\n",
            "Ep 5 (Step 014175): Train loss 3.472, Val loss 4.122\n",
            "Ep 5 (Step 014180): Train loss 3.381, Val loss 4.116\n",
            "Ep 5 (Step 014185): Train loss 3.081, Val loss 4.112\n",
            "Ep 5 (Step 014190): Train loss 3.250, Val loss 4.103\n",
            "Ep 5 (Step 014195): Train loss 3.390, Val loss 4.093\n",
            "Ep 5 (Step 014200): Train loss 3.423, Val loss 4.096\n",
            "Ep 5 (Step 014205): Train loss 3.478, Val loss 4.100\n",
            "Ep 5 (Step 014210): Train loss 3.248, Val loss 4.086\n",
            "Ep 5 (Step 014215): Train loss 3.227, Val loss 4.073\n",
            "Ep 5 (Step 014220): Train loss 3.384, Val loss 4.070\n",
            "Ep 5 (Step 014225): Train loss 3.389, Val loss 4.079\n",
            "Ep 5 (Step 014230): Train loss 3.036, Val loss 4.086\n",
            "Ep 5 (Step 014235): Train loss 3.044, Val loss 4.078\n",
            "Ep 5 (Step 014240): Train loss 3.389, Val loss 4.074\n",
            "Ep 5 (Step 014245): Train loss 3.267, Val loss 4.093\n",
            "Ep 5 (Step 014250): Train loss 3.266, Val loss 4.102\n",
            "Ep 5 (Step 014255): Train loss 3.724, Val loss 4.105\n",
            "Ep 5 (Step 014260): Train loss 3.363, Val loss 4.103\n",
            "Ep 5 (Step 014265): Train loss 3.487, Val loss 4.102\n",
            "Ep 5 (Step 014270): Train loss 3.329, Val loss 4.101\n",
            "Ep 5 (Step 014275): Train loss 3.249, Val loss 4.113\n",
            "Ep 5 (Step 014280): Train loss 3.479, Val loss 4.124\n",
            "Ep 5 (Step 014285): Train loss 3.279, Val loss 4.117\n",
            "Ep 5 (Step 014290): Train loss 3.495, Val loss 4.116\n",
            "Ep 5 (Step 014295): Train loss 3.500, Val loss 4.109\n",
            "Ep 5 (Step 014300): Train loss 3.577, Val loss 4.108\n",
            "Ep 5 (Step 014305): Train loss 3.262, Val loss 4.113\n",
            "Ep 5 (Step 014310): Train loss 3.227, Val loss 4.114\n",
            "Ep 5 (Step 014315): Train loss 3.498, Val loss 4.102\n",
            "Ep 5 (Step 014320): Train loss 3.173, Val loss 4.089\n",
            "Ep 5 (Step 014325): Train loss 3.271, Val loss 4.081\n",
            "Ep 5 (Step 014330): Train loss 3.301, Val loss 4.085\n",
            "Ep 5 (Step 014335): Train loss 3.116, Val loss 4.106\n",
            "Ep 5 (Step 014340): Train loss 3.241, Val loss 4.097\n",
            "Ep 5 (Step 014345): Train loss 3.198, Val loss 4.088\n",
            "Ep 5 (Step 014350): Train loss 3.520, Val loss 4.077\n",
            "Ep 5 (Step 014355): Train loss 3.361, Val loss 4.081\n",
            "Ep 5 (Step 014360): Train loss 3.406, Val loss 4.092\n",
            "Ep 5 (Step 014365): Train loss 3.085, Val loss 4.091\n",
            "Ep 5 (Step 014370): Train loss 3.029, Val loss 4.089\n",
            "Ep 5 (Step 014375): Train loss 2.949, Val loss 4.106\n",
            "Ep 5 (Step 014380): Train loss 3.149, Val loss 4.112\n",
            "Ep 5 (Step 014385): Train loss 3.173, Val loss 4.098\n",
            "Ep 5 (Step 014390): Train loss 3.495, Val loss 4.089\n",
            "Ep 5 (Step 014395): Train loss 3.421, Val loss 4.088\n",
            "Ep 5 (Step 014400): Train loss 3.507, Val loss 4.096\n",
            "Ep 5 (Step 014405): Train loss 3.258, Val loss 4.098\n",
            "Ep 5 (Step 014410): Train loss 3.491, Val loss 4.086\n",
            "Ep 5 (Step 014415): Train loss 3.374, Val loss 4.079\n",
            "Ep 5 (Step 014420): Train loss 3.085, Val loss 4.087\n",
            "Ep 5 (Step 014425): Train loss 3.303, Val loss 4.114\n",
            "Ep 5 (Step 014430): Train loss 3.159, Val loss 4.111\n",
            "Ep 5 (Step 014435): Train loss 3.297, Val loss 4.101\n",
            "Ep 5 (Step 014440): Train loss 3.522, Val loss 4.108\n",
            "Ep 5 (Step 014445): Train loss 2.960, Val loss 4.092\n",
            "Ep 5 (Step 014450): Train loss 3.243, Val loss 4.096\n",
            "Ep 5 (Step 014455): Train loss 3.081, Val loss 4.109\n",
            "Ep 5 (Step 014460): Train loss 2.983, Val loss 4.116\n",
            "Ep 5 (Step 014465): Train loss 3.147, Val loss 4.109\n",
            "Ep 5 (Step 014470): Train loss 3.256, Val loss 4.118\n",
            "Ep 5 (Step 014475): Train loss 3.337, Val loss 4.123\n",
            "Ep 5 (Step 014480): Train loss 3.036, Val loss 4.120\n",
            "Ep 5 (Step 014485): Train loss 3.275, Val loss 4.126\n",
            "Ep 5 (Step 014490): Train loss 3.367, Val loss 4.128\n",
            "Ep 5 (Step 014495): Train loss 2.921, Val loss 4.098\n",
            "Ep 5 (Step 014500): Train loss 3.194, Val loss 4.111\n",
            "Ep 5 (Step 014505): Train loss 3.231, Val loss 4.138\n",
            "Ep 5 (Step 014510): Train loss 3.385, Val loss 4.109\n",
            "Ep 5 (Step 014515): Train loss 3.128, Val loss 4.107\n",
            "Ep 5 (Step 014520): Train loss 3.157, Val loss 4.089\n",
            "Ep 5 (Step 014525): Train loss 3.293, Val loss 4.091\n",
            "Ep 5 (Step 014530): Train loss 3.110, Val loss 4.081\n",
            "Ep 5 (Step 014535): Train loss 3.278, Val loss 4.083\n",
            "Ep 5 (Step 014540): Train loss 3.138, Val loss 4.081\n",
            "Ep 5 (Step 014545): Train loss 3.028, Val loss 4.085\n",
            "Ep 5 (Step 014550): Train loss 3.527, Val loss 4.112\n",
            "Ep 5 (Step 014555): Train loss 3.182, Val loss 4.095\n",
            "Ep 5 (Step 014560): Train loss 3.145, Val loss 4.075\n",
            "Ep 5 (Step 014565): Train loss 3.361, Val loss 4.071\n",
            "Ep 5 (Step 014570): Train loss 3.238, Val loss 4.076\n",
            "Ep 5 (Step 014575): Train loss 3.499, Val loss 4.078\n",
            "Ep 5 (Step 014580): Train loss 2.600, Val loss 4.084\n",
            "Ep 5 (Step 014585): Train loss 3.182, Val loss 4.095\n",
            "Ep 5 (Step 014590): Train loss 3.400, Val loss 4.099\n",
            "Ep 5 (Step 014595): Train loss 3.132, Val loss 4.080\n",
            "Ep 5 (Step 014600): Train loss 3.361, Val loss 4.072\n",
            "Ep 5 (Step 014605): Train loss 3.337, Val loss 4.091\n",
            "Ep 5 (Step 014610): Train loss 3.462, Val loss 4.085\n",
            "Ep 5 (Step 014615): Train loss 2.995, Val loss 4.055\n",
            "Ep 5 (Step 014620): Train loss 3.385, Val loss 4.062\n",
            "Ep 5 (Step 014625): Train loss 3.172, Val loss 4.073\n",
            "Ep 5 (Step 014630): Train loss 3.517, Val loss 4.080\n",
            "Ep 5 (Step 014635): Train loss 3.527, Val loss 4.080\n",
            "Ep 5 (Step 014640): Train loss 3.405, Val loss 4.085\n",
            "Ep 5 (Step 014645): Train loss 3.102, Val loss 4.084\n",
            "Ep 5 (Step 014650): Train loss 3.201, Val loss 4.094\n",
            "Ep 5 (Step 014655): Train loss 2.856, Val loss 4.083\n",
            "Ep 5 (Step 014660): Train loss 3.138, Val loss 4.081\n",
            "Ep 5 (Step 014665): Train loss 3.509, Val loss 4.069\n",
            "Ep 5 (Step 014670): Train loss 3.284, Val loss 4.065\n",
            "Ep 5 (Step 014675): Train loss 3.086, Val loss 4.067\n",
            "Ep 5 (Step 014680): Train loss 3.349, Val loss 4.086\n",
            "Ep 5 (Step 014685): Train loss 3.637, Val loss 4.096\n",
            "Ep 5 (Step 014690): Train loss 3.325, Val loss 4.092\n",
            "Ep 5 (Step 014695): Train loss 3.206, Val loss 4.087\n",
            "Ep 5 (Step 014700): Train loss 3.081, Val loss 4.090\n",
            "Ep 5 (Step 014705): Train loss 2.991, Val loss 4.112\n",
            "Ep 5 (Step 014710): Train loss 3.055, Val loss 4.122\n",
            "Ep 5 (Step 014715): Train loss 3.424, Val loss 4.107\n",
            "Ep 5 (Step 014720): Train loss 3.265, Val loss 4.095\n",
            "Ep 5 (Step 014725): Train loss 3.114, Val loss 4.094\n",
            "Ep 5 (Step 014730): Train loss 3.458, Val loss 4.101\n",
            "Ep 5 (Step 014735): Train loss 3.349, Val loss 4.102\n",
            "Ep 5 (Step 014740): Train loss 2.996, Val loss 4.090\n",
            "Ep 5 (Step 014745): Train loss 3.331, Val loss 4.085\n",
            "Ep 5 (Step 014750): Train loss 3.566, Val loss 4.085\n",
            "Ep 5 (Step 014755): Train loss 3.428, Val loss 4.101\n",
            "Ep 5 (Step 014760): Train loss 3.284, Val loss 4.104\n",
            "Ep 5 (Step 014765): Train loss 3.097, Val loss 4.107\n",
            "Ep 5 (Step 014770): Train loss 3.262, Val loss 4.108\n",
            "Ep 5 (Step 014775): Train loss 3.360, Val loss 4.098\n",
            "Ep 5 (Step 014780): Train loss 3.152, Val loss 4.097\n",
            "Ep 5 (Step 014785): Train loss 3.257, Val loss 4.094\n",
            "Ep 5 (Step 014790): Train loss 2.963, Val loss 4.101\n",
            "Ep 5 (Step 014795): Train loss 3.059, Val loss 4.085\n",
            "Ep 5 (Step 014800): Train loss 2.993, Val loss 4.071\n",
            "Ep 5 (Step 014805): Train loss 3.060, Val loss 4.067\n",
            "Ep 5 (Step 014810): Train loss 3.055, Val loss 4.075\n",
            "Ep 5 (Step 014815): Train loss 3.364, Val loss 4.065\n",
            "Ep 5 (Step 014820): Train loss 3.096, Val loss 4.066\n",
            "Ep 5 (Step 014825): Train loss 3.230, Val loss 4.065\n",
            "Ep 5 (Step 014830): Train loss 3.360, Val loss 4.073\n",
            "Ep 5 (Step 014835): Train loss 3.509, Val loss 4.079\n",
            "Ep 5 (Step 014840): Train loss 3.044, Val loss 4.080\n",
            "Ep 5 (Step 014845): Train loss 3.305, Val loss 4.078\n",
            "Ep 5 (Step 014850): Train loss 2.930, Val loss 4.091\n",
            "Ep 5 (Step 014855): Train loss 3.044, Val loss 4.078\n",
            "Ep 5 (Step 014860): Train loss 3.361, Val loss 4.058\n",
            "Ep 5 (Step 014865): Train loss 3.318, Val loss 4.045\n",
            "Ep 5 (Step 014870): Train loss 3.086, Val loss 4.051\n",
            "Ep 5 (Step 014875): Train loss 3.434, Val loss 4.050\n",
            "Ep 5 (Step 014880): Train loss 3.142, Val loss 4.053\n",
            "Every effort moves you.  LEAR. I’ll tell you, sir, and I’ll be a man.  LEAR. I’ll be a man.  LEAR.  LEAR. \n",
            "Ep 6 (Step 014885): Train loss 3.141, Val loss 4.071\n",
            "Ep 6 (Step 014890): Train loss 3.122, Val loss 4.064\n",
            "Ep 6 (Step 014895): Train loss 3.165, Val loss 4.078\n",
            "Ep 6 (Step 014900): Train loss 2.783, Val loss 4.094\n",
            "Ep 6 (Step 014905): Train loss 3.351, Val loss 4.109\n",
            "Ep 6 (Step 014910): Train loss 3.061, Val loss 4.111\n",
            "Ep 6 (Step 014915): Train loss 3.346, Val loss 4.106\n",
            "Ep 6 (Step 014920): Train loss 3.527, Val loss 4.118\n",
            "Ep 6 (Step 014925): Train loss 3.304, Val loss 4.149\n",
            "Ep 6 (Step 014930): Train loss 3.357, Val loss 4.162\n",
            "Ep 6 (Step 014935): Train loss 3.395, Val loss 4.128\n",
            "Ep 6 (Step 014940): Train loss 3.140, Val loss 4.125\n",
            "Ep 6 (Step 014945): Train loss 3.335, Val loss 4.126\n",
            "Ep 6 (Step 014950): Train loss 3.008, Val loss 4.143\n",
            "Ep 6 (Step 014955): Train loss 3.208, Val loss 4.150\n",
            "Ep 6 (Step 014960): Train loss 3.190, Val loss 4.119\n",
            "Ep 6 (Step 014965): Train loss 3.344, Val loss 4.090\n",
            "Ep 6 (Step 014970): Train loss 3.174, Val loss 4.088\n",
            "Ep 6 (Step 014975): Train loss 3.087, Val loss 4.103\n",
            "Ep 6 (Step 014980): Train loss 3.241, Val loss 4.127\n",
            "Ep 6 (Step 014985): Train loss 3.277, Val loss 4.120\n",
            "Ep 6 (Step 014990): Train loss 3.441, Val loss 4.136\n",
            "Ep 6 (Step 014995): Train loss 3.022, Val loss 4.131\n",
            "Ep 6 (Step 015000): Train loss 3.081, Val loss 4.134\n",
            "Ep 6 (Step 015005): Train loss 3.332, Val loss 4.135\n",
            "Ep 6 (Step 015010): Train loss 3.225, Val loss 4.134\n",
            "Ep 6 (Step 015015): Train loss 3.334, Val loss 4.146\n",
            "Ep 6 (Step 015020): Train loss 3.103, Val loss 4.132\n",
            "Ep 6 (Step 015025): Train loss 3.340, Val loss 4.119\n",
            "Ep 6 (Step 015030): Train loss 3.179, Val loss 4.115\n",
            "Ep 6 (Step 015035): Train loss 3.328, Val loss 4.106\n",
            "Ep 6 (Step 015040): Train loss 3.321, Val loss 4.114\n",
            "Ep 6 (Step 015045): Train loss 2.978, Val loss 4.109\n",
            "Ep 6 (Step 015050): Train loss 3.217, Val loss 4.106\n",
            "Ep 6 (Step 015055): Train loss 3.152, Val loss 4.118\n",
            "Ep 6 (Step 015060): Train loss 3.600, Val loss 4.123\n",
            "Ep 6 (Step 015065): Train loss 3.072, Val loss 4.122\n",
            "Ep 6 (Step 015070): Train loss 3.280, Val loss 4.120\n",
            "Ep 6 (Step 015075): Train loss 2.979, Val loss 4.123\n",
            "Ep 6 (Step 015080): Train loss 3.160, Val loss 4.132\n",
            "Ep 6 (Step 015085): Train loss 3.130, Val loss 4.127\n",
            "Ep 6 (Step 015090): Train loss 3.049, Val loss 4.142\n",
            "Ep 6 (Step 015095): Train loss 3.184, Val loss 4.141\n",
            "Ep 6 (Step 015100): Train loss 3.384, Val loss 4.136\n",
            "Ep 6 (Step 015105): Train loss 3.260, Val loss 4.143\n",
            "Ep 6 (Step 015110): Train loss 3.134, Val loss 4.163\n",
            "Ep 6 (Step 015115): Train loss 3.424, Val loss 4.153\n",
            "Ep 6 (Step 015120): Train loss 3.227, Val loss 4.147\n",
            "Ep 6 (Step 015125): Train loss 3.263, Val loss 4.139\n",
            "Ep 6 (Step 015130): Train loss 3.081, Val loss 4.144\n",
            "Ep 6 (Step 015135): Train loss 2.914, Val loss 4.148\n",
            "Ep 6 (Step 015140): Train loss 3.460, Val loss 4.151\n",
            "Ep 6 (Step 015145): Train loss 2.881, Val loss 4.162\n",
            "Ep 6 (Step 015150): Train loss 3.408, Val loss 4.147\n",
            "Ep 6 (Step 015155): Train loss 2.998, Val loss 4.142\n",
            "Ep 6 (Step 015160): Train loss 3.088, Val loss 4.133\n",
            "Ep 6 (Step 015165): Train loss 3.286, Val loss 4.127\n",
            "Ep 6 (Step 015170): Train loss 3.451, Val loss 4.113\n",
            "Ep 6 (Step 015175): Train loss 3.066, Val loss 4.111\n",
            "Ep 6 (Step 015180): Train loss 2.977, Val loss 4.116\n",
            "Ep 6 (Step 015185): Train loss 3.345, Val loss 4.132\n",
            "Ep 6 (Step 015190): Train loss 3.127, Val loss 4.128\n",
            "Ep 6 (Step 015195): Train loss 3.255, Val loss 4.133\n",
            "Ep 6 (Step 015200): Train loss 3.133, Val loss 4.129\n",
            "Ep 6 (Step 015205): Train loss 3.232, Val loss 4.137\n",
            "Ep 6 (Step 015210): Train loss 3.316, Val loss 4.135\n",
            "Ep 6 (Step 015215): Train loss 3.310, Val loss 4.147\n",
            "Ep 6 (Step 015220): Train loss 3.415, Val loss 4.147\n",
            "Ep 6 (Step 015225): Train loss 3.376, Val loss 4.152\n",
            "Ep 6 (Step 015230): Train loss 3.362, Val loss 4.129\n",
            "Ep 6 (Step 015235): Train loss 2.957, Val loss 4.136\n",
            "Ep 6 (Step 015240): Train loss 2.919, Val loss 4.146\n",
            "Ep 6 (Step 015245): Train loss 3.147, Val loss 4.153\n",
            "Ep 6 (Step 015250): Train loss 3.332, Val loss 4.167\n",
            "Ep 6 (Step 015255): Train loss 3.057, Val loss 4.159\n",
            "Ep 6 (Step 015260): Train loss 2.715, Val loss 4.148\n",
            "Ep 6 (Step 015265): Train loss 3.416, Val loss 4.158\n",
            "Ep 6 (Step 015270): Train loss 3.128, Val loss 4.169\n",
            "Ep 6 (Step 015275): Train loss 3.288, Val loss 4.158\n",
            "Ep 6 (Step 015280): Train loss 3.284, Val loss 4.153\n",
            "Ep 6 (Step 015285): Train loss 3.011, Val loss 4.149\n",
            "Ep 6 (Step 015290): Train loss 3.002, Val loss 4.152\n",
            "Ep 6 (Step 015295): Train loss 3.309, Val loss 4.152\n",
            "Ep 6 (Step 015300): Train loss 3.291, Val loss 4.133\n",
            "Ep 6 (Step 015305): Train loss 3.141, Val loss 4.142\n",
            "Ep 6 (Step 015310): Train loss 3.203, Val loss 4.167\n",
            "Ep 6 (Step 015315): Train loss 3.276, Val loss 4.156\n",
            "Ep 6 (Step 015320): Train loss 3.094, Val loss 4.156\n",
            "Ep 6 (Step 015325): Train loss 3.523, Val loss 4.182\n",
            "Ep 6 (Step 015330): Train loss 3.049, Val loss 4.173\n",
            "Ep 6 (Step 015335): Train loss 3.205, Val loss 4.152\n",
            "Ep 6 (Step 015340): Train loss 3.334, Val loss 4.136\n",
            "Ep 6 (Step 015345): Train loss 3.436, Val loss 4.136\n",
            "Ep 6 (Step 015350): Train loss 3.247, Val loss 4.140\n",
            "Ep 6 (Step 015355): Train loss 3.253, Val loss 4.145\n",
            "Ep 6 (Step 015360): Train loss 3.079, Val loss 4.136\n",
            "Ep 6 (Step 015365): Train loss 3.061, Val loss 4.133\n",
            "Ep 6 (Step 015370): Train loss 3.283, Val loss 4.139\n",
            "Ep 6 (Step 015375): Train loss 3.262, Val loss 4.144\n",
            "Ep 6 (Step 015380): Train loss 3.368, Val loss 4.153\n",
            "Ep 6 (Step 015385): Train loss 3.447, Val loss 4.159\n",
            "Ep 6 (Step 015390): Train loss 3.001, Val loss 4.170\n",
            "Ep 6 (Step 015395): Train loss 3.124, Val loss 4.162\n",
            "Ep 6 (Step 015400): Train loss 2.878, Val loss 4.168\n",
            "Ep 6 (Step 015405): Train loss 3.036, Val loss 4.171\n",
            "Ep 6 (Step 015410): Train loss 3.266, Val loss 4.160\n",
            "Ep 6 (Step 015415): Train loss 3.091, Val loss 4.150\n",
            "Ep 6 (Step 015420): Train loss 3.388, Val loss 4.155\n",
            "Ep 6 (Step 015425): Train loss 3.406, Val loss 4.155\n",
            "Ep 6 (Step 015430): Train loss 3.022, Val loss 4.153\n",
            "Ep 6 (Step 015435): Train loss 3.320, Val loss 4.153\n",
            "Ep 6 (Step 015440): Train loss 3.149, Val loss 4.162\n",
            "Ep 6 (Step 015445): Train loss 3.254, Val loss 4.168\n",
            "Ep 6 (Step 015450): Train loss 3.085, Val loss 4.155\n",
            "Ep 6 (Step 015455): Train loss 2.998, Val loss 4.143\n",
            "Ep 6 (Step 015460): Train loss 3.371, Val loss 4.151\n",
            "Ep 6 (Step 015465): Train loss 3.222, Val loss 4.145\n",
            "Ep 6 (Step 015470): Train loss 3.156, Val loss 4.154\n",
            "Ep 6 (Step 015475): Train loss 3.270, Val loss 4.155\n",
            "Ep 6 (Step 015480): Train loss 3.101, Val loss 4.161\n",
            "Ep 6 (Step 015485): Train loss 3.366, Val loss 4.167\n",
            "Ep 6 (Step 015490): Train loss 3.079, Val loss 4.173\n",
            "Ep 6 (Step 015495): Train loss 3.337, Val loss 4.163\n",
            "Ep 6 (Step 015500): Train loss 3.205, Val loss 4.159\n",
            "Ep 6 (Step 015505): Train loss 3.449, Val loss 4.156\n",
            "Ep 6 (Step 015510): Train loss 3.105, Val loss 4.159\n",
            "Ep 6 (Step 015515): Train loss 3.324, Val loss 4.145\n",
            "Ep 6 (Step 015520): Train loss 3.025, Val loss 4.150\n",
            "Ep 6 (Step 015525): Train loss 3.517, Val loss 4.160\n",
            "Ep 6 (Step 015530): Train loss 3.297, Val loss 4.168\n",
            "Ep 6 (Step 015535): Train loss 3.384, Val loss 4.161\n",
            "Ep 6 (Step 015540): Train loss 3.049, Val loss 4.160\n",
            "Ep 6 (Step 015545): Train loss 3.373, Val loss 4.158\n",
            "Ep 6 (Step 015550): Train loss 3.559, Val loss 4.161\n",
            "Ep 6 (Step 015555): Train loss 3.151, Val loss 4.154\n",
            "Ep 6 (Step 015560): Train loss 2.967, Val loss 4.167\n",
            "Ep 6 (Step 015565): Train loss 3.016, Val loss 4.178\n",
            "Ep 6 (Step 015570): Train loss 2.967, Val loss 4.164\n",
            "Ep 6 (Step 015575): Train loss 3.207, Val loss 4.164\n",
            "Ep 6 (Step 015580): Train loss 3.618, Val loss 4.177\n",
            "Ep 6 (Step 015585): Train loss 3.084, Val loss 4.172\n",
            "Ep 6 (Step 015590): Train loss 3.048, Val loss 4.148\n",
            "Ep 6 (Step 015595): Train loss 2.931, Val loss 4.149\n",
            "Ep 6 (Step 015600): Train loss 3.211, Val loss 4.146\n",
            "Ep 6 (Step 015605): Train loss 3.462, Val loss 4.154\n",
            "Ep 6 (Step 015610): Train loss 3.252, Val loss 4.159\n",
            "Ep 6 (Step 015615): Train loss 3.125, Val loss 4.165\n",
            "Ep 6 (Step 015620): Train loss 2.741, Val loss 4.151\n",
            "Ep 6 (Step 015625): Train loss 3.168, Val loss 4.151\n",
            "Ep 6 (Step 015630): Train loss 3.337, Val loss 4.160\n",
            "Ep 6 (Step 015635): Train loss 3.446, Val loss 4.173\n",
            "Ep 6 (Step 015640): Train loss 3.139, Val loss 4.162\n",
            "Ep 6 (Step 015645): Train loss 3.316, Val loss 4.147\n",
            "Ep 6 (Step 015650): Train loss 3.405, Val loss 4.142\n",
            "Ep 6 (Step 015655): Train loss 2.956, Val loss 4.130\n",
            "Ep 6 (Step 015660): Train loss 3.288, Val loss 4.138\n",
            "Ep 6 (Step 015665): Train loss 3.279, Val loss 4.137\n",
            "Ep 6 (Step 015670): Train loss 3.276, Val loss 4.131\n",
            "Ep 6 (Step 015675): Train loss 3.097, Val loss 4.132\n",
            "Ep 6 (Step 015680): Train loss 3.043, Val loss 4.147\n",
            "Ep 6 (Step 015685): Train loss 2.954, Val loss 4.143\n",
            "Ep 6 (Step 015690): Train loss 3.265, Val loss 4.142\n",
            "Ep 6 (Step 015695): Train loss 2.965, Val loss 4.145\n",
            "Ep 6 (Step 015700): Train loss 3.067, Val loss 4.142\n",
            "Ep 6 (Step 015705): Train loss 3.243, Val loss 4.145\n",
            "Ep 6 (Step 015710): Train loss 3.031, Val loss 4.154\n",
            "Ep 6 (Step 015715): Train loss 3.123, Val loss 4.158\n",
            "Ep 6 (Step 015720): Train loss 3.225, Val loss 4.157\n",
            "Ep 6 (Step 015725): Train loss 2.783, Val loss 4.157\n",
            "Ep 6 (Step 015730): Train loss 3.142, Val loss 4.153\n",
            "Ep 6 (Step 015735): Train loss 3.105, Val loss 4.154\n",
            "Ep 6 (Step 015740): Train loss 3.398, Val loss 4.149\n",
            "Ep 6 (Step 015745): Train loss 3.042, Val loss 4.134\n",
            "Ep 6 (Step 015750): Train loss 3.161, Val loss 4.130\n",
            "Ep 6 (Step 015755): Train loss 3.012, Val loss 4.119\n",
            "Ep 6 (Step 015760): Train loss 3.544, Val loss 4.106\n",
            "Ep 6 (Step 015765): Train loss 3.314, Val loss 4.120\n",
            "Ep 6 (Step 015770): Train loss 3.192, Val loss 4.135\n",
            "Ep 6 (Step 015775): Train loss 3.150, Val loss 4.142\n",
            "Ep 6 (Step 015780): Train loss 3.534, Val loss 4.142\n",
            "Ep 6 (Step 015785): Train loss 3.394, Val loss 4.154\n",
            "Ep 6 (Step 015790): Train loss 2.833, Val loss 4.139\n",
            "Ep 6 (Step 015795): Train loss 2.986, Val loss 4.130\n",
            "Ep 6 (Step 015800): Train loss 3.113, Val loss 4.131\n",
            "Ep 6 (Step 015805): Train loss 3.409, Val loss 4.126\n",
            "Ep 6 (Step 015810): Train loss 3.103, Val loss 4.117\n",
            "Ep 6 (Step 015815): Train loss 3.329, Val loss 4.116\n",
            "Ep 6 (Step 015820): Train loss 3.424, Val loss 4.117\n",
            "Ep 6 (Step 015825): Train loss 3.528, Val loss 4.130\n",
            "Ep 6 (Step 015830): Train loss 3.125, Val loss 4.125\n",
            "Ep 6 (Step 015835): Train loss 2.788, Val loss 4.131\n",
            "Ep 6 (Step 015840): Train loss 3.082, Val loss 4.133\n",
            "Ep 6 (Step 015845): Train loss 3.014, Val loss 4.126\n",
            "Ep 6 (Step 015850): Train loss 3.361, Val loss 4.103\n",
            "Ep 6 (Step 015855): Train loss 3.043, Val loss 4.102\n",
            "Ep 6 (Step 015860): Train loss 2.869, Val loss 4.095\n",
            "Ep 6 (Step 015865): Train loss 3.126, Val loss 4.091\n",
            "Ep 6 (Step 015870): Train loss 3.083, Val loss 4.094\n",
            "Ep 6 (Step 015875): Train loss 3.001, Val loss 4.091\n",
            "Ep 6 (Step 015880): Train loss 3.149, Val loss 4.086\n",
            "Ep 6 (Step 015885): Train loss 3.343, Val loss 4.088\n",
            "Ep 6 (Step 015890): Train loss 3.113, Val loss 4.110\n",
            "Ep 6 (Step 015895): Train loss 3.148, Val loss 4.109\n",
            "Ep 6 (Step 015900): Train loss 3.008, Val loss 4.094\n",
            "Ep 6 (Step 015905): Train loss 2.900, Val loss 4.096\n",
            "Ep 6 (Step 015910): Train loss 3.017, Val loss 4.110\n",
            "Ep 6 (Step 015915): Train loss 2.984, Val loss 4.109\n",
            "Ep 6 (Step 015920): Train loss 3.021, Val loss 4.104\n",
            "Ep 6 (Step 015925): Train loss 2.955, Val loss 4.104\n",
            "Ep 6 (Step 015930): Train loss 3.323, Val loss 4.114\n",
            "Ep 6 (Step 015935): Train loss 3.557, Val loss 4.129\n",
            "Ep 6 (Step 015940): Train loss 3.233, Val loss 4.132\n",
            "Ep 6 (Step 015945): Train loss 3.061, Val loss 4.121\n",
            "Ep 6 (Step 015950): Train loss 3.450, Val loss 4.133\n",
            "Ep 6 (Step 015955): Train loss 3.166, Val loss 4.128\n",
            "Ep 6 (Step 015960): Train loss 3.420, Val loss 4.117\n",
            "Ep 6 (Step 015965): Train loss 3.262, Val loss 4.102\n",
            "Ep 6 (Step 015970): Train loss 3.363, Val loss 4.100\n",
            "Ep 6 (Step 015975): Train loss 2.938, Val loss 4.114\n",
            "Ep 6 (Step 015980): Train loss 3.321, Val loss 4.124\n",
            "Ep 6 (Step 015985): Train loss 3.396, Val loss 4.129\n",
            "Ep 6 (Step 015990): Train loss 3.288, Val loss 4.130\n",
            "Ep 6 (Step 015995): Train loss 3.118, Val loss 4.132\n",
            "Ep 6 (Step 016000): Train loss 3.451, Val loss 4.119\n",
            "Ep 6 (Step 016005): Train loss 2.838, Val loss 4.117\n",
            "Ep 6 (Step 016010): Train loss 3.040, Val loss 4.123\n",
            "Ep 6 (Step 016015): Train loss 3.297, Val loss 4.129\n",
            "Ep 6 (Step 016020): Train loss 2.996, Val loss 4.118\n",
            "Ep 6 (Step 016025): Train loss 2.953, Val loss 4.096\n",
            "Ep 6 (Step 016030): Train loss 3.497, Val loss 4.095\n",
            "Ep 6 (Step 016035): Train loss 3.208, Val loss 4.104\n",
            "Ep 6 (Step 016040): Train loss 3.223, Val loss 4.102\n",
            "Ep 6 (Step 016045): Train loss 3.047, Val loss 4.093\n",
            "Ep 6 (Step 016050): Train loss 2.882, Val loss 4.085\n",
            "Ep 6 (Step 016055): Train loss 3.217, Val loss 4.089\n",
            "Ep 6 (Step 016060): Train loss 3.437, Val loss 4.101\n",
            "Ep 6 (Step 016065): Train loss 3.429, Val loss 4.108\n",
            "Ep 6 (Step 016070): Train loss 3.117, Val loss 4.102\n",
            "Ep 6 (Step 016075): Train loss 3.174, Val loss 4.092\n",
            "Ep 6 (Step 016080): Train loss 3.224, Val loss 4.096\n",
            "Ep 6 (Step 016085): Train loss 3.155, Val loss 4.118\n",
            "Ep 6 (Step 016090): Train loss 2.808, Val loss 4.128\n",
            "Ep 6 (Step 016095): Train loss 3.012, Val loss 4.120\n",
            "Ep 6 (Step 016100): Train loss 3.244, Val loss 4.110\n",
            "Ep 6 (Step 016105): Train loss 3.201, Val loss 4.128\n",
            "Ep 6 (Step 016110): Train loss 2.956, Val loss 4.148\n",
            "Ep 6 (Step 016115): Train loss 2.942, Val loss 4.141\n",
            "Ep 6 (Step 016120): Train loss 3.137, Val loss 4.131\n",
            "Ep 6 (Step 016125): Train loss 2.878, Val loss 4.125\n",
            "Ep 6 (Step 016130): Train loss 3.061, Val loss 4.111\n",
            "Ep 6 (Step 016135): Train loss 3.288, Val loss 4.114\n",
            "Ep 6 (Step 016140): Train loss 3.148, Val loss 4.121\n",
            "Ep 6 (Step 016145): Train loss 2.852, Val loss 4.128\n",
            "Ep 6 (Step 016150): Train loss 2.956, Val loss 4.094\n",
            "Ep 6 (Step 016155): Train loss 3.004, Val loss 4.085\n",
            "Ep 6 (Step 016160): Train loss 3.304, Val loss 4.080\n",
            "Ep 6 (Step 016165): Train loss 3.261, Val loss 4.086\n",
            "Ep 6 (Step 016170): Train loss 3.073, Val loss 4.095\n",
            "Ep 6 (Step 016175): Train loss 3.496, Val loss 4.103\n",
            "Ep 6 (Step 016180): Train loss 3.034, Val loss 4.102\n",
            "Ep 6 (Step 016185): Train loss 3.242, Val loss 4.101\n",
            "Ep 6 (Step 016190): Train loss 3.261, Val loss 4.101\n",
            "Ep 6 (Step 016195): Train loss 2.973, Val loss 4.097\n",
            "Ep 6 (Step 016200): Train loss 3.045, Val loss 4.083\n",
            "Ep 6 (Step 016205): Train loss 2.968, Val loss 4.095\n",
            "Ep 6 (Step 016210): Train loss 3.232, Val loss 4.122\n",
            "Ep 6 (Step 016215): Train loss 3.047, Val loss 4.110\n",
            "Ep 6 (Step 016220): Train loss 2.884, Val loss 4.093\n",
            "Ep 6 (Step 016225): Train loss 2.895, Val loss 4.090\n",
            "Ep 6 (Step 016230): Train loss 3.134, Val loss 4.095\n",
            "Ep 6 (Step 016235): Train loss 3.117, Val loss 4.089\n",
            "Ep 6 (Step 016240): Train loss 3.012, Val loss 4.087\n",
            "Ep 6 (Step 016245): Train loss 2.925, Val loss 4.083\n",
            "Ep 6 (Step 016250): Train loss 3.024, Val loss 4.085\n",
            "Ep 6 (Step 016255): Train loss 3.179, Val loss 4.093\n",
            "Ep 6 (Step 016260): Train loss 3.426, Val loss 4.101\n",
            "Ep 6 (Step 016265): Train loss 3.149, Val loss 4.115\n",
            "Ep 6 (Step 016270): Train loss 2.856, Val loss 4.118\n",
            "Ep 6 (Step 016275): Train loss 3.146, Val loss 4.094\n",
            "Ep 6 (Step 016280): Train loss 2.927, Val loss 4.082\n",
            "Ep 6 (Step 016285): Train loss 3.092, Val loss 4.075\n",
            "Ep 6 (Step 016290): Train loss 3.008, Val loss 4.076\n",
            "Ep 6 (Step 016295): Train loss 3.074, Val loss 4.078\n",
            "Ep 6 (Step 016300): Train loss 3.051, Val loss 4.076\n",
            "Ep 6 (Step 016305): Train loss 3.310, Val loss 4.081\n",
            "Ep 6 (Step 016310): Train loss 3.360, Val loss 4.080\n",
            "Ep 6 (Step 016315): Train loss 3.065, Val loss 4.089\n",
            "Ep 6 (Step 016320): Train loss 3.071, Val loss 4.098\n",
            "Ep 6 (Step 016325): Train loss 3.054, Val loss 4.092\n",
            "Ep 6 (Step 016330): Train loss 3.018, Val loss 4.092\n",
            "Ep 6 (Step 016335): Train loss 2.922, Val loss 4.092\n",
            "Ep 6 (Step 016340): Train loss 3.351, Val loss 4.090\n",
            "Ep 6 (Step 016345): Train loss 2.672, Val loss 4.092\n",
            "Ep 6 (Step 016350): Train loss 3.021, Val loss 4.092\n",
            "Ep 6 (Step 016355): Train loss 3.114, Val loss 4.092\n",
            "Ep 6 (Step 016360): Train loss 3.031, Val loss 4.097\n",
            "Ep 6 (Step 016365): Train loss 3.178, Val loss 4.096\n",
            "Ep 6 (Step 016370): Train loss 3.200, Val loss 4.117\n",
            "Ep 6 (Step 016375): Train loss 3.308, Val loss 4.108\n",
            "Ep 6 (Step 016380): Train loss 3.499, Val loss 4.105\n",
            "Ep 6 (Step 016385): Train loss 3.273, Val loss 4.108\n",
            "Ep 6 (Step 016390): Train loss 3.148, Val loss 4.115\n",
            "Ep 6 (Step 016395): Train loss 3.007, Val loss 4.113\n",
            "Ep 6 (Step 016400): Train loss 2.709, Val loss 4.104\n",
            "Ep 6 (Step 016405): Train loss 3.221, Val loss 4.093\n",
            "Ep 6 (Step 016410): Train loss 2.902, Val loss 4.074\n",
            "Ep 6 (Step 016415): Train loss 3.299, Val loss 4.069\n",
            "Ep 6 (Step 016420): Train loss 3.203, Val loss 4.078\n",
            "Ep 6 (Step 016425): Train loss 3.267, Val loss 4.084\n",
            "Ep 6 (Step 016430): Train loss 2.975, Val loss 4.076\n",
            "Ep 6 (Step 016435): Train loss 3.103, Val loss 4.066\n",
            "Ep 6 (Step 016440): Train loss 3.060, Val loss 4.068\n",
            "Ep 6 (Step 016445): Train loss 3.053, Val loss 4.083\n",
            "Ep 6 (Step 016450): Train loss 2.965, Val loss 4.084\n",
            "Ep 6 (Step 016455): Train loss 2.899, Val loss 4.081\n",
            "Ep 6 (Step 016460): Train loss 3.131, Val loss 4.086\n",
            "Ep 6 (Step 016465): Train loss 3.100, Val loss 4.090\n",
            "Ep 6 (Step 016470): Train loss 3.217, Val loss 4.097\n",
            "Ep 6 (Step 016475): Train loss 3.176, Val loss 4.093\n",
            "Ep 6 (Step 016480): Train loss 3.061, Val loss 4.073\n",
            "Ep 6 (Step 016485): Train loss 3.365, Val loss 4.052\n",
            "Ep 6 (Step 016490): Train loss 2.917, Val loss 4.045\n",
            "Ep 6 (Step 016495): Train loss 3.120, Val loss 4.056\n",
            "Ep 6 (Step 016500): Train loss 3.104, Val loss 4.067\n",
            "Ep 6 (Step 016505): Train loss 3.006, Val loss 4.068\n",
            "Ep 6 (Step 016510): Train loss 3.056, Val loss 4.065\n",
            "Ep 6 (Step 016515): Train loss 3.454, Val loss 4.070\n",
            "Ep 6 (Step 016520): Train loss 2.941, Val loss 4.078\n",
            "Ep 6 (Step 016525): Train loss 2.985, Val loss 4.094\n",
            "Ep 6 (Step 016530): Train loss 3.123, Val loss 4.094\n",
            "Ep 6 (Step 016535): Train loss 2.989, Val loss 4.104\n",
            "Ep 6 (Step 016540): Train loss 2.886, Val loss 4.101\n",
            "Ep 6 (Step 016545): Train loss 2.771, Val loss 4.092\n",
            "Ep 6 (Step 016550): Train loss 3.058, Val loss 4.084\n",
            "Ep 6 (Step 016555): Train loss 3.112, Val loss 4.081\n",
            "Ep 6 (Step 016560): Train loss 2.940, Val loss 4.093\n",
            "Ep 6 (Step 016565): Train loss 2.970, Val loss 4.092\n",
            "Ep 6 (Step 016570): Train loss 2.946, Val loss 4.079\n",
            "Ep 6 (Step 016575): Train loss 2.930, Val loss 4.077\n",
            "Ep 6 (Step 016580): Train loss 3.076, Val loss 4.079\n",
            "Ep 6 (Step 016585): Train loss 3.228, Val loss 4.084\n",
            "Ep 6 (Step 016590): Train loss 3.096, Val loss 4.077\n",
            "Ep 6 (Step 016595): Train loss 3.022, Val loss 4.072\n",
            "Ep 6 (Step 016600): Train loss 2.851, Val loss 4.072\n",
            "Ep 6 (Step 016605): Train loss 2.950, Val loss 4.072\n",
            "Ep 6 (Step 016610): Train loss 3.275, Val loss 4.067\n",
            "Ep 6 (Step 016615): Train loss 3.044, Val loss 4.055\n",
            "Ep 6 (Step 016620): Train loss 2.867, Val loss 4.060\n",
            "Ep 6 (Step 016625): Train loss 2.974, Val loss 4.057\n",
            "Ep 6 (Step 016630): Train loss 2.968, Val loss 4.051\n",
            "Ep 6 (Step 016635): Train loss 3.096, Val loss 4.047\n",
            "Ep 6 (Step 016640): Train loss 2.861, Val loss 4.056\n",
            "Ep 6 (Step 016645): Train loss 3.363, Val loss 4.040\n",
            "Ep 6 (Step 016650): Train loss 3.064, Val loss 4.046\n",
            "Ep 6 (Step 016655): Train loss 3.201, Val loss 4.055\n",
            "Ep 6 (Step 016660): Train loss 3.117, Val loss 4.052\n",
            "Ep 6 (Step 016665): Train loss 3.402, Val loss 4.040\n",
            "Ep 6 (Step 016670): Train loss 3.123, Val loss 4.028\n",
            "Ep 6 (Step 016675): Train loss 3.210, Val loss 4.031\n",
            "Ep 6 (Step 016680): Train loss 2.826, Val loss 4.038\n",
            "Ep 6 (Step 016685): Train loss 3.372, Val loss 4.046\n",
            "Ep 6 (Step 016690): Train loss 2.715, Val loss 4.047\n",
            "Ep 6 (Step 016695): Train loss 2.876, Val loss 4.043\n",
            "Ep 6 (Step 016700): Train loss 2.993, Val loss 4.035\n",
            "Ep 6 (Step 016705): Train loss 3.063, Val loss 4.034\n",
            "Ep 6 (Step 016710): Train loss 3.250, Val loss 4.039\n",
            "Ep 6 (Step 016715): Train loss 2.737, Val loss 4.036\n",
            "Ep 6 (Step 016720): Train loss 2.971, Val loss 4.031\n",
            "Ep 6 (Step 016725): Train loss 3.094, Val loss 4.034\n",
            "Ep 6 (Step 016730): Train loss 3.145, Val loss 4.051\n",
            "Ep 6 (Step 016735): Train loss 2.761, Val loss 4.049\n",
            "Ep 6 (Step 016740): Train loss 3.151, Val loss 4.052\n",
            "Ep 6 (Step 016745): Train loss 3.085, Val loss 4.052\n",
            "Ep 6 (Step 016750): Train loss 2.717, Val loss 4.049\n",
            "Ep 6 (Step 016755): Train loss 3.241, Val loss 4.051\n",
            "Ep 6 (Step 016760): Train loss 3.112, Val loss 4.053\n",
            "Ep 6 (Step 016765): Train loss 2.882, Val loss 4.054\n",
            "Ep 6 (Step 016770): Train loss 3.064, Val loss 4.061\n",
            "Ep 6 (Step 016775): Train loss 3.112, Val loss 4.063\n",
            "Ep 6 (Step 016780): Train loss 2.972, Val loss 4.056\n",
            "Ep 6 (Step 016785): Train loss 3.473, Val loss 4.049\n",
            "Ep 6 (Step 016790): Train loss 2.971, Val loss 4.056\n",
            "Ep 6 (Step 016795): Train loss 3.158, Val loss 4.057\n",
            "Ep 6 (Step 016800): Train loss 2.989, Val loss 4.049\n",
            "Ep 6 (Step 016805): Train loss 2.911, Val loss 4.052\n",
            "Ep 6 (Step 016810): Train loss 3.282, Val loss 4.060\n",
            "Ep 6 (Step 016815): Train loss 3.232, Val loss 4.061\n",
            "Ep 6 (Step 016820): Train loss 3.059, Val loss 4.053\n",
            "Ep 6 (Step 016825): Train loss 3.069, Val loss 4.050\n",
            "Ep 6 (Step 016830): Train loss 2.973, Val loss 4.056\n",
            "Ep 6 (Step 016835): Train loss 2.958, Val loss 4.052\n",
            "Ep 6 (Step 016840): Train loss 3.167, Val loss 4.058\n",
            "Ep 6 (Step 016845): Train loss 3.039, Val loss 4.058\n",
            "Ep 6 (Step 016850): Train loss 3.246, Val loss 4.058\n",
            "Ep 6 (Step 016855): Train loss 2.897, Val loss 4.064\n",
            "Ep 6 (Step 016860): Train loss 3.278, Val loss 4.067\n",
            "Ep 6 (Step 016865): Train loss 2.905, Val loss 4.057\n",
            "Ep 6 (Step 016870): Train loss 3.055, Val loss 4.048\n",
            "Ep 6 (Step 016875): Train loss 3.019, Val loss 4.049\n",
            "Ep 6 (Step 016880): Train loss 3.007, Val loss 4.053\n",
            "Ep 6 (Step 016885): Train loss 3.083, Val loss 4.049\n",
            "Ep 6 (Step 016890): Train loss 3.178, Val loss 4.063\n",
            "Ep 6 (Step 016895): Train loss 3.361, Val loss 4.083\n",
            "Ep 6 (Step 016900): Train loss 3.294, Val loss 4.077\n",
            "Ep 6 (Step 016905): Train loss 2.991, Val loss 4.065\n",
            "Ep 6 (Step 016910): Train loss 2.921, Val loss 4.057\n",
            "Ep 6 (Step 016915): Train loss 2.993, Val loss 4.062\n",
            "Ep 6 (Step 016920): Train loss 2.849, Val loss 4.052\n",
            "Ep 6 (Step 016925): Train loss 3.151, Val loss 4.036\n",
            "Ep 6 (Step 016930): Train loss 3.054, Val loss 4.039\n",
            "Ep 6 (Step 016935): Train loss 3.158, Val loss 4.054\n",
            "Ep 6 (Step 016940): Train loss 3.034, Val loss 4.063\n",
            "Ep 6 (Step 016945): Train loss 3.195, Val loss 4.063\n",
            "Ep 6 (Step 016950): Train loss 3.154, Val loss 4.066\n",
            "Ep 6 (Step 016955): Train loss 3.197, Val loss 4.069\n",
            "Ep 6 (Step 016960): Train loss 3.150, Val loss 4.078\n",
            "Ep 6 (Step 016965): Train loss 3.235, Val loss 4.064\n",
            "Ep 6 (Step 016970): Train loss 2.978, Val loss 4.050\n",
            "Ep 6 (Step 016975): Train loss 2.974, Val loss 4.051\n",
            "Ep 6 (Step 016980): Train loss 3.138, Val loss 4.063\n",
            "Ep 6 (Step 016985): Train loss 3.017, Val loss 4.072\n",
            "Ep 6 (Step 016990): Train loss 2.924, Val loss 4.056\n",
            "Ep 6 (Step 016995): Train loss 3.143, Val loss 4.052\n",
            "Ep 6 (Step 017000): Train loss 2.934, Val loss 4.055\n",
            "Ep 6 (Step 017005): Train loss 3.274, Val loss 4.070\n",
            "Ep 6 (Step 017010): Train loss 3.064, Val loss 4.066\n",
            "Ep 6 (Step 017015): Train loss 3.135, Val loss 4.053\n",
            "Ep 6 (Step 017020): Train loss 2.967, Val loss 4.072\n",
            "Ep 6 (Step 017025): Train loss 2.864, Val loss 4.090\n",
            "Ep 6 (Step 017030): Train loss 3.187, Val loss 4.080\n",
            "Ep 6 (Step 017035): Train loss 3.096, Val loss 4.058\n",
            "Ep 6 (Step 017040): Train loss 3.224, Val loss 4.058\n",
            "Ep 6 (Step 017045): Train loss 3.035, Val loss 4.066\n",
            "Ep 6 (Step 017050): Train loss 2.890, Val loss 4.075\n",
            "Ep 6 (Step 017055): Train loss 2.917, Val loss 4.067\n",
            "Ep 6 (Step 017060): Train loss 2.968, Val loss 4.054\n",
            "Ep 6 (Step 017065): Train loss 3.166, Val loss 4.048\n",
            "Ep 6 (Step 017070): Train loss 3.186, Val loss 4.055\n",
            "Ep 6 (Step 017075): Train loss 3.253, Val loss 4.080\n",
            "Ep 6 (Step 017080): Train loss 3.244, Val loss 4.070\n",
            "Ep 6 (Step 017085): Train loss 3.142, Val loss 4.054\n",
            "Ep 6 (Step 017090): Train loss 3.283, Val loss 4.027\n",
            "Ep 6 (Step 017095): Train loss 3.072, Val loss 4.019\n",
            "Ep 6 (Step 017100): Train loss 3.386, Val loss 4.026\n",
            "Ep 6 (Step 017105): Train loss 2.995, Val loss 4.029\n",
            "Ep 6 (Step 017110): Train loss 3.112, Val loss 4.020\n",
            "Ep 6 (Step 017115): Train loss 2.955, Val loss 4.016\n",
            "Ep 6 (Step 017120): Train loss 3.096, Val loss 4.035\n",
            "Ep 6 (Step 017125): Train loss 3.374, Val loss 4.050\n",
            "Ep 6 (Step 017130): Train loss 3.221, Val loss 4.039\n",
            "Ep 6 (Step 017135): Train loss 2.977, Val loss 4.037\n",
            "Ep 6 (Step 017140): Train loss 3.033, Val loss 4.040\n",
            "Ep 6 (Step 017145): Train loss 2.959, Val loss 4.041\n",
            "Ep 6 (Step 017150): Train loss 3.178, Val loss 4.045\n",
            "Ep 6 (Step 017155): Train loss 3.006, Val loss 4.045\n",
            "Ep 6 (Step 017160): Train loss 3.343, Val loss 4.053\n",
            "Ep 6 (Step 017165): Train loss 3.014, Val loss 4.043\n",
            "Ep 6 (Step 017170): Train loss 3.180, Val loss 4.048\n",
            "Ep 6 (Step 017175): Train loss 2.824, Val loss 4.045\n",
            "Ep 6 (Step 017180): Train loss 2.984, Val loss 4.048\n",
            "Ep 6 (Step 017185): Train loss 3.133, Val loss 4.046\n",
            "Ep 6 (Step 017190): Train loss 3.109, Val loss 4.046\n",
            "Ep 6 (Step 017195): Train loss 3.179, Val loss 4.053\n",
            "Ep 6 (Step 017200): Train loss 3.228, Val loss 4.059\n",
            "Ep 6 (Step 017205): Train loss 2.937, Val loss 4.046\n",
            "Ep 6 (Step 017210): Train loss 3.048, Val loss 4.035\n",
            "Ep 6 (Step 017215): Train loss 3.374, Val loss 4.040\n",
            "Ep 6 (Step 017220): Train loss 3.258, Val loss 4.038\n",
            "Ep 6 (Step 017225): Train loss 3.208, Val loss 4.037\n",
            "Ep 6 (Step 017230): Train loss 2.999, Val loss 4.038\n",
            "Ep 6 (Step 017235): Train loss 2.999, Val loss 4.037\n",
            "Ep 6 (Step 017240): Train loss 3.017, Val loss 4.039\n",
            "Ep 6 (Step 017245): Train loss 2.958, Val loss 4.037\n",
            "Ep 6 (Step 017250): Train loss 2.751, Val loss 4.031\n",
            "Ep 6 (Step 017255): Train loss 3.030, Val loss 4.037\n",
            "Ep 6 (Step 017260): Train loss 3.091, Val loss 4.030\n",
            "Ep 6 (Step 017265): Train loss 2.978, Val loss 4.023\n",
            "Ep 6 (Step 017270): Train loss 3.137, Val loss 4.027\n",
            "Ep 6 (Step 017275): Train loss 2.892, Val loss 4.040\n",
            "Ep 6 (Step 017280): Train loss 2.857, Val loss 4.039\n",
            "Ep 6 (Step 017285): Train loss 2.967, Val loss 4.042\n",
            "Ep 6 (Step 017290): Train loss 2.921, Val loss 4.050\n",
            "Ep 6 (Step 017295): Train loss 2.927, Val loss 4.049\n",
            "Ep 6 (Step 017300): Train loss 2.854, Val loss 4.047\n",
            "Ep 6 (Step 017305): Train loss 3.042, Val loss 4.055\n",
            "Ep 6 (Step 017310): Train loss 2.478, Val loss 4.066\n",
            "Ep 6 (Step 017315): Train loss 3.227, Val loss 4.061\n",
            "Ep 6 (Step 017320): Train loss 3.188, Val loss 4.069\n",
            "Ep 6 (Step 017325): Train loss 2.675, Val loss 4.060\n",
            "Ep 6 (Step 017330): Train loss 3.169, Val loss 4.067\n",
            "Ep 6 (Step 017335): Train loss 3.217, Val loss 4.071\n",
            "Ep 6 (Step 017340): Train loss 2.828, Val loss 4.065\n",
            "Ep 6 (Step 017345): Train loss 3.059, Val loss 4.066\n",
            "Ep 6 (Step 017350): Train loss 2.996, Val loss 4.069\n",
            "Ep 6 (Step 017355): Train loss 3.230, Val loss 4.079\n",
            "Ep 6 (Step 017360): Train loss 3.183, Val loss 4.064\n",
            "Ep 6 (Step 017365): Train loss 2.742, Val loss 4.048\n",
            "Ep 6 (Step 017370): Train loss 3.208, Val loss 4.046\n",
            "Ep 6 (Step 017375): Train loss 3.111, Val loss 4.053\n",
            "Ep 6 (Step 017380): Train loss 3.013, Val loss 4.053\n",
            "Ep 6 (Step 017385): Train loss 2.542, Val loss 4.042\n",
            "Ep 6 (Step 017390): Train loss 3.117, Val loss 4.043\n",
            "Ep 6 (Step 017395): Train loss 3.141, Val loss 4.056\n",
            "Ep 6 (Step 017400): Train loss 3.280, Val loss 4.049\n",
            "Ep 6 (Step 017405): Train loss 2.786, Val loss 4.036\n",
            "Ep 6 (Step 017410): Train loss 3.160, Val loss 4.039\n",
            "Ep 6 (Step 017415): Train loss 3.306, Val loss 4.055\n",
            "Ep 6 (Step 017420): Train loss 3.095, Val loss 4.055\n",
            "Ep 6 (Step 017425): Train loss 3.204, Val loss 4.036\n",
            "Ep 6 (Step 017430): Train loss 3.100, Val loss 4.031\n",
            "Ep 6 (Step 017435): Train loss 2.996, Val loss 4.041\n",
            "Ep 6 (Step 017440): Train loss 2.712, Val loss 4.065\n",
            "Ep 6 (Step 017445): Train loss 3.150, Val loss 4.055\n",
            "Ep 6 (Step 017450): Train loss 3.283, Val loss 4.052\n",
            "Ep 6 (Step 017455): Train loss 2.895, Val loss 4.049\n",
            "Ep 6 (Step 017460): Train loss 3.052, Val loss 4.040\n",
            "Ep 6 (Step 017465): Train loss 3.085, Val loss 4.035\n",
            "Ep 6 (Step 017470): Train loss 3.048, Val loss 4.030\n",
            "Ep 6 (Step 017475): Train loss 2.730, Val loss 4.019\n",
            "Ep 6 (Step 017480): Train loss 2.927, Val loss 4.020\n",
            "Ep 6 (Step 017485): Train loss 3.045, Val loss 4.026\n",
            "Ep 6 (Step 017490): Train loss 3.157, Val loss 4.029\n",
            "Ep 6 (Step 017495): Train loss 3.230, Val loss 4.045\n",
            "Ep 6 (Step 017500): Train loss 2.803, Val loss 4.046\n",
            "Ep 6 (Step 017505): Train loss 3.065, Val loss 4.053\n",
            "Ep 6 (Step 017510): Train loss 2.879, Val loss 4.055\n",
            "Ep 6 (Step 017515): Train loss 3.026, Val loss 4.054\n",
            "Ep 6 (Step 017520): Train loss 3.017, Val loss 4.049\n",
            "Ep 6 (Step 017525): Train loss 3.055, Val loss 4.052\n",
            "Ep 6 (Step 017530): Train loss 3.083, Val loss 4.060\n",
            "Ep 6 (Step 017535): Train loss 3.241, Val loss 4.056\n",
            "Ep 6 (Step 017540): Train loss 3.172, Val loss 4.063\n",
            "Ep 6 (Step 017545): Train loss 3.132, Val loss 4.066\n",
            "Ep 6 (Step 017550): Train loss 3.016, Val loss 4.061\n",
            "Ep 6 (Step 017555): Train loss 3.047, Val loss 4.063\n",
            "Ep 6 (Step 017560): Train loss 3.128, Val loss 4.074\n",
            "Ep 6 (Step 017565): Train loss 2.996, Val loss 4.063\n",
            "Ep 6 (Step 017570): Train loss 2.993, Val loss 4.059\n",
            "Ep 6 (Step 017575): Train loss 3.016, Val loss 4.051\n",
            "Ep 6 (Step 017580): Train loss 2.988, Val loss 4.056\n",
            "Ep 6 (Step 017585): Train loss 3.199, Val loss 4.062\n",
            "Ep 6 (Step 017590): Train loss 2.848, Val loss 4.060\n",
            "Ep 6 (Step 017595): Train loss 3.022, Val loss 4.049\n",
            "Ep 6 (Step 017600): Train loss 3.065, Val loss 4.045\n",
            "Ep 6 (Step 017605): Train loss 2.937, Val loss 4.051\n",
            "Ep 6 (Step 017610): Train loss 3.077, Val loss 4.039\n",
            "Ep 6 (Step 017615): Train loss 2.810, Val loss 4.038\n",
            "Ep 6 (Step 017620): Train loss 2.985, Val loss 4.044\n",
            "Ep 6 (Step 017625): Train loss 3.118, Val loss 4.058\n",
            "Ep 6 (Step 017630): Train loss 2.961, Val loss 4.068\n",
            "Ep 6 (Step 017635): Train loss 3.084, Val loss 4.069\n",
            "Ep 6 (Step 017640): Train loss 2.924, Val loss 4.082\n",
            "Ep 6 (Step 017645): Train loss 3.018, Val loss 4.071\n",
            "Ep 6 (Step 017650): Train loss 3.124, Val loss 4.047\n",
            "Ep 6 (Step 017655): Train loss 3.124, Val loss 4.039\n",
            "Ep 6 (Step 017660): Train loss 2.985, Val loss 4.040\n",
            "Ep 6 (Step 017665): Train loss 2.756, Val loss 4.051\n",
            "Ep 6 (Step 017670): Train loss 3.091, Val loss 4.053\n",
            "Ep 6 (Step 017675): Train loss 2.950, Val loss 4.051\n",
            "Ep 6 (Step 017680): Train loss 3.254, Val loss 4.050\n",
            "Ep 6 (Step 017685): Train loss 2.823, Val loss 4.051\n",
            "Ep 6 (Step 017690): Train loss 3.005, Val loss 4.058\n",
            "Ep 6 (Step 017695): Train loss 3.071, Val loss 4.040\n",
            "Ep 6 (Step 017700): Train loss 2.915, Val loss 4.034\n",
            "Ep 6 (Step 017705): Train loss 2.820, Val loss 4.044\n",
            "Ep 6 (Step 017710): Train loss 3.075, Val loss 4.054\n",
            "Ep 6 (Step 017715): Train loss 3.024, Val loss 4.044\n",
            "Ep 6 (Step 017720): Train loss 3.111, Val loss 4.039\n",
            "Ep 6 (Step 017725): Train loss 2.785, Val loss 4.050\n",
            "Ep 6 (Step 017730): Train loss 2.812, Val loss 4.051\n",
            "Ep 6 (Step 017735): Train loss 2.901, Val loss 4.053\n",
            "Ep 6 (Step 017740): Train loss 3.037, Val loss 4.052\n",
            "Ep 6 (Step 017745): Train loss 2.964, Val loss 4.042\n",
            "Ep 6 (Step 017750): Train loss 2.892, Val loss 4.044\n",
            "Ep 6 (Step 017755): Train loss 3.130, Val loss 4.063\n",
            "Ep 6 (Step 017760): Train loss 2.950, Val loss 4.068\n",
            "Ep 6 (Step 017765): Train loss 3.363, Val loss 4.060\n",
            "Ep 6 (Step 017770): Train loss 2.915, Val loss 4.041\n",
            "Ep 6 (Step 017775): Train loss 2.951, Val loss 4.040\n",
            "Ep 6 (Step 017780): Train loss 3.093, Val loss 4.041\n",
            "Ep 6 (Step 017785): Train loss 2.901, Val loss 4.037\n",
            "Ep 6 (Step 017790): Train loss 3.203, Val loss 4.024\n",
            "Ep 6 (Step 017795): Train loss 2.852, Val loss 4.027\n",
            "Ep 6 (Step 017800): Train loss 2.708, Val loss 4.034\n",
            "Ep 6 (Step 017805): Train loss 3.029, Val loss 4.032\n",
            "Ep 6 (Step 017810): Train loss 3.232, Val loss 4.039\n",
            "Ep 6 (Step 017815): Train loss 2.954, Val loss 4.047\n",
            "Ep 6 (Step 017820): Train loss 2.988, Val loss 4.043\n",
            "Ep 6 (Step 017825): Train loss 2.929, Val loss 4.019\n",
            "Ep 6 (Step 017830): Train loss 3.088, Val loss 4.008\n",
            "Ep 6 (Step 017835): Train loss 3.100, Val loss 4.002\n",
            "Ep 6 (Step 017840): Train loss 3.039, Val loss 3.998\n",
            "Ep 6 (Step 017845): Train loss 2.996, Val loss 4.007\n",
            "Ep 6 (Step 017850): Train loss 2.951, Val loss 4.008\n",
            "Ep 6 (Step 017855): Train loss 2.992, Val loss 4.006\n",
            "Ep 6 (Step 017860): Train loss 3.147, Val loss 4.001\n",
            "Every effort moves you, my lord, And I have not been in my heart And I have been my heart, and my heart I’ll be my father’s wife.  [_Exeunt._]  SCENE II\n",
            "Ep 7 (Step 017865): Train loss 2.799, Val loss 3.998\n",
            "Ep 7 (Step 017870): Train loss 3.125, Val loss 3.998\n",
            "Ep 7 (Step 017875): Train loss 3.148, Val loss 4.016\n",
            "Ep 7 (Step 017880): Train loss 3.159, Val loss 4.040\n",
            "Ep 7 (Step 017885): Train loss 3.235, Val loss 4.051\n",
            "Ep 7 (Step 017890): Train loss 2.928, Val loss 4.056\n",
            "Ep 7 (Step 017895): Train loss 3.014, Val loss 4.059\n",
            "Ep 7 (Step 017900): Train loss 3.124, Val loss 4.049\n",
            "Ep 7 (Step 017905): Train loss 3.167, Val loss 4.059\n",
            "Ep 7 (Step 017910): Train loss 2.636, Val loss 4.070\n",
            "Ep 7 (Step 017915): Train loss 3.054, Val loss 4.059\n",
            "Ep 7 (Step 017920): Train loss 3.079, Val loss 4.058\n",
            "Ep 7 (Step 017925): Train loss 2.978, Val loss 4.073\n",
            "Ep 7 (Step 017930): Train loss 3.005, Val loss 4.090\n",
            "Ep 7 (Step 017935): Train loss 3.004, Val loss 4.077\n",
            "Ep 7 (Step 017940): Train loss 2.648, Val loss 4.057\n",
            "Ep 7 (Step 017945): Train loss 2.783, Val loss 4.050\n",
            "Ep 7 (Step 017950): Train loss 3.217, Val loss 4.079\n",
            "Ep 7 (Step 017955): Train loss 3.039, Val loss 4.062\n",
            "Ep 7 (Step 017960): Train loss 3.054, Val loss 4.075\n",
            "Ep 7 (Step 017965): Train loss 2.936, Val loss 4.078\n",
            "Ep 7 (Step 017970): Train loss 3.262, Val loss 4.099\n",
            "Ep 7 (Step 017975): Train loss 3.126, Val loss 4.107\n",
            "Ep 7 (Step 017980): Train loss 2.936, Val loss 4.091\n",
            "Ep 7 (Step 017985): Train loss 2.926, Val loss 4.083\n",
            "Ep 7 (Step 017990): Train loss 3.076, Val loss 4.093\n",
            "Ep 7 (Step 017995): Train loss 2.875, Val loss 4.113\n",
            "Ep 7 (Step 018000): Train loss 3.158, Val loss 4.082\n",
            "Ep 7 (Step 018005): Train loss 2.907, Val loss 4.081\n",
            "Ep 7 (Step 018010): Train loss 3.122, Val loss 4.086\n",
            "Ep 7 (Step 018015): Train loss 3.021, Val loss 4.089\n",
            "Ep 7 (Step 018020): Train loss 3.160, Val loss 4.086\n",
            "Ep 7 (Step 018025): Train loss 2.899, Val loss 4.088\n",
            "Ep 7 (Step 018030): Train loss 2.998, Val loss 4.097\n",
            "Ep 7 (Step 018035): Train loss 2.708, Val loss 4.093\n",
            "Ep 7 (Step 018040): Train loss 3.144, Val loss 4.087\n",
            "Ep 7 (Step 018045): Train loss 2.829, Val loss 4.079\n",
            "Ep 7 (Step 018050): Train loss 2.870, Val loss 4.080\n",
            "Ep 7 (Step 018055): Train loss 2.940, Val loss 4.102\n",
            "Ep 7 (Step 018060): Train loss 2.569, Val loss 4.109\n",
            "Ep 7 (Step 018065): Train loss 3.050, Val loss 4.104\n",
            "Ep 7 (Step 018070): Train loss 2.592, Val loss 4.101\n",
            "Ep 7 (Step 018075): Train loss 2.839, Val loss 4.098\n",
            "Ep 7 (Step 018080): Train loss 2.905, Val loss 4.109\n",
            "Ep 7 (Step 018085): Train loss 2.763, Val loss 4.119\n",
            "Ep 7 (Step 018090): Train loss 2.983, Val loss 4.116\n",
            "Ep 7 (Step 018095): Train loss 3.067, Val loss 4.123\n",
            "Ep 7 (Step 018100): Train loss 3.249, Val loss 4.124\n",
            "Ep 7 (Step 018105): Train loss 3.042, Val loss 4.116\n",
            "Ep 7 (Step 018110): Train loss 2.924, Val loss 4.112\n",
            "Ep 7 (Step 018115): Train loss 2.980, Val loss 4.103\n",
            "Ep 7 (Step 018120): Train loss 3.087, Val loss 4.105\n",
            "Ep 7 (Step 018125): Train loss 2.971, Val loss 4.100\n",
            "Ep 7 (Step 018130): Train loss 3.121, Val loss 4.108\n",
            "Ep 7 (Step 018135): Train loss 3.097, Val loss 4.119\n",
            "Ep 7 (Step 018140): Train loss 3.056, Val loss 4.107\n",
            "Ep 7 (Step 018145): Train loss 2.696, Val loss 4.095\n",
            "Ep 7 (Step 018150): Train loss 3.025, Val loss 4.100\n",
            "Ep 7 (Step 018155): Train loss 2.973, Val loss 4.129\n",
            "Ep 7 (Step 018160): Train loss 3.007, Val loss 4.111\n",
            "Ep 7 (Step 018165): Train loss 2.917, Val loss 4.124\n",
            "Ep 7 (Step 018170): Train loss 3.184, Val loss 4.128\n",
            "Ep 7 (Step 018175): Train loss 3.292, Val loss 4.109\n",
            "Ep 7 (Step 018180): Train loss 2.967, Val loss 4.101\n",
            "Ep 7 (Step 018185): Train loss 3.090, Val loss 4.093\n",
            "Ep 7 (Step 018190): Train loss 2.992, Val loss 4.102\n",
            "Ep 7 (Step 018195): Train loss 2.795, Val loss 4.103\n",
            "Ep 7 (Step 018200): Train loss 2.737, Val loss 4.108\n",
            "Ep 7 (Step 018205): Train loss 2.754, Val loss 4.095\n",
            "Ep 7 (Step 018210): Train loss 2.706, Val loss 4.096\n",
            "Ep 7 (Step 018215): Train loss 2.773, Val loss 4.096\n",
            "Ep 7 (Step 018220): Train loss 3.085, Val loss 4.112\n",
            "Ep 7 (Step 018225): Train loss 2.971, Val loss 4.123\n",
            "Ep 7 (Step 018230): Train loss 2.849, Val loss 4.100\n",
            "Ep 7 (Step 018235): Train loss 2.860, Val loss 4.095\n",
            "Ep 7 (Step 018240): Train loss 3.019, Val loss 4.090\n",
            "Ep 7 (Step 018245): Train loss 2.540, Val loss 4.092\n",
            "Ep 7 (Step 018250): Train loss 3.016, Val loss 4.097\n",
            "Ep 7 (Step 018255): Train loss 2.579, Val loss 4.110\n",
            "Ep 7 (Step 018260): Train loss 2.896, Val loss 4.118\n",
            "Ep 7 (Step 018265): Train loss 2.677, Val loss 4.113\n",
            "Ep 7 (Step 018270): Train loss 3.009, Val loss 4.114\n",
            "Ep 7 (Step 018275): Train loss 2.844, Val loss 4.119\n",
            "Ep 7 (Step 018280): Train loss 2.964, Val loss 4.112\n",
            "Ep 7 (Step 018285): Train loss 2.963, Val loss 4.116\n",
            "Ep 7 (Step 018290): Train loss 3.059, Val loss 4.119\n",
            "Ep 7 (Step 018295): Train loss 2.669, Val loss 4.110\n",
            "Ep 7 (Step 018300): Train loss 2.988, Val loss 4.102\n",
            "Ep 7 (Step 018305): Train loss 3.099, Val loss 4.111\n",
            "Ep 7 (Step 018310): Train loss 3.143, Val loss 4.103\n",
            "Ep 7 (Step 018315): Train loss 2.913, Val loss 4.118\n",
            "Ep 7 (Step 018320): Train loss 2.796, Val loss 4.117\n",
            "Ep 7 (Step 018325): Train loss 2.969, Val loss 4.110\n",
            "Ep 7 (Step 018330): Train loss 2.815, Val loss 4.104\n",
            "Ep 7 (Step 018335): Train loss 2.860, Val loss 4.105\n",
            "Ep 7 (Step 018340): Train loss 3.184, Val loss 4.104\n",
            "Ep 7 (Step 018345): Train loss 3.117, Val loss 4.101\n",
            "Ep 7 (Step 018350): Train loss 3.086, Val loss 4.098\n",
            "Ep 7 (Step 018355): Train loss 3.124, Val loss 4.106\n",
            "Ep 7 (Step 018360): Train loss 3.019, Val loss 4.111\n",
            "Ep 7 (Step 018365): Train loss 2.728, Val loss 4.104\n",
            "Ep 7 (Step 018370): Train loss 3.018, Val loss 4.097\n",
            "Ep 7 (Step 018375): Train loss 2.917, Val loss 4.076\n",
            "Ep 7 (Step 018380): Train loss 3.124, Val loss 4.074\n",
            "Ep 7 (Step 018385): Train loss 2.955, Val loss 4.080\n",
            "Ep 7 (Step 018390): Train loss 3.423, Val loss 4.078\n",
            "Ep 7 (Step 018395): Train loss 3.123, Val loss 4.082\n",
            "Ep 7 (Step 018400): Train loss 2.997, Val loss 4.088\n",
            "Ep 7 (Step 018405): Train loss 3.034, Val loss 4.084\n",
            "Ep 7 (Step 018410): Train loss 3.096, Val loss 4.095\n",
            "Ep 7 (Step 018415): Train loss 2.708, Val loss 4.105\n",
            "Ep 7 (Step 018420): Train loss 2.921, Val loss 4.108\n",
            "Ep 7 (Step 018425): Train loss 2.669, Val loss 4.100\n",
            "Ep 7 (Step 018430): Train loss 2.917, Val loss 4.096\n",
            "Ep 7 (Step 018435): Train loss 2.813, Val loss 4.104\n",
            "Ep 7 (Step 018440): Train loss 2.750, Val loss 4.115\n",
            "Ep 7 (Step 018445): Train loss 2.868, Val loss 4.123\n",
            "Ep 7 (Step 018450): Train loss 3.006, Val loss 4.120\n",
            "Ep 7 (Step 018455): Train loss 3.032, Val loss 4.120\n",
            "Ep 7 (Step 018460): Train loss 2.695, Val loss 4.107\n",
            "Ep 7 (Step 018465): Train loss 3.039, Val loss 4.098\n",
            "Ep 7 (Step 018470): Train loss 3.021, Val loss 4.081\n",
            "Ep 7 (Step 018475): Train loss 3.014, Val loss 4.085\n",
            "Ep 7 (Step 018480): Train loss 3.219, Val loss 4.098\n",
            "Ep 7 (Step 018485): Train loss 2.694, Val loss 4.102\n",
            "Ep 7 (Step 018490): Train loss 2.960, Val loss 4.107\n",
            "Ep 7 (Step 018495): Train loss 3.035, Val loss 4.104\n",
            "Ep 7 (Step 018500): Train loss 2.786, Val loss 4.115\n",
            "Ep 7 (Step 018505): Train loss 3.105, Val loss 4.120\n",
            "Ep 7 (Step 018510): Train loss 3.010, Val loss 4.109\n",
            "Ep 7 (Step 018515): Train loss 2.785, Val loss 4.101\n",
            "Ep 7 (Step 018520): Train loss 2.975, Val loss 4.115\n",
            "Ep 7 (Step 018525): Train loss 3.053, Val loss 4.101\n",
            "Ep 7 (Step 018530): Train loss 3.205, Val loss 4.102\n",
            "Ep 7 (Step 018535): Train loss 2.984, Val loss 4.092\n",
            "Ep 7 (Step 018540): Train loss 3.084, Val loss 4.088\n",
            "Ep 7 (Step 018545): Train loss 2.787, Val loss 4.095\n",
            "Ep 7 (Step 018550): Train loss 2.799, Val loss 4.105\n",
            "Ep 7 (Step 018555): Train loss 3.250, Val loss 4.118\n",
            "Ep 7 (Step 018560): Train loss 3.257, Val loss 4.115\n",
            "Ep 7 (Step 018565): Train loss 2.801, Val loss 4.120\n",
            "Ep 7 (Step 018570): Train loss 2.817, Val loss 4.105\n",
            "Ep 7 (Step 018575): Train loss 2.904, Val loss 4.096\n",
            "Ep 7 (Step 018580): Train loss 2.732, Val loss 4.100\n",
            "Ep 7 (Step 018585): Train loss 3.111, Val loss 4.102\n",
            "Ep 7 (Step 018590): Train loss 2.751, Val loss 4.108\n",
            "Ep 7 (Step 018595): Train loss 3.042, Val loss 4.106\n",
            "Ep 7 (Step 018600): Train loss 2.781, Val loss 4.106\n",
            "Ep 7 (Step 018605): Train loss 2.953, Val loss 4.102\n",
            "Ep 7 (Step 018610): Train loss 3.076, Val loss 4.103\n",
            "Ep 7 (Step 018615): Train loss 3.092, Val loss 4.098\n",
            "Ep 7 (Step 018620): Train loss 3.170, Val loss 4.095\n",
            "Ep 7 (Step 018625): Train loss 3.143, Val loss 4.106\n",
            "Ep 7 (Step 018630): Train loss 2.812, Val loss 4.098\n",
            "Ep 7 (Step 018635): Train loss 2.731, Val loss 4.083\n",
            "Ep 7 (Step 018640): Train loss 3.043, Val loss 4.084\n",
            "Ep 7 (Step 018645): Train loss 2.811, Val loss 4.097\n",
            "Ep 7 (Step 018650): Train loss 3.120, Val loss 4.087\n",
            "Ep 7 (Step 018655): Train loss 2.774, Val loss 4.079\n",
            "Ep 7 (Step 018660): Train loss 2.771, Val loss 4.093\n",
            "Ep 7 (Step 018665): Train loss 3.088, Val loss 4.099\n",
            "Ep 7 (Step 018670): Train loss 2.815, Val loss 4.091\n",
            "Ep 7 (Step 018675): Train loss 2.825, Val loss 4.098\n",
            "Ep 7 (Step 018680): Train loss 3.007, Val loss 4.108\n",
            "Ep 7 (Step 018685): Train loss 2.995, Val loss 4.104\n",
            "Ep 7 (Step 018690): Train loss 2.823, Val loss 4.095\n",
            "Ep 7 (Step 018695): Train loss 2.830, Val loss 4.093\n",
            "Ep 7 (Step 018700): Train loss 3.010, Val loss 4.088\n",
            "Ep 7 (Step 018705): Train loss 2.866, Val loss 4.086\n",
            "Ep 7 (Step 018710): Train loss 2.983, Val loss 4.080\n",
            "Ep 7 (Step 018715): Train loss 2.932, Val loss 4.088\n",
            "Ep 7 (Step 018720): Train loss 3.119, Val loss 4.109\n",
            "Ep 7 (Step 018725): Train loss 2.954, Val loss 4.111\n",
            "Ep 7 (Step 018730): Train loss 3.058, Val loss 4.104\n",
            "Ep 7 (Step 018735): Train loss 3.026, Val loss 4.104\n",
            "Ep 7 (Step 018740): Train loss 2.942, Val loss 4.115\n",
            "Ep 7 (Step 018745): Train loss 2.885, Val loss 4.125\n",
            "Ep 7 (Step 018750): Train loss 2.858, Val loss 4.118\n",
            "Ep 7 (Step 018755): Train loss 2.700, Val loss 4.115\n",
            "Ep 7 (Step 018760): Train loss 2.754, Val loss 4.113\n",
            "Ep 7 (Step 018765): Train loss 2.930, Val loss 4.107\n",
            "Ep 7 (Step 018770): Train loss 3.037, Val loss 4.107\n",
            "Ep 7 (Step 018775): Train loss 2.911, Val loss 4.098\n",
            "Ep 7 (Step 018780): Train loss 3.079, Val loss 4.092\n",
            "Ep 7 (Step 018785): Train loss 3.071, Val loss 4.095\n",
            "Ep 7 (Step 018790): Train loss 3.028, Val loss 4.092\n",
            "Ep 7 (Step 018795): Train loss 3.140, Val loss 4.092\n",
            "Ep 7 (Step 018800): Train loss 3.091, Val loss 4.090\n",
            "Ep 7 (Step 018805): Train loss 3.018, Val loss 4.092\n",
            "Ep 7 (Step 018810): Train loss 2.900, Val loss 4.098\n",
            "Ep 7 (Step 018815): Train loss 2.986, Val loss 4.099\n",
            "Ep 7 (Step 018820): Train loss 2.817, Val loss 4.082\n",
            "Ep 7 (Step 018825): Train loss 2.806, Val loss 4.069\n",
            "Ep 7 (Step 018830): Train loss 3.099, Val loss 4.065\n",
            "Ep 7 (Step 018835): Train loss 2.774, Val loss 4.070\n",
            "Ep 7 (Step 018840): Train loss 3.147, Val loss 4.064\n",
            "Ep 7 (Step 018845): Train loss 2.898, Val loss 4.076\n",
            "Ep 7 (Step 018850): Train loss 2.710, Val loss 4.100\n",
            "Ep 7 (Step 018855): Train loss 2.861, Val loss 4.082\n",
            "Ep 7 (Step 018860): Train loss 2.901, Val loss 4.083\n",
            "Ep 7 (Step 018865): Train loss 2.583, Val loss 4.089\n",
            "Ep 7 (Step 018870): Train loss 3.136, Val loss 4.109\n",
            "Ep 7 (Step 018875): Train loss 2.998, Val loss 4.118\n",
            "Ep 7 (Step 018880): Train loss 2.893, Val loss 4.109\n",
            "Ep 7 (Step 018885): Train loss 2.968, Val loss 4.096\n",
            "Ep 7 (Step 018890): Train loss 2.868, Val loss 4.086\n",
            "Ep 7 (Step 018895): Train loss 3.093, Val loss 4.091\n",
            "Ep 7 (Step 018900): Train loss 2.967, Val loss 4.102\n",
            "Ep 7 (Step 018905): Train loss 3.249, Val loss 4.113\n",
            "Ep 7 (Step 018910): Train loss 2.889, Val loss 4.100\n",
            "Ep 7 (Step 018915): Train loss 2.978, Val loss 4.099\n",
            "Ep 7 (Step 018920): Train loss 2.782, Val loss 4.106\n",
            "Ep 7 (Step 018925): Train loss 2.969, Val loss 4.093\n",
            "Ep 7 (Step 018930): Train loss 2.836, Val loss 4.102\n",
            "Ep 7 (Step 018935): Train loss 2.671, Val loss 4.105\n",
            "Ep 7 (Step 018940): Train loss 2.733, Val loss 4.099\n",
            "Ep 7 (Step 018945): Train loss 2.869, Val loss 4.111\n",
            "Ep 7 (Step 018950): Train loss 3.260, Val loss 4.122\n",
            "Ep 7 (Step 018955): Train loss 2.908, Val loss 4.112\n",
            "Ep 7 (Step 018960): Train loss 3.100, Val loss 4.098\n",
            "Ep 7 (Step 018965): Train loss 3.123, Val loss 4.107\n",
            "Ep 7 (Step 018970): Train loss 2.783, Val loss 4.108\n",
            "Ep 7 (Step 018975): Train loss 2.710, Val loss 4.102\n",
            "Ep 7 (Step 018980): Train loss 3.112, Val loss 4.092\n",
            "Ep 7 (Step 018985): Train loss 2.750, Val loss 4.091\n",
            "Ep 7 (Step 018990): Train loss 3.143, Val loss 4.108\n",
            "Ep 7 (Step 018995): Train loss 3.093, Val loss 4.119\n",
            "Ep 7 (Step 019000): Train loss 2.892, Val loss 4.097\n",
            "Ep 7 (Step 019005): Train loss 3.115, Val loss 4.081\n",
            "Ep 7 (Step 019010): Train loss 2.918, Val loss 4.084\n",
            "Ep 7 (Step 019015): Train loss 2.784, Val loss 4.097\n",
            "Ep 7 (Step 019020): Train loss 2.901, Val loss 4.102\n",
            "Ep 7 (Step 019025): Train loss 2.992, Val loss 4.099\n",
            "Ep 7 (Step 019030): Train loss 3.073, Val loss 4.107\n",
            "Ep 7 (Step 019035): Train loss 2.960, Val loss 4.105\n",
            "Ep 7 (Step 019040): Train loss 3.220, Val loss 4.101\n",
            "Ep 7 (Step 019045): Train loss 2.780, Val loss 4.088\n",
            "Ep 7 (Step 019050): Train loss 2.954, Val loss 4.087\n",
            "Ep 7 (Step 019055): Train loss 2.863, Val loss 4.080\n",
            "Ep 7 (Step 019060): Train loss 3.239, Val loss 4.086\n",
            "Ep 7 (Step 019065): Train loss 2.641, Val loss 4.093\n",
            "Ep 7 (Step 019070): Train loss 2.806, Val loss 4.100\n",
            "Ep 7 (Step 019075): Train loss 2.972, Val loss 4.099\n",
            "Ep 7 (Step 019080): Train loss 2.871, Val loss 4.111\n",
            "Ep 7 (Step 019085): Train loss 2.975, Val loss 4.130\n",
            "Ep 7 (Step 019090): Train loss 2.864, Val loss 4.111\n",
            "Ep 7 (Step 019095): Train loss 2.795, Val loss 4.107\n",
            "Ep 7 (Step 019100): Train loss 2.796, Val loss 4.103\n",
            "Ep 7 (Step 019105): Train loss 2.790, Val loss 4.108\n",
            "Ep 7 (Step 019110): Train loss 3.181, Val loss 4.124\n",
            "Ep 7 (Step 019115): Train loss 2.829, Val loss 4.133\n",
            "Ep 7 (Step 019120): Train loss 2.867, Val loss 4.127\n",
            "Ep 7 (Step 019125): Train loss 2.803, Val loss 4.106\n",
            "Ep 7 (Step 019130): Train loss 2.754, Val loss 4.103\n",
            "Ep 7 (Step 019135): Train loss 2.738, Val loss 4.101\n",
            "Ep 7 (Step 019140): Train loss 2.876, Val loss 4.103\n",
            "Ep 7 (Step 019145): Train loss 2.684, Val loss 4.099\n",
            "Ep 7 (Step 019150): Train loss 3.003, Val loss 4.085\n",
            "Ep 7 (Step 019155): Train loss 3.004, Val loss 4.081\n",
            "Ep 7 (Step 019160): Train loss 2.958, Val loss 4.086\n",
            "Ep 7 (Step 019165): Train loss 2.767, Val loss 4.094\n",
            "Ep 7 (Step 019170): Train loss 3.214, Val loss 4.090\n",
            "Ep 7 (Step 019175): Train loss 2.611, Val loss 4.093\n",
            "Ep 7 (Step 019180): Train loss 2.874, Val loss 4.096\n",
            "Ep 7 (Step 019185): Train loss 2.680, Val loss 4.095\n",
            "Ep 7 (Step 019190): Train loss 2.990, Val loss 4.099\n",
            "Ep 7 (Step 019195): Train loss 2.931, Val loss 4.108\n",
            "Ep 7 (Step 019200): Train loss 2.578, Val loss 4.118\n",
            "Ep 7 (Step 019205): Train loss 2.729, Val loss 4.108\n",
            "Ep 7 (Step 019210): Train loss 3.153, Val loss 4.100\n",
            "Ep 7 (Step 019215): Train loss 3.036, Val loss 4.100\n",
            "Ep 7 (Step 019220): Train loss 2.970, Val loss 4.109\n",
            "Ep 7 (Step 019225): Train loss 2.785, Val loss 4.118\n",
            "Ep 7 (Step 019230): Train loss 2.763, Val loss 4.126\n",
            "Ep 7 (Step 019235): Train loss 2.931, Val loss 4.109\n",
            "Ep 7 (Step 019240): Train loss 2.771, Val loss 4.096\n",
            "Ep 7 (Step 019245): Train loss 2.995, Val loss 4.086\n",
            "Ep 7 (Step 019250): Train loss 2.819, Val loss 4.093\n",
            "Ep 7 (Step 019255): Train loss 2.694, Val loss 4.103\n",
            "Ep 7 (Step 019260): Train loss 2.873, Val loss 4.117\n",
            "Ep 7 (Step 019265): Train loss 3.169, Val loss 4.131\n",
            "Ep 7 (Step 019270): Train loss 3.073, Val loss 4.128\n",
            "Ep 7 (Step 019275): Train loss 2.780, Val loss 4.119\n",
            "Ep 7 (Step 019280): Train loss 2.732, Val loss 4.112\n",
            "Ep 7 (Step 019285): Train loss 2.983, Val loss 4.108\n",
            "Ep 7 (Step 019290): Train loss 2.818, Val loss 4.093\n",
            "Ep 7 (Step 019295): Train loss 3.040, Val loss 4.094\n",
            "Ep 7 (Step 019300): Train loss 2.833, Val loss 4.091\n",
            "Ep 7 (Step 019305): Train loss 3.144, Val loss 4.105\n",
            "Ep 7 (Step 019310): Train loss 3.088, Val loss 4.109\n",
            "Ep 7 (Step 019315): Train loss 3.071, Val loss 4.107\n",
            "Ep 7 (Step 019320): Train loss 2.651, Val loss 4.104\n",
            "Ep 7 (Step 019325): Train loss 3.149, Val loss 4.109\n",
            "Ep 7 (Step 019330): Train loss 2.787, Val loss 4.121\n",
            "Ep 7 (Step 019335): Train loss 2.719, Val loss 4.112\n",
            "Ep 7 (Step 019340): Train loss 2.995, Val loss 4.105\n",
            "Ep 7 (Step 019345): Train loss 2.760, Val loss 4.107\n",
            "Ep 7 (Step 019350): Train loss 3.089, Val loss 4.114\n",
            "Ep 7 (Step 019355): Train loss 2.813, Val loss 4.111\n",
            "Ep 7 (Step 019360): Train loss 2.943, Val loss 4.102\n",
            "Ep 7 (Step 019365): Train loss 3.114, Val loss 4.086\n",
            "Ep 7 (Step 019370): Train loss 3.042, Val loss 4.084\n",
            "Ep 7 (Step 019375): Train loss 3.091, Val loss 4.087\n",
            "Ep 7 (Step 019380): Train loss 3.243, Val loss 4.082\n",
            "Ep 7 (Step 019385): Train loss 2.808, Val loss 4.088\n",
            "Ep 7 (Step 019390): Train loss 3.268, Val loss 4.094\n",
            "Ep 7 (Step 019395): Train loss 2.966, Val loss 4.105\n",
            "Ep 7 (Step 019400): Train loss 2.692, Val loss 4.114\n",
            "Ep 7 (Step 019405): Train loss 2.931, Val loss 4.100\n",
            "Ep 7 (Step 019410): Train loss 3.060, Val loss 4.099\n",
            "Ep 7 (Step 019415): Train loss 2.840, Val loss 4.092\n",
            "Ep 7 (Step 019420): Train loss 2.679, Val loss 4.087\n",
            "Ep 7 (Step 019425): Train loss 2.918, Val loss 4.089\n",
            "Ep 7 (Step 019430): Train loss 3.063, Val loss 4.079\n",
            "Ep 7 (Step 019435): Train loss 2.877, Val loss 4.087\n",
            "Ep 7 (Step 019440): Train loss 2.942, Val loss 4.075\n",
            "Ep 7 (Step 019445): Train loss 2.790, Val loss 4.062\n",
            "Ep 7 (Step 019450): Train loss 2.983, Val loss 4.067\n",
            "Ep 7 (Step 019455): Train loss 2.908, Val loss 4.071\n",
            "Ep 7 (Step 019460): Train loss 3.152, Val loss 4.072\n",
            "Ep 7 (Step 019465): Train loss 3.134, Val loss 4.066\n",
            "Ep 7 (Step 019470): Train loss 2.968, Val loss 4.065\n",
            "Ep 7 (Step 019475): Train loss 2.844, Val loss 4.056\n",
            "Ep 7 (Step 019480): Train loss 2.927, Val loss 4.055\n",
            "Ep 7 (Step 019485): Train loss 3.031, Val loss 4.064\n",
            "Ep 7 (Step 019490): Train loss 3.046, Val loss 4.075\n",
            "Ep 7 (Step 019495): Train loss 3.049, Val loss 4.074\n",
            "Ep 7 (Step 019500): Train loss 2.658, Val loss 4.067\n",
            "Ep 7 (Step 019505): Train loss 2.821, Val loss 4.057\n",
            "Ep 7 (Step 019510): Train loss 2.751, Val loss 4.059\n",
            "Ep 7 (Step 019515): Train loss 2.883, Val loss 4.076\n",
            "Ep 7 (Step 019520): Train loss 2.764, Val loss 4.086\n",
            "Ep 7 (Step 019525): Train loss 2.770, Val loss 4.078\n",
            "Ep 7 (Step 019530): Train loss 3.015, Val loss 4.068\n",
            "Ep 7 (Step 019535): Train loss 2.648, Val loss 4.070\n",
            "Ep 7 (Step 019540): Train loss 3.199, Val loss 4.071\n",
            "Ep 7 (Step 019545): Train loss 2.755, Val loss 4.078\n",
            "Ep 7 (Step 019550): Train loss 2.978, Val loss 4.082\n",
            "Ep 7 (Step 019555): Train loss 2.781, Val loss 4.086\n",
            "Ep 7 (Step 019560): Train loss 3.002, Val loss 4.076\n",
            "Ep 7 (Step 019565): Train loss 2.942, Val loss 4.075\n",
            "Ep 7 (Step 019570): Train loss 2.921, Val loss 4.078\n",
            "Ep 7 (Step 019575): Train loss 2.712, Val loss 4.073\n",
            "Ep 7 (Step 019580): Train loss 2.849, Val loss 4.068\n",
            "Ep 7 (Step 019585): Train loss 2.934, Val loss 4.058\n",
            "Ep 7 (Step 019590): Train loss 2.633, Val loss 4.061\n",
            "Ep 7 (Step 019595): Train loss 2.739, Val loss 4.069\n",
            "Ep 7 (Step 019600): Train loss 2.920, Val loss 4.083\n",
            "Ep 7 (Step 019605): Train loss 2.801, Val loss 4.081\n",
            "Ep 7 (Step 019610): Train loss 2.729, Val loss 4.076\n",
            "Ep 7 (Step 019615): Train loss 2.763, Val loss 4.073\n",
            "Ep 7 (Step 019620): Train loss 2.985, Val loss 4.079\n",
            "Ep 7 (Step 019625): Train loss 2.940, Val loss 4.070\n",
            "Ep 7 (Step 019630): Train loss 2.593, Val loss 4.060\n",
            "Ep 7 (Step 019635): Train loss 2.669, Val loss 4.049\n",
            "Ep 7 (Step 019640): Train loss 2.983, Val loss 4.043\n",
            "Ep 7 (Step 019645): Train loss 2.937, Val loss 4.060\n",
            "Ep 7 (Step 019650): Train loss 2.608, Val loss 4.050\n",
            "Ep 7 (Step 019655): Train loss 2.910, Val loss 4.038\n",
            "Ep 7 (Step 019660): Train loss 2.919, Val loss 4.049\n",
            "Ep 7 (Step 019665): Train loss 2.871, Val loss 4.062\n",
            "Ep 7 (Step 019670): Train loss 2.741, Val loss 4.074\n",
            "Ep 7 (Step 019675): Train loss 3.099, Val loss 4.086\n",
            "Ep 7 (Step 019680): Train loss 2.754, Val loss 4.078\n",
            "Ep 7 (Step 019685): Train loss 2.824, Val loss 4.076\n",
            "Ep 7 (Step 019690): Train loss 3.019, Val loss 4.070\n",
            "Ep 7 (Step 019695): Train loss 2.952, Val loss 4.067\n",
            "Ep 7 (Step 019700): Train loss 2.965, Val loss 4.073\n",
            "Ep 7 (Step 019705): Train loss 2.804, Val loss 4.070\n",
            "Ep 7 (Step 019710): Train loss 2.819, Val loss 4.075\n",
            "Ep 7 (Step 019715): Train loss 2.671, Val loss 4.079\n",
            "Ep 7 (Step 019720): Train loss 2.951, Val loss 4.069\n",
            "Ep 7 (Step 019725): Train loss 2.899, Val loss 4.065\n",
            "Ep 7 (Step 019730): Train loss 3.032, Val loss 4.073\n",
            "Ep 7 (Step 019735): Train loss 2.601, Val loss 4.071\n",
            "Ep 7 (Step 019740): Train loss 2.964, Val loss 4.076\n",
            "Ep 7 (Step 019745): Train loss 2.743, Val loss 4.079\n",
            "Ep 7 (Step 019750): Train loss 3.118, Val loss 4.086\n",
            "Ep 7 (Step 019755): Train loss 2.642, Val loss 4.076\n",
            "Ep 7 (Step 019760): Train loss 2.737, Val loss 4.069\n",
            "Ep 7 (Step 019765): Train loss 2.723, Val loss 4.068\n",
            "Ep 7 (Step 019770): Train loss 3.138, Val loss 4.082\n",
            "Ep 7 (Step 019775): Train loss 3.137, Val loss 4.087\n",
            "Ep 7 (Step 019780): Train loss 2.932, Val loss 4.084\n",
            "Ep 7 (Step 019785): Train loss 2.923, Val loss 4.073\n",
            "Ep 7 (Step 019790): Train loss 3.001, Val loss 4.060\n",
            "Ep 7 (Step 019795): Train loss 2.877, Val loss 4.054\n",
            "Ep 7 (Step 019800): Train loss 2.883, Val loss 4.056\n",
            "Ep 7 (Step 019805): Train loss 2.959, Val loss 4.054\n",
            "Ep 7 (Step 019810): Train loss 2.906, Val loss 4.049\n",
            "Ep 7 (Step 019815): Train loss 2.888, Val loss 4.049\n",
            "Ep 7 (Step 019820): Train loss 3.087, Val loss 4.061\n",
            "Ep 7 (Step 019825): Train loss 2.824, Val loss 4.053\n",
            "Ep 7 (Step 019830): Train loss 2.772, Val loss 4.044\n",
            "Ep 7 (Step 019835): Train loss 3.079, Val loss 4.053\n",
            "Ep 7 (Step 019840): Train loss 2.635, Val loss 4.050\n",
            "Ep 7 (Step 019845): Train loss 2.642, Val loss 4.043\n",
            "Ep 7 (Step 019850): Train loss 2.941, Val loss 4.040\n",
            "Ep 7 (Step 019855): Train loss 3.011, Val loss 4.055\n",
            "Ep 7 (Step 019860): Train loss 3.057, Val loss 4.065\n",
            "Ep 7 (Step 019865): Train loss 2.830, Val loss 4.068\n",
            "Ep 7 (Step 019870): Train loss 2.833, Val loss 4.081\n",
            "Ep 7 (Step 019875): Train loss 2.602, Val loss 4.082\n",
            "Ep 7 (Step 019880): Train loss 2.597, Val loss 4.085\n",
            "Ep 7 (Step 019885): Train loss 2.785, Val loss 4.084\n",
            "Ep 7 (Step 019890): Train loss 2.683, Val loss 4.079\n",
            "Ep 7 (Step 019895): Train loss 2.922, Val loss 4.081\n",
            "Ep 7 (Step 019900): Train loss 2.986, Val loss 4.075\n",
            "Ep 7 (Step 019905): Train loss 2.657, Val loss 4.069\n",
            "Ep 7 (Step 019910): Train loss 3.053, Val loss 4.074\n",
            "Ep 7 (Step 019915): Train loss 3.069, Val loss 4.083\n",
            "Ep 7 (Step 019920): Train loss 2.935, Val loss 4.075\n",
            "Ep 7 (Step 019925): Train loss 3.026, Val loss 4.073\n",
            "Ep 7 (Step 019930): Train loss 2.786, Val loss 4.093\n",
            "Ep 7 (Step 019935): Train loss 2.787, Val loss 4.103\n",
            "Ep 7 (Step 019940): Train loss 2.868, Val loss 4.063\n",
            "Ep 7 (Step 019945): Train loss 2.833, Val loss 4.057\n",
            "Ep 7 (Step 019950): Train loss 2.477, Val loss 4.068\n",
            "Ep 7 (Step 019955): Train loss 2.824, Val loss 4.056\n",
            "Ep 7 (Step 019960): Train loss 2.835, Val loss 4.059\n",
            "Ep 7 (Step 019965): Train loss 2.916, Val loss 4.065\n",
            "Ep 7 (Step 019970): Train loss 2.908, Val loss 4.043\n",
            "Ep 7 (Step 019975): Train loss 3.261, Val loss 4.038\n",
            "Ep 7 (Step 019980): Train loss 3.080, Val loss 4.043\n",
            "Ep 7 (Step 019985): Train loss 2.832, Val loss 4.051\n",
            "Ep 7 (Step 019990): Train loss 2.502, Val loss 4.053\n",
            "Ep 7 (Step 019995): Train loss 2.793, Val loss 4.050\n",
            "Ep 7 (Step 020000): Train loss 2.974, Val loss 4.055\n",
            "Ep 7 (Step 020005): Train loss 2.963, Val loss 4.065\n",
            "Ep 7 (Step 020010): Train loss 2.595, Val loss 4.065\n",
            "Ep 7 (Step 020015): Train loss 2.966, Val loss 4.083\n",
            "Ep 7 (Step 020020): Train loss 2.811, Val loss 4.083\n",
            "Ep 7 (Step 020025): Train loss 2.794, Val loss 4.065\n",
            "Ep 7 (Step 020030): Train loss 2.850, Val loss 4.054\n",
            "Ep 7 (Step 020035): Train loss 2.853, Val loss 4.059\n",
            "Ep 7 (Step 020040): Train loss 2.493, Val loss 4.058\n",
            "Ep 7 (Step 020045): Train loss 2.742, Val loss 4.067\n",
            "Ep 7 (Step 020050): Train loss 2.848, Val loss 4.077\n",
            "Ep 7 (Step 020055): Train loss 3.098, Val loss 4.070\n",
            "Ep 7 (Step 020060): Train loss 2.648, Val loss 4.049\n",
            "Ep 7 (Step 020065): Train loss 2.807, Val loss 4.053\n",
            "Ep 7 (Step 020070): Train loss 2.881, Val loss 4.074\n",
            "Ep 7 (Step 020075): Train loss 2.797, Val loss 4.081\n",
            "Ep 7 (Step 020080): Train loss 2.928, Val loss 4.063\n",
            "Ep 7 (Step 020085): Train loss 3.006, Val loss 4.059\n",
            "Ep 7 (Step 020090): Train loss 2.950, Val loss 4.073\n",
            "Ep 7 (Step 020095): Train loss 2.614, Val loss 4.054\n",
            "Ep 7 (Step 020100): Train loss 2.642, Val loss 4.044\n",
            "Ep 7 (Step 020105): Train loss 2.780, Val loss 4.062\n",
            "Ep 7 (Step 020110): Train loss 2.858, Val loss 4.047\n",
            "Ep 7 (Step 020115): Train loss 2.651, Val loss 4.041\n",
            "Ep 7 (Step 020120): Train loss 2.818, Val loss 4.031\n",
            "Ep 7 (Step 020125): Train loss 2.775, Val loss 4.037\n",
            "Ep 7 (Step 020130): Train loss 2.851, Val loss 4.027\n",
            "Ep 7 (Step 020135): Train loss 2.784, Val loss 4.019\n",
            "Ep 7 (Step 020140): Train loss 2.957, Val loss 4.021\n",
            "Ep 7 (Step 020145): Train loss 2.756, Val loss 4.033\n",
            "Ep 7 (Step 020150): Train loss 2.719, Val loss 4.038\n",
            "Ep 7 (Step 020155): Train loss 2.913, Val loss 4.039\n",
            "Ep 7 (Step 020160): Train loss 2.897, Val loss 4.044\n",
            "Ep 7 (Step 020165): Train loss 2.736, Val loss 4.047\n",
            "Ep 7 (Step 020170): Train loss 3.012, Val loss 4.043\n",
            "Ep 7 (Step 020175): Train loss 2.847, Val loss 4.035\n",
            "Ep 7 (Step 020180): Train loss 2.411, Val loss 4.041\n",
            "Ep 7 (Step 020185): Train loss 2.726, Val loss 4.035\n",
            "Ep 7 (Step 020190): Train loss 2.601, Val loss 4.029\n",
            "Ep 7 (Step 020195): Train loss 2.786, Val loss 4.032\n",
            "Ep 7 (Step 020200): Train loss 2.589, Val loss 4.028\n",
            "Ep 7 (Step 020205): Train loss 3.037, Val loss 4.020\n",
            "Ep 7 (Step 020210): Train loss 2.929, Val loss 4.014\n",
            "Ep 7 (Step 020215): Train loss 2.905, Val loss 4.013\n",
            "Ep 7 (Step 020220): Train loss 2.929, Val loss 4.029\n",
            "Ep 7 (Step 020225): Train loss 2.575, Val loss 4.032\n",
            "Ep 7 (Step 020230): Train loss 2.736, Val loss 4.030\n",
            "Ep 7 (Step 020235): Train loss 2.674, Val loss 4.017\n",
            "Ep 7 (Step 020240): Train loss 3.076, Val loss 4.007\n",
            "Ep 7 (Step 020245): Train loss 3.042, Val loss 4.010\n",
            "Ep 7 (Step 020250): Train loss 2.732, Val loss 4.020\n",
            "Ep 7 (Step 020255): Train loss 2.904, Val loss 4.025\n",
            "Ep 7 (Step 020260): Train loss 3.028, Val loss 4.030\n",
            "Ep 7 (Step 020265): Train loss 2.557, Val loss 4.039\n",
            "Ep 7 (Step 020270): Train loss 2.759, Val loss 4.042\n",
            "Ep 7 (Step 020275): Train loss 2.708, Val loss 4.050\n",
            "Ep 7 (Step 020280): Train loss 2.866, Val loss 4.059\n",
            "Ep 7 (Step 020285): Train loss 2.765, Val loss 4.067\n",
            "Ep 7 (Step 020290): Train loss 2.824, Val loss 4.054\n",
            "Ep 7 (Step 020295): Train loss 2.629, Val loss 4.043\n",
            "Ep 7 (Step 020300): Train loss 2.730, Val loss 4.042\n",
            "Ep 7 (Step 020305): Train loss 2.851, Val loss 4.054\n",
            "Ep 7 (Step 020310): Train loss 2.935, Val loss 4.051\n",
            "Ep 7 (Step 020315): Train loss 3.002, Val loss 4.050\n",
            "Ep 7 (Step 020320): Train loss 3.211, Val loss 4.052\n",
            "Ep 7 (Step 020325): Train loss 3.088, Val loss 4.045\n",
            "Ep 7 (Step 020330): Train loss 2.672, Val loss 4.041\n",
            "Ep 7 (Step 020335): Train loss 3.384, Val loss 4.055\n",
            "Ep 7 (Step 020340): Train loss 2.727, Val loss 4.065\n",
            "Ep 7 (Step 020345): Train loss 2.986, Val loss 4.067\n",
            "Ep 7 (Step 020350): Train loss 3.099, Val loss 4.060\n",
            "Ep 7 (Step 020355): Train loss 2.856, Val loss 4.063\n",
            "Ep 7 (Step 020360): Train loss 3.105, Val loss 4.057\n",
            "Ep 7 (Step 020365): Train loss 2.833, Val loss 4.059\n",
            "Ep 7 (Step 020370): Train loss 2.882, Val loss 4.053\n",
            "Ep 7 (Step 020375): Train loss 2.697, Val loss 4.051\n",
            "Ep 7 (Step 020380): Train loss 2.902, Val loss 4.049\n",
            "Ep 7 (Step 020385): Train loss 3.103, Val loss 4.049\n",
            "Ep 7 (Step 020390): Train loss 2.844, Val loss 4.048\n",
            "Ep 7 (Step 020395): Train loss 2.970, Val loss 4.049\n",
            "Ep 7 (Step 020400): Train loss 2.681, Val loss 4.053\n",
            "Ep 7 (Step 020405): Train loss 2.737, Val loss 4.062\n",
            "Ep 7 (Step 020410): Train loss 2.789, Val loss 4.065\n",
            "Ep 7 (Step 020415): Train loss 3.029, Val loss 4.059\n",
            "Ep 7 (Step 020420): Train loss 2.924, Val loss 4.054\n",
            "Ep 7 (Step 020425): Train loss 2.797, Val loss 4.057\n",
            "Ep 7 (Step 020430): Train loss 2.907, Val loss 4.061\n",
            "Ep 7 (Step 020435): Train loss 2.887, Val loss 4.054\n",
            "Ep 7 (Step 020440): Train loss 2.876, Val loss 4.044\n",
            "Ep 7 (Step 020445): Train loss 2.872, Val loss 4.044\n",
            "Ep 7 (Step 020450): Train loss 2.807, Val loss 4.049\n",
            "Ep 7 (Step 020455): Train loss 2.727, Val loss 4.057\n",
            "Ep 7 (Step 020460): Train loss 2.775, Val loss 4.062\n",
            "Ep 7 (Step 020465): Train loss 2.694, Val loss 4.058\n",
            "Ep 7 (Step 020470): Train loss 3.034, Val loss 4.037\n",
            "Ep 7 (Step 020475): Train loss 3.000, Val loss 4.032\n",
            "Ep 7 (Step 020480): Train loss 2.691, Val loss 4.043\n",
            "Ep 7 (Step 020485): Train loss 2.828, Val loss 4.033\n",
            "Ep 7 (Step 020490): Train loss 2.722, Val loss 4.020\n",
            "Ep 7 (Step 020495): Train loss 2.576, Val loss 4.025\n",
            "Ep 7 (Step 020500): Train loss 2.918, Val loss 4.031\n",
            "Ep 7 (Step 020505): Train loss 2.855, Val loss 4.029\n",
            "Ep 7 (Step 020510): Train loss 2.808, Val loss 4.033\n",
            "Ep 7 (Step 020515): Train loss 2.637, Val loss 4.033\n",
            "Ep 7 (Step 020520): Train loss 2.801, Val loss 4.027\n",
            "Ep 7 (Step 020525): Train loss 2.928, Val loss 4.030\n",
            "Ep 7 (Step 020530): Train loss 2.716, Val loss 4.027\n",
            "Ep 7 (Step 020535): Train loss 2.776, Val loss 4.036\n",
            "Ep 7 (Step 020540): Train loss 2.846, Val loss 4.042\n",
            "Ep 7 (Step 020545): Train loss 2.818, Val loss 4.048\n",
            "Ep 7 (Step 020550): Train loss 2.753, Val loss 4.028\n",
            "Ep 7 (Step 020555): Train loss 2.610, Val loss 4.038\n",
            "Ep 7 (Step 020560): Train loss 2.733, Val loss 4.057\n",
            "Ep 7 (Step 020565): Train loss 2.799, Val loss 4.055\n",
            "Ep 7 (Step 020570): Train loss 2.790, Val loss 4.041\n",
            "Ep 7 (Step 020575): Train loss 2.766, Val loss 4.040\n",
            "Ep 7 (Step 020580): Train loss 2.619, Val loss 4.042\n",
            "Ep 7 (Step 020585): Train loss 2.682, Val loss 4.030\n",
            "Ep 7 (Step 020590): Train loss 2.850, Val loss 4.030\n",
            "Ep 7 (Step 020595): Train loss 2.917, Val loss 4.032\n",
            "Ep 7 (Step 020600): Train loss 2.665, Val loss 4.033\n",
            "Ep 7 (Step 020605): Train loss 2.866, Val loss 4.028\n",
            "Ep 7 (Step 020610): Train loss 2.745, Val loss 4.033\n",
            "Ep 7 (Step 020615): Train loss 2.790, Val loss 4.038\n",
            "Ep 7 (Step 020620): Train loss 2.847, Val loss 4.042\n",
            "Ep 7 (Step 020625): Train loss 3.030, Val loss 4.029\n",
            "Ep 7 (Step 020630): Train loss 2.557, Val loss 4.037\n",
            "Ep 7 (Step 020635): Train loss 2.881, Val loss 4.038\n",
            "Ep 7 (Step 020640): Train loss 2.611, Val loss 4.042\n",
            "Ep 7 (Step 020645): Train loss 2.787, Val loss 4.042\n",
            "Ep 7 (Step 020650): Train loss 3.013, Val loss 4.026\n",
            "Ep 7 (Step 020655): Train loss 2.669, Val loss 4.018\n",
            "Ep 7 (Step 020660): Train loss 2.911, Val loss 4.028\n",
            "Ep 7 (Step 020665): Train loss 2.915, Val loss 4.035\n",
            "Ep 7 (Step 020670): Train loss 2.781, Val loss 4.031\n",
            "Ep 7 (Step 020675): Train loss 2.539, Val loss 4.032\n",
            "Ep 7 (Step 020680): Train loss 2.832, Val loss 4.038\n",
            "Ep 7 (Step 020685): Train loss 2.605, Val loss 4.035\n",
            "Ep 7 (Step 020690): Train loss 2.562, Val loss 4.049\n",
            "Ep 7 (Step 020695): Train loss 3.145, Val loss 4.052\n",
            "Ep 7 (Step 020700): Train loss 2.795, Val loss 4.051\n",
            "Ep 7 (Step 020705): Train loss 2.910, Val loss 4.042\n",
            "Ep 7 (Step 020710): Train loss 2.740, Val loss 4.037\n",
            "Ep 7 (Step 020715): Train loss 2.892, Val loss 4.036\n",
            "Ep 7 (Step 020720): Train loss 2.983, Val loss 4.034\n",
            "Ep 7 (Step 020725): Train loss 2.679, Val loss 4.035\n",
            "Ep 7 (Step 020730): Train loss 2.807, Val loss 4.041\n",
            "Ep 7 (Step 020735): Train loss 2.810, Val loss 4.036\n",
            "Ep 7 (Step 020740): Train loss 2.639, Val loss 4.038\n",
            "Ep 7 (Step 020745): Train loss 2.617, Val loss 4.029\n",
            "Ep 7 (Step 020750): Train loss 2.589, Val loss 4.029\n",
            "Ep 7 (Step 020755): Train loss 3.043, Val loss 4.030\n",
            "Ep 7 (Step 020760): Train loss 2.877, Val loss 4.031\n",
            "Ep 7 (Step 020765): Train loss 2.956, Val loss 4.040\n",
            "Ep 7 (Step 020770): Train loss 2.811, Val loss 4.027\n",
            "Ep 7 (Step 020775): Train loss 2.731, Val loss 4.027\n",
            "Ep 7 (Step 020780): Train loss 2.813, Val loss 4.030\n",
            "Ep 7 (Step 020785): Train loss 2.798, Val loss 4.038\n",
            "Ep 7 (Step 020790): Train loss 2.630, Val loss 4.035\n",
            "Ep 7 (Step 020795): Train loss 2.868, Val loss 4.035\n",
            "Ep 7 (Step 020800): Train loss 2.853, Val loss 4.039\n",
            "Ep 7 (Step 020805): Train loss 2.932, Val loss 4.040\n",
            "Ep 7 (Step 020810): Train loss 2.681, Val loss 4.044\n",
            "Ep 7 (Step 020815): Train loss 3.020, Val loss 4.050\n",
            "Ep 7 (Step 020820): Train loss 2.662, Val loss 4.048\n",
            "Ep 7 (Step 020825): Train loss 2.811, Val loss 4.051\n",
            "Ep 7 (Step 020830): Train loss 2.860, Val loss 4.049\n",
            "Ep 7 (Step 020835): Train loss 2.681, Val loss 4.057\n",
            "Every effort moves you, sir, And you shall have your cause to your good.  [_Exeunt._]  SCENE III. The same. A Room in the Palace  Enter Orlando and Touchstone and Audrey.  CEL\n",
            "Ep 8 (Step 020840): Train loss 2.984, Val loss 4.061\n",
            "Ep 8 (Step 020845): Train loss 2.758, Val loss 4.070\n",
            "Ep 8 (Step 020850): Train loss 2.697, Val loss 4.062\n",
            "Ep 8 (Step 020855): Train loss 2.634, Val loss 4.059\n",
            "Ep 8 (Step 020860): Train loss 2.824, Val loss 4.053\n",
            "Ep 8 (Step 020865): Train loss 2.724, Val loss 4.063\n",
            "Ep 8 (Step 020870): Train loss 2.716, Val loss 4.061\n",
            "Ep 8 (Step 020875): Train loss 2.621, Val loss 4.061\n",
            "Ep 8 (Step 020880): Train loss 2.634, Val loss 4.060\n",
            "Ep 8 (Step 020885): Train loss 2.792, Val loss 4.057\n",
            "Ep 8 (Step 020890): Train loss 2.677, Val loss 4.054\n",
            "Ep 8 (Step 020895): Train loss 2.994, Val loss 4.064\n",
            "Ep 8 (Step 020900): Train loss 2.611, Val loss 4.077\n",
            "Ep 8 (Step 020905): Train loss 2.613, Val loss 4.073\n",
            "Ep 8 (Step 020910): Train loss 2.684, Val loss 4.068\n",
            "Ep 8 (Step 020915): Train loss 2.650, Val loss 4.059\n",
            "Ep 8 (Step 020920): Train loss 2.315, Val loss 4.061\n",
            "Ep 8 (Step 020925): Train loss 2.746, Val loss 4.066\n",
            "Ep 8 (Step 020930): Train loss 2.694, Val loss 4.073\n",
            "Ep 8 (Step 020935): Train loss 3.067, Val loss 4.082\n",
            "Ep 8 (Step 020940): Train loss 2.677, Val loss 4.086\n",
            "Ep 8 (Step 020945): Train loss 2.589, Val loss 4.082\n",
            "Ep 8 (Step 020950): Train loss 2.514, Val loss 4.083\n",
            "Ep 8 (Step 020955): Train loss 2.610, Val loss 4.091\n",
            "Ep 8 (Step 020960): Train loss 2.831, Val loss 4.100\n",
            "Ep 8 (Step 020965): Train loss 2.798, Val loss 4.102\n",
            "Ep 8 (Step 020970): Train loss 2.612, Val loss 4.096\n",
            "Ep 8 (Step 020975): Train loss 2.665, Val loss 4.091\n",
            "Ep 8 (Step 020980): Train loss 2.818, Val loss 4.086\n",
            "Ep 8 (Step 020985): Train loss 2.808, Val loss 4.096\n",
            "Ep 8 (Step 020990): Train loss 2.620, Val loss 4.100\n",
            "Ep 8 (Step 020995): Train loss 2.797, Val loss 4.089\n",
            "Ep 8 (Step 021000): Train loss 2.440, Val loss 4.085\n",
            "Ep 8 (Step 021005): Train loss 2.683, Val loss 4.088\n",
            "Ep 8 (Step 021010): Train loss 2.648, Val loss 4.087\n",
            "Ep 8 (Step 021015): Train loss 2.963, Val loss 4.086\n",
            "Ep 8 (Step 021020): Train loss 2.781, Val loss 4.107\n",
            "Ep 8 (Step 021025): Train loss 2.343, Val loss 4.116\n",
            "Ep 8 (Step 021030): Train loss 2.752, Val loss 4.126\n",
            "Ep 8 (Step 021035): Train loss 2.864, Val loss 4.133\n",
            "Ep 8 (Step 021040): Train loss 2.672, Val loss 4.132\n",
            "Ep 8 (Step 021045): Train loss 2.495, Val loss 4.116\n",
            "Ep 8 (Step 021050): Train loss 2.755, Val loss 4.122\n",
            "Ep 8 (Step 021055): Train loss 2.968, Val loss 4.117\n",
            "Ep 8 (Step 021060): Train loss 2.994, Val loss 4.119\n",
            "Ep 8 (Step 021065): Train loss 2.832, Val loss 4.118\n",
            "Ep 8 (Step 021070): Train loss 2.619, Val loss 4.136\n",
            "Ep 8 (Step 021075): Train loss 2.849, Val loss 4.135\n",
            "Ep 8 (Step 021080): Train loss 2.762, Val loss 4.123\n",
            "Ep 8 (Step 021085): Train loss 2.556, Val loss 4.115\n",
            "Ep 8 (Step 021090): Train loss 2.490, Val loss 4.110\n",
            "Ep 8 (Step 021095): Train loss 2.557, Val loss 4.104\n",
            "Ep 8 (Step 021100): Train loss 2.688, Val loss 4.093\n",
            "Ep 8 (Step 021105): Train loss 2.595, Val loss 4.085\n",
            "Ep 8 (Step 021110): Train loss 2.775, Val loss 4.089\n",
            "Ep 8 (Step 021115): Train loss 2.903, Val loss 4.099\n",
            "Ep 8 (Step 021120): Train loss 2.590, Val loss 4.105\n",
            "Ep 8 (Step 021125): Train loss 2.890, Val loss 4.101\n",
            "Ep 8 (Step 021130): Train loss 2.640, Val loss 4.101\n",
            "Ep 8 (Step 021135): Train loss 2.772, Val loss 4.093\n",
            "Ep 8 (Step 021140): Train loss 3.066, Val loss 4.102\n",
            "Ep 8 (Step 021145): Train loss 2.557, Val loss 4.127\n",
            "Ep 8 (Step 021150): Train loss 2.769, Val loss 4.133\n",
            "Ep 8 (Step 021155): Train loss 2.390, Val loss 4.129\n",
            "Ep 8 (Step 021160): Train loss 2.809, Val loss 4.112\n",
            "Ep 8 (Step 021165): Train loss 2.720, Val loss 4.115\n",
            "Ep 8 (Step 021170): Train loss 2.742, Val loss 4.108\n",
            "Ep 8 (Step 021175): Train loss 2.658, Val loss 4.113\n",
            "Ep 8 (Step 021180): Train loss 2.736, Val loss 4.119\n",
            "Ep 8 (Step 021185): Train loss 2.896, Val loss 4.106\n",
            "Ep 8 (Step 021190): Train loss 2.823, Val loss 4.105\n",
            "Ep 8 (Step 021195): Train loss 2.781, Val loss 4.112\n",
            "Ep 8 (Step 021200): Train loss 2.904, Val loss 4.119\n",
            "Ep 8 (Step 021205): Train loss 3.038, Val loss 4.107\n",
            "Ep 8 (Step 021210): Train loss 2.649, Val loss 4.118\n",
            "Ep 8 (Step 021215): Train loss 2.756, Val loss 4.121\n",
            "Ep 8 (Step 021220): Train loss 2.651, Val loss 4.113\n",
            "Ep 8 (Step 021225): Train loss 2.874, Val loss 4.133\n",
            "Ep 8 (Step 021230): Train loss 2.292, Val loss 4.132\n",
            "Ep 8 (Step 021235): Train loss 2.836, Val loss 4.137\n",
            "Ep 8 (Step 021240): Train loss 3.166, Val loss 4.138\n",
            "Ep 8 (Step 021245): Train loss 3.018, Val loss 4.150\n",
            "Ep 8 (Step 021250): Train loss 2.598, Val loss 4.145\n",
            "Ep 8 (Step 021255): Train loss 3.102, Val loss 4.139\n",
            "Ep 8 (Step 021260): Train loss 2.464, Val loss 4.137\n",
            "Ep 8 (Step 021265): Train loss 2.620, Val loss 4.142\n",
            "Ep 8 (Step 021270): Train loss 2.849, Val loss 4.136\n",
            "Ep 8 (Step 021275): Train loss 2.509, Val loss 4.135\n",
            "Ep 8 (Step 021280): Train loss 2.856, Val loss 4.136\n",
            "Ep 8 (Step 021285): Train loss 2.817, Val loss 4.115\n",
            "Ep 8 (Step 021290): Train loss 3.091, Val loss 4.113\n",
            "Ep 8 (Step 021295): Train loss 2.365, Val loss 4.135\n",
            "Ep 8 (Step 021300): Train loss 2.834, Val loss 4.137\n",
            "Ep 8 (Step 021305): Train loss 2.970, Val loss 4.137\n",
            "Ep 8 (Step 021310): Train loss 2.829, Val loss 4.141\n",
            "Ep 8 (Step 021315): Train loss 2.343, Val loss 4.135\n",
            "Ep 8 (Step 021320): Train loss 2.755, Val loss 4.143\n",
            "Ep 8 (Step 021325): Train loss 2.881, Val loss 4.143\n",
            "Ep 8 (Step 021330): Train loss 2.755, Val loss 4.145\n",
            "Ep 8 (Step 021335): Train loss 2.807, Val loss 4.146\n",
            "Ep 8 (Step 021340): Train loss 2.806, Val loss 4.138\n",
            "Ep 8 (Step 021345): Train loss 2.712, Val loss 4.149\n",
            "Ep 8 (Step 021350): Train loss 2.513, Val loss 4.153\n",
            "Ep 8 (Step 021355): Train loss 2.861, Val loss 4.148\n",
            "Ep 8 (Step 021360): Train loss 2.833, Val loss 4.167\n",
            "Ep 8 (Step 021365): Train loss 2.637, Val loss 4.157\n",
            "Ep 8 (Step 021370): Train loss 2.798, Val loss 4.133\n",
            "Ep 8 (Step 021375): Train loss 2.863, Val loss 4.133\n",
            "Ep 8 (Step 021380): Train loss 2.866, Val loss 4.148\n",
            "Ep 8 (Step 021385): Train loss 2.762, Val loss 4.155\n",
            "Ep 8 (Step 021390): Train loss 2.751, Val loss 4.155\n",
            "Ep 8 (Step 021395): Train loss 2.712, Val loss 4.141\n",
            "Ep 8 (Step 021400): Train loss 2.711, Val loss 4.118\n",
            "Ep 8 (Step 021405): Train loss 2.548, Val loss 4.113\n",
            "Ep 8 (Step 021410): Train loss 2.961, Val loss 4.129\n",
            "Ep 8 (Step 021415): Train loss 2.761, Val loss 4.143\n",
            "Ep 8 (Step 021420): Train loss 2.587, Val loss 4.127\n",
            "Ep 8 (Step 021425): Train loss 2.784, Val loss 4.114\n",
            "Ep 8 (Step 021430): Train loss 2.580, Val loss 4.118\n",
            "Ep 8 (Step 021435): Train loss 2.475, Val loss 4.112\n",
            "Ep 8 (Step 021440): Train loss 2.562, Val loss 4.108\n",
            "Ep 8 (Step 021445): Train loss 2.725, Val loss 4.112\n",
            "Ep 8 (Step 021450): Train loss 2.851, Val loss 4.119\n",
            "Ep 8 (Step 021455): Train loss 3.077, Val loss 4.116\n",
            "Ep 8 (Step 021460): Train loss 2.626, Val loss 4.123\n",
            "Ep 8 (Step 021465): Train loss 2.571, Val loss 4.133\n",
            "Ep 8 (Step 021470): Train loss 2.758, Val loss 4.141\n",
            "Ep 8 (Step 021475): Train loss 2.682, Val loss 4.140\n",
            "Ep 8 (Step 021480): Train loss 2.741, Val loss 4.136\n",
            "Ep 8 (Step 021485): Train loss 2.373, Val loss 4.128\n",
            "Ep 8 (Step 021490): Train loss 2.840, Val loss 4.139\n",
            "Ep 8 (Step 021495): Train loss 2.631, Val loss 4.141\n",
            "Ep 8 (Step 021500): Train loss 2.644, Val loss 4.123\n",
            "Ep 8 (Step 021505): Train loss 2.501, Val loss 4.122\n",
            "Ep 8 (Step 021510): Train loss 2.593, Val loss 4.126\n",
            "Ep 8 (Step 021515): Train loss 2.625, Val loss 4.123\n",
            "Ep 8 (Step 021520): Train loss 2.659, Val loss 4.113\n",
            "Ep 8 (Step 021525): Train loss 2.588, Val loss 4.113\n",
            "Ep 8 (Step 021530): Train loss 2.876, Val loss 4.113\n",
            "Ep 8 (Step 021535): Train loss 2.909, Val loss 4.133\n",
            "Ep 8 (Step 021540): Train loss 2.739, Val loss 4.131\n",
            "Ep 8 (Step 021545): Train loss 3.123, Val loss 4.123\n",
            "Ep 8 (Step 021550): Train loss 2.617, Val loss 4.119\n",
            "Ep 8 (Step 021555): Train loss 2.632, Val loss 4.118\n",
            "Ep 8 (Step 021560): Train loss 2.818, Val loss 4.124\n",
            "Ep 8 (Step 021565): Train loss 2.611, Val loss 4.104\n",
            "Ep 8 (Step 021570): Train loss 2.569, Val loss 4.099\n",
            "Ep 8 (Step 021575): Train loss 2.805, Val loss 4.111\n",
            "Ep 8 (Step 021580): Train loss 2.707, Val loss 4.124\n",
            "Ep 8 (Step 021585): Train loss 2.900, Val loss 4.132\n",
            "Ep 8 (Step 021590): Train loss 2.440, Val loss 4.131\n",
            "Ep 8 (Step 021595): Train loss 2.415, Val loss 4.133\n",
            "Ep 8 (Step 021600): Train loss 2.822, Val loss 4.142\n",
            "Ep 8 (Step 021605): Train loss 2.893, Val loss 4.139\n",
            "Ep 8 (Step 021610): Train loss 2.799, Val loss 4.143\n",
            "Ep 8 (Step 021615): Train loss 2.880, Val loss 4.134\n",
            "Ep 8 (Step 021620): Train loss 2.877, Val loss 4.127\n",
            "Ep 8 (Step 021625): Train loss 2.852, Val loss 4.137\n",
            "Ep 8 (Step 021630): Train loss 2.559, Val loss 4.129\n",
            "Ep 8 (Step 021635): Train loss 2.221, Val loss 4.124\n",
            "Ep 8 (Step 021640): Train loss 3.023, Val loss 4.123\n",
            "Ep 8 (Step 021645): Train loss 2.556, Val loss 4.127\n",
            "Ep 8 (Step 021650): Train loss 2.724, Val loss 4.118\n",
            "Ep 8 (Step 021655): Train loss 2.694, Val loss 4.115\n",
            "Ep 8 (Step 021660): Train loss 2.565, Val loss 4.095\n",
            "Ep 8 (Step 021665): Train loss 2.926, Val loss 4.096\n",
            "Ep 8 (Step 021670): Train loss 2.602, Val loss 4.099\n",
            "Ep 8 (Step 021675): Train loss 2.664, Val loss 4.096\n",
            "Ep 8 (Step 021680): Train loss 2.658, Val loss 4.087\n",
            "Ep 8 (Step 021685): Train loss 2.700, Val loss 4.087\n",
            "Ep 8 (Step 021690): Train loss 2.628, Val loss 4.097\n",
            "Ep 8 (Step 021695): Train loss 2.566, Val loss 4.128\n",
            "Ep 8 (Step 021700): Train loss 2.670, Val loss 4.132\n",
            "Ep 8 (Step 021705): Train loss 2.844, Val loss 4.127\n",
            "Ep 8 (Step 021710): Train loss 2.700, Val loss 4.141\n",
            "Ep 8 (Step 021715): Train loss 3.037, Val loss 4.153\n",
            "Ep 8 (Step 021720): Train loss 2.928, Val loss 4.149\n",
            "Ep 8 (Step 021725): Train loss 2.361, Val loss 4.124\n",
            "Ep 8 (Step 021730): Train loss 2.392, Val loss 4.111\n",
            "Ep 8 (Step 021735): Train loss 2.550, Val loss 4.110\n",
            "Ep 8 (Step 021740): Train loss 2.391, Val loss 4.118\n",
            "Ep 8 (Step 021745): Train loss 2.786, Val loss 4.110\n",
            "Ep 8 (Step 021750): Train loss 2.705, Val loss 4.107\n",
            "Ep 8 (Step 021755): Train loss 2.316, Val loss 4.103\n",
            "Ep 8 (Step 021760): Train loss 2.671, Val loss 4.105\n",
            "Ep 8 (Step 021765): Train loss 2.618, Val loss 4.103\n",
            "Ep 8 (Step 021770): Train loss 2.678, Val loss 4.097\n",
            "Ep 8 (Step 021775): Train loss 2.725, Val loss 4.107\n",
            "Ep 8 (Step 021780): Train loss 2.779, Val loss 4.114\n",
            "Ep 8 (Step 021785): Train loss 2.362, Val loss 4.113\n",
            "Ep 8 (Step 021790): Train loss 2.850, Val loss 4.111\n",
            "Ep 8 (Step 021795): Train loss 2.527, Val loss 4.111\n",
            "Ep 8 (Step 021800): Train loss 2.851, Val loss 4.112\n",
            "Ep 8 (Step 021805): Train loss 2.751, Val loss 4.106\n",
            "Ep 8 (Step 021810): Train loss 3.029, Val loss 4.107\n",
            "Ep 8 (Step 021815): Train loss 2.866, Val loss 4.106\n",
            "Ep 8 (Step 021820): Train loss 2.614, Val loss 4.118\n",
            "Ep 8 (Step 021825): Train loss 2.715, Val loss 4.109\n",
            "Ep 8 (Step 021830): Train loss 2.920, Val loss 4.110\n",
            "Ep 8 (Step 021835): Train loss 2.637, Val loss 4.102\n",
            "Ep 8 (Step 021840): Train loss 2.535, Val loss 4.102\n",
            "Ep 8 (Step 021845): Train loss 2.784, Val loss 4.105\n",
            "Ep 8 (Step 021850): Train loss 2.625, Val loss 4.101\n",
            "Ep 8 (Step 021855): Train loss 2.526, Val loss 4.091\n",
            "Ep 8 (Step 021860): Train loss 2.798, Val loss 4.098\n",
            "Ep 8 (Step 021865): Train loss 2.733, Val loss 4.106\n",
            "Ep 8 (Step 021870): Train loss 2.614, Val loss 4.111\n",
            "Ep 8 (Step 021875): Train loss 2.547, Val loss 4.115\n",
            "Ep 8 (Step 021880): Train loss 2.740, Val loss 4.116\n",
            "Ep 8 (Step 021885): Train loss 2.436, Val loss 4.119\n",
            "Ep 8 (Step 021890): Train loss 2.646, Val loss 4.112\n",
            "Ep 8 (Step 021895): Train loss 2.724, Val loss 4.118\n",
            "Ep 8 (Step 021900): Train loss 3.028, Val loss 4.116\n",
            "Ep 8 (Step 021905): Train loss 2.795, Val loss 4.114\n",
            "Ep 8 (Step 021910): Train loss 3.088, Val loss 4.109\n",
            "Ep 8 (Step 021915): Train loss 2.819, Val loss 4.105\n",
            "Ep 8 (Step 021920): Train loss 2.458, Val loss 4.094\n",
            "Ep 8 (Step 021925): Train loss 2.617, Val loss 4.089\n",
            "Ep 8 (Step 021930): Train loss 2.292, Val loss 4.093\n",
            "Ep 8 (Step 021935): Train loss 2.707, Val loss 4.093\n",
            "Ep 8 (Step 021940): Train loss 2.827, Val loss 4.100\n",
            "Ep 8 (Step 021945): Train loss 2.416, Val loss 4.102\n",
            "Ep 8 (Step 021950): Train loss 2.432, Val loss 4.107\n",
            "Ep 8 (Step 021955): Train loss 2.626, Val loss 4.104\n",
            "Ep 8 (Step 021960): Train loss 2.680, Val loss 4.100\n",
            "Ep 8 (Step 021965): Train loss 2.304, Val loss 4.103\n",
            "Ep 8 (Step 021970): Train loss 2.853, Val loss 4.106\n",
            "Ep 8 (Step 021975): Train loss 2.665, Val loss 4.118\n",
            "Ep 8 (Step 021980): Train loss 2.422, Val loss 4.126\n",
            "Ep 8 (Step 021985): Train loss 3.035, Val loss 4.124\n",
            "Ep 8 (Step 021990): Train loss 2.853, Val loss 4.116\n",
            "Ep 8 (Step 021995): Train loss 2.449, Val loss 4.119\n",
            "Ep 8 (Step 022000): Train loss 2.740, Val loss 4.120\n",
            "Ep 8 (Step 022005): Train loss 2.591, Val loss 4.118\n",
            "Ep 8 (Step 022010): Train loss 2.713, Val loss 4.109\n",
            "Ep 8 (Step 022015): Train loss 2.306, Val loss 4.116\n",
            "Ep 8 (Step 022020): Train loss 2.396, Val loss 4.122\n",
            "Ep 8 (Step 022025): Train loss 2.574, Val loss 4.108\n",
            "Ep 8 (Step 022030): Train loss 2.851, Val loss 4.102\n",
            "Ep 8 (Step 022035): Train loss 2.893, Val loss 4.098\n",
            "Ep 8 (Step 022040): Train loss 2.658, Val loss 4.098\n",
            "Ep 8 (Step 022045): Train loss 2.560, Val loss 4.101\n",
            "Ep 8 (Step 022050): Train loss 2.897, Val loss 4.104\n",
            "Ep 8 (Step 022055): Train loss 2.590, Val loss 4.107\n",
            "Ep 8 (Step 022060): Train loss 2.685, Val loss 4.110\n",
            "Ep 8 (Step 022065): Train loss 2.418, Val loss 4.105\n",
            "Ep 8 (Step 022070): Train loss 2.369, Val loss 4.095\n",
            "Ep 8 (Step 022075): Train loss 2.872, Val loss 4.101\n",
            "Ep 8 (Step 022080): Train loss 2.695, Val loss 4.107\n",
            "Ep 8 (Step 022085): Train loss 2.645, Val loss 4.105\n",
            "Ep 8 (Step 022090): Train loss 2.486, Val loss 4.100\n",
            "Ep 8 (Step 022095): Train loss 3.152, Val loss 4.109\n",
            "Ep 8 (Step 022100): Train loss 2.774, Val loss 4.116\n",
            "Ep 8 (Step 022105): Train loss 2.810, Val loss 4.111\n",
            "Ep 8 (Step 022110): Train loss 2.624, Val loss 4.121\n",
            "Ep 8 (Step 022115): Train loss 2.671, Val loss 4.116\n",
            "Ep 8 (Step 022120): Train loss 2.893, Val loss 4.120\n",
            "Ep 8 (Step 022125): Train loss 2.530, Val loss 4.119\n",
            "Ep 8 (Step 022130): Train loss 2.982, Val loss 4.117\n",
            "Ep 8 (Step 022135): Train loss 2.734, Val loss 4.107\n",
            "Ep 8 (Step 022140): Train loss 2.729, Val loss 4.097\n",
            "Ep 8 (Step 022145): Train loss 2.651, Val loss 4.103\n",
            "Ep 8 (Step 022150): Train loss 2.705, Val loss 4.108\n",
            "Ep 8 (Step 022155): Train loss 2.670, Val loss 4.114\n",
            "Ep 8 (Step 022160): Train loss 2.751, Val loss 4.113\n",
            "Ep 8 (Step 022165): Train loss 2.598, Val loss 4.112\n",
            "Ep 8 (Step 022170): Train loss 2.323, Val loss 4.107\n",
            "Ep 8 (Step 022175): Train loss 2.697, Val loss 4.113\n",
            "Ep 8 (Step 022180): Train loss 2.359, Val loss 4.122\n",
            "Ep 8 (Step 022185): Train loss 2.688, Val loss 4.127\n",
            "Ep 8 (Step 022190): Train loss 2.401, Val loss 4.117\n",
            "Ep 8 (Step 022195): Train loss 2.463, Val loss 4.119\n",
            "Ep 8 (Step 022200): Train loss 2.653, Val loss 4.137\n",
            "Ep 8 (Step 022205): Train loss 2.641, Val loss 4.149\n",
            "Ep 8 (Step 022210): Train loss 2.943, Val loss 4.146\n",
            "Ep 8 (Step 022215): Train loss 2.759, Val loss 4.135\n",
            "Ep 8 (Step 022220): Train loss 2.622, Val loss 4.123\n",
            "Ep 8 (Step 022225): Train loss 2.592, Val loss 4.113\n",
            "Ep 8 (Step 022230): Train loss 2.791, Val loss 4.117\n",
            "Ep 8 (Step 022235): Train loss 2.545, Val loss 4.121\n",
            "Ep 8 (Step 022240): Train loss 2.817, Val loss 4.123\n",
            "Ep 8 (Step 022245): Train loss 2.145, Val loss 4.121\n",
            "Ep 8 (Step 022250): Train loss 2.831, Val loss 4.120\n",
            "Ep 8 (Step 022255): Train loss 2.699, Val loss 4.108\n",
            "Ep 8 (Step 022260): Train loss 2.526, Val loss 4.099\n",
            "Ep 8 (Step 022265): Train loss 2.541, Val loss 4.108\n",
            "Ep 8 (Step 022270): Train loss 2.549, Val loss 4.116\n",
            "Ep 8 (Step 022275): Train loss 2.974, Val loss 4.125\n",
            "Ep 8 (Step 022280): Train loss 2.620, Val loss 4.123\n",
            "Ep 8 (Step 022285): Train loss 2.652, Val loss 4.125\n",
            "Ep 8 (Step 022290): Train loss 2.761, Val loss 4.125\n",
            "Ep 8 (Step 022295): Train loss 2.975, Val loss 4.111\n",
            "Ep 8 (Step 022300): Train loss 2.758, Val loss 4.109\n",
            "Ep 8 (Step 022305): Train loss 2.947, Val loss 4.113\n",
            "Ep 8 (Step 022310): Train loss 2.460, Val loss 4.117\n",
            "Ep 8 (Step 022315): Train loss 2.746, Val loss 4.117\n",
            "Ep 8 (Step 022320): Train loss 2.500, Val loss 4.114\n",
            "Ep 8 (Step 022325): Train loss 2.489, Val loss 4.103\n",
            "Ep 8 (Step 022330): Train loss 2.795, Val loss 4.089\n",
            "Ep 8 (Step 022335): Train loss 3.082, Val loss 4.069\n",
            "Ep 8 (Step 022340): Train loss 2.588, Val loss 4.060\n",
            "Ep 8 (Step 022345): Train loss 2.595, Val loss 4.053\n",
            "Ep 8 (Step 022350): Train loss 2.774, Val loss 4.049\n",
            "Ep 8 (Step 022355): Train loss 2.633, Val loss 4.049\n",
            "Ep 8 (Step 022360): Train loss 2.850, Val loss 4.066\n",
            "Ep 8 (Step 022365): Train loss 2.685, Val loss 4.084\n",
            "Ep 8 (Step 022370): Train loss 2.868, Val loss 4.074\n",
            "Ep 8 (Step 022375): Train loss 2.859, Val loss 4.078\n",
            "Ep 8 (Step 022380): Train loss 2.633, Val loss 4.084\n",
            "Ep 8 (Step 022385): Train loss 2.983, Val loss 4.079\n",
            "Ep 8 (Step 022390): Train loss 2.617, Val loss 4.075\n",
            "Ep 8 (Step 022395): Train loss 2.419, Val loss 4.079\n",
            "Ep 8 (Step 022400): Train loss 3.024, Val loss 4.078\n",
            "Ep 8 (Step 022405): Train loss 2.619, Val loss 4.072\n",
            "Ep 8 (Step 022410): Train loss 2.396, Val loss 4.075\n",
            "Ep 8 (Step 022415): Train loss 2.841, Val loss 4.091\n",
            "Ep 8 (Step 022420): Train loss 2.756, Val loss 4.088\n",
            "Ep 8 (Step 022425): Train loss 2.411, Val loss 4.076\n",
            "Ep 8 (Step 022430): Train loss 2.726, Val loss 4.081\n",
            "Ep 8 (Step 022435): Train loss 2.508, Val loss 4.089\n",
            "Ep 8 (Step 022440): Train loss 2.733, Val loss 4.079\n",
            "Ep 8 (Step 022445): Train loss 2.517, Val loss 4.087\n",
            "Ep 8 (Step 022450): Train loss 2.702, Val loss 4.099\n",
            "Ep 8 (Step 022455): Train loss 2.497, Val loss 4.104\n",
            "Ep 8 (Step 022460): Train loss 2.635, Val loss 4.112\n",
            "Ep 8 (Step 022465): Train loss 2.819, Val loss 4.109\n",
            "Ep 8 (Step 022470): Train loss 2.749, Val loss 4.108\n",
            "Ep 8 (Step 022475): Train loss 2.484, Val loss 4.102\n",
            "Ep 8 (Step 022480): Train loss 2.721, Val loss 4.102\n",
            "Ep 8 (Step 022485): Train loss 2.509, Val loss 4.094\n",
            "Ep 8 (Step 022490): Train loss 2.704, Val loss 4.075\n",
            "Ep 8 (Step 022495): Train loss 2.679, Val loss 4.070\n",
            "Ep 8 (Step 022500): Train loss 2.796, Val loss 4.071\n",
            "Ep 8 (Step 022505): Train loss 2.693, Val loss 4.088\n",
            "Ep 8 (Step 022510): Train loss 2.594, Val loss 4.091\n",
            "Ep 8 (Step 022515): Train loss 2.567, Val loss 4.083\n",
            "Ep 8 (Step 022520): Train loss 2.459, Val loss 4.071\n",
            "Ep 8 (Step 022525): Train loss 2.769, Val loss 4.058\n",
            "Ep 8 (Step 022530): Train loss 2.744, Val loss 4.068\n",
            "Ep 8 (Step 022535): Train loss 2.494, Val loss 4.074\n",
            "Ep 8 (Step 022540): Train loss 2.457, Val loss 4.058\n",
            "Ep 8 (Step 022545): Train loss 2.369, Val loss 4.065\n",
            "Ep 8 (Step 022550): Train loss 2.269, Val loss 4.073\n",
            "Ep 8 (Step 022555): Train loss 2.970, Val loss 4.094\n",
            "Ep 8 (Step 022560): Train loss 2.769, Val loss 4.083\n",
            "Ep 8 (Step 022565): Train loss 2.473, Val loss 4.071\n",
            "Ep 8 (Step 022570): Train loss 2.902, Val loss 4.070\n",
            "Ep 8 (Step 022575): Train loss 2.538, Val loss 4.084\n",
            "Ep 8 (Step 022580): Train loss 2.520, Val loss 4.088\n",
            "Ep 8 (Step 022585): Train loss 2.499, Val loss 4.083\n",
            "Ep 8 (Step 022590): Train loss 2.514, Val loss 4.070\n",
            "Ep 8 (Step 022595): Train loss 2.515, Val loss 4.067\n",
            "Ep 8 (Step 022600): Train loss 3.266, Val loss 4.063\n",
            "Ep 8 (Step 022605): Train loss 2.770, Val loss 4.066\n",
            "Ep 8 (Step 022610): Train loss 2.530, Val loss 4.078\n",
            "Ep 8 (Step 022615): Train loss 2.962, Val loss 4.081\n",
            "Ep 8 (Step 022620): Train loss 2.777, Val loss 4.073\n",
            "Ep 8 (Step 022625): Train loss 2.496, Val loss 4.070\n",
            "Ep 8 (Step 022630): Train loss 2.910, Val loss 4.067\n",
            "Ep 8 (Step 022635): Train loss 2.585, Val loss 4.068\n",
            "Ep 8 (Step 022640): Train loss 2.917, Val loss 4.076\n",
            "Ep 8 (Step 022645): Train loss 2.505, Val loss 4.084\n",
            "Ep 8 (Step 022650): Train loss 2.720, Val loss 4.090\n",
            "Ep 8 (Step 022655): Train loss 2.690, Val loss 4.086\n",
            "Ep 8 (Step 022660): Train loss 2.327, Val loss 4.081\n",
            "Ep 8 (Step 022665): Train loss 2.610, Val loss 4.078\n",
            "Ep 8 (Step 022670): Train loss 2.451, Val loss 4.079\n",
            "Ep 8 (Step 022675): Train loss 2.529, Val loss 4.087\n",
            "Ep 8 (Step 022680): Train loss 2.690, Val loss 4.086\n",
            "Ep 8 (Step 022685): Train loss 2.541, Val loss 4.086\n",
            "Ep 8 (Step 022690): Train loss 2.769, Val loss 4.092\n",
            "Ep 8 (Step 022695): Train loss 2.720, Val loss 4.083\n",
            "Ep 8 (Step 022700): Train loss 2.617, Val loss 4.088\n",
            "Ep 8 (Step 022705): Train loss 2.484, Val loss 4.093\n",
            "Ep 8 (Step 022710): Train loss 2.793, Val loss 4.077\n",
            "Ep 8 (Step 022715): Train loss 2.610, Val loss 4.076\n",
            "Ep 8 (Step 022720): Train loss 2.618, Val loss 4.081\n",
            "Ep 8 (Step 022725): Train loss 2.484, Val loss 4.097\n",
            "Ep 8 (Step 022730): Train loss 2.562, Val loss 4.101\n",
            "Ep 8 (Step 022735): Train loss 2.832, Val loss 4.102\n",
            "Ep 8 (Step 022740): Train loss 2.601, Val loss 4.105\n",
            "Ep 8 (Step 022745): Train loss 2.524, Val loss 4.125\n",
            "Ep 8 (Step 022750): Train loss 2.703, Val loss 4.111\n",
            "Ep 8 (Step 022755): Train loss 2.528, Val loss 4.106\n",
            "Ep 8 (Step 022760): Train loss 2.493, Val loss 4.106\n",
            "Ep 8 (Step 022765): Train loss 2.544, Val loss 4.104\n",
            "Ep 8 (Step 022770): Train loss 2.566, Val loss 4.102\n",
            "Ep 8 (Step 022775): Train loss 2.670, Val loss 4.108\n",
            "Ep 8 (Step 022780): Train loss 2.763, Val loss 4.107\n",
            "Ep 8 (Step 022785): Train loss 2.761, Val loss 4.113\n",
            "Ep 8 (Step 022790): Train loss 2.350, Val loss 4.092\n",
            "Ep 8 (Step 022795): Train loss 2.574, Val loss 4.077\n",
            "Ep 8 (Step 022800): Train loss 2.549, Val loss 4.066\n",
            "Ep 8 (Step 022805): Train loss 2.851, Val loss 4.082\n",
            "Ep 8 (Step 022810): Train loss 2.893, Val loss 4.096\n",
            "Ep 8 (Step 022815): Train loss 2.589, Val loss 4.092\n",
            "Ep 8 (Step 022820): Train loss 2.512, Val loss 4.081\n",
            "Ep 8 (Step 022825): Train loss 2.645, Val loss 4.076\n",
            "Ep 8 (Step 022830): Train loss 2.817, Val loss 4.088\n",
            "Ep 8 (Step 022835): Train loss 2.781, Val loss 4.099\n",
            "Ep 8 (Step 022840): Train loss 2.619, Val loss 4.104\n",
            "Ep 8 (Step 022845): Train loss 2.488, Val loss 4.112\n",
            "Ep 8 (Step 022850): Train loss 2.790, Val loss 4.120\n",
            "Ep 8 (Step 022855): Train loss 2.451, Val loss 4.112\n",
            "Ep 8 (Step 022860): Train loss 2.576, Val loss 4.107\n",
            "Ep 8 (Step 022865): Train loss 2.858, Val loss 4.109\n",
            "Ep 8 (Step 022870): Train loss 2.910, Val loss 4.099\n",
            "Ep 8 (Step 022875): Train loss 2.430, Val loss 4.088\n",
            "Ep 8 (Step 022880): Train loss 2.518, Val loss 4.079\n",
            "Ep 8 (Step 022885): Train loss 2.802, Val loss 4.072\n",
            "Ep 8 (Step 022890): Train loss 2.448, Val loss 4.074\n",
            "Ep 8 (Step 022895): Train loss 2.601, Val loss 4.089\n",
            "Ep 8 (Step 022900): Train loss 2.459, Val loss 4.094\n",
            "Ep 8 (Step 022905): Train loss 2.742, Val loss 4.093\n",
            "Ep 8 (Step 022910): Train loss 2.700, Val loss 4.089\n",
            "Ep 8 (Step 022915): Train loss 2.716, Val loss 4.103\n",
            "Ep 8 (Step 022920): Train loss 2.593, Val loss 4.086\n",
            "Ep 8 (Step 022925): Train loss 2.304, Val loss 4.078\n",
            "Ep 8 (Step 022930): Train loss 2.584, Val loss 4.075\n",
            "Ep 8 (Step 022935): Train loss 2.641, Val loss 4.087\n",
            "Ep 8 (Step 022940): Train loss 2.573, Val loss 4.102\n",
            "Ep 8 (Step 022945): Train loss 2.481, Val loss 4.112\n",
            "Ep 8 (Step 022950): Train loss 2.901, Val loss 4.106\n",
            "Ep 8 (Step 022955): Train loss 2.528, Val loss 4.110\n",
            "Ep 8 (Step 022960): Train loss 2.495, Val loss 4.115\n",
            "Ep 8 (Step 022965): Train loss 2.730, Val loss 4.128\n",
            "Ep 8 (Step 022970): Train loss 2.727, Val loss 4.109\n",
            "Ep 8 (Step 022975): Train loss 2.728, Val loss 4.102\n",
            "Ep 8 (Step 022980): Train loss 2.626, Val loss 4.112\n",
            "Ep 8 (Step 022985): Train loss 2.770, Val loss 4.129\n",
            "Ep 8 (Step 022990): Train loss 2.757, Val loss 4.135\n",
            "Ep 8 (Step 022995): Train loss 2.708, Val loss 4.133\n",
            "Ep 8 (Step 023000): Train loss 2.442, Val loss 4.121\n",
            "Ep 8 (Step 023005): Train loss 2.444, Val loss 4.120\n",
            "Ep 8 (Step 023010): Train loss 2.509, Val loss 4.113\n",
            "Ep 8 (Step 023015): Train loss 2.522, Val loss 4.081\n",
            "Ep 8 (Step 023020): Train loss 2.525, Val loss 4.075\n",
            "Ep 8 (Step 023025): Train loss 2.674, Val loss 4.093\n",
            "Ep 8 (Step 023030): Train loss 2.317, Val loss 4.112\n",
            "Ep 8 (Step 023035): Train loss 2.543, Val loss 4.103\n",
            "Ep 8 (Step 023040): Train loss 2.562, Val loss 4.086\n",
            "Ep 8 (Step 023045): Train loss 2.907, Val loss 4.087\n",
            "Ep 8 (Step 023050): Train loss 2.656, Val loss 4.096\n",
            "Ep 8 (Step 023055): Train loss 2.707, Val loss 4.090\n",
            "Ep 8 (Step 023060): Train loss 2.705, Val loss 4.090\n",
            "Ep 8 (Step 023065): Train loss 2.629, Val loss 4.100\n",
            "Ep 8 (Step 023070): Train loss 2.440, Val loss 4.109\n",
            "Ep 8 (Step 023075): Train loss 2.799, Val loss 4.095\n",
            "Ep 8 (Step 023080): Train loss 2.501, Val loss 4.092\n",
            "Ep 8 (Step 023085): Train loss 2.680, Val loss 4.084\n",
            "Ep 8 (Step 023090): Train loss 2.422, Val loss 4.093\n",
            "Ep 8 (Step 023095): Train loss 2.743, Val loss 4.100\n",
            "Ep 8 (Step 023100): Train loss 2.950, Val loss 4.100\n",
            "Ep 8 (Step 023105): Train loss 2.890, Val loss 4.109\n",
            "Ep 8 (Step 023110): Train loss 2.468, Val loss 4.109\n",
            "Ep 8 (Step 023115): Train loss 2.428, Val loss 4.111\n",
            "Ep 8 (Step 023120): Train loss 2.802, Val loss 4.098\n",
            "Ep 8 (Step 023125): Train loss 2.701, Val loss 4.090\n",
            "Ep 8 (Step 023130): Train loss 2.958, Val loss 4.084\n",
            "Ep 8 (Step 023135): Train loss 2.519, Val loss 4.084\n",
            "Ep 8 (Step 023140): Train loss 2.516, Val loss 4.087\n",
            "Ep 8 (Step 023145): Train loss 2.602, Val loss 4.092\n",
            "Ep 8 (Step 023150): Train loss 2.726, Val loss 4.092\n",
            "Ep 8 (Step 023155): Train loss 2.659, Val loss 4.092\n",
            "Ep 8 (Step 023160): Train loss 2.226, Val loss 4.095\n",
            "Ep 8 (Step 023165): Train loss 2.555, Val loss 4.093\n",
            "Ep 8 (Step 023170): Train loss 2.313, Val loss 4.094\n",
            "Ep 8 (Step 023175): Train loss 2.442, Val loss 4.092\n",
            "Ep 8 (Step 023180): Train loss 2.853, Val loss 4.094\n",
            "Ep 8 (Step 023185): Train loss 2.533, Val loss 4.090\n",
            "Ep 8 (Step 023190): Train loss 2.703, Val loss 4.091\n",
            "Ep 8 (Step 023195): Train loss 2.774, Val loss 4.087\n",
            "Ep 8 (Step 023200): Train loss 2.360, Val loss 4.081\n",
            "Ep 8 (Step 023205): Train loss 2.416, Val loss 4.077\n",
            "Ep 8 (Step 023210): Train loss 2.767, Val loss 4.072\n",
            "Ep 8 (Step 023215): Train loss 2.597, Val loss 4.072\n",
            "Ep 8 (Step 023220): Train loss 2.647, Val loss 4.078\n",
            "Ep 8 (Step 023225): Train loss 2.947, Val loss 4.084\n",
            "Ep 8 (Step 023230): Train loss 2.705, Val loss 4.090\n",
            "Ep 8 (Step 023235): Train loss 2.638, Val loss 4.077\n",
            "Ep 8 (Step 023240): Train loss 2.375, Val loss 4.070\n",
            "Ep 8 (Step 023245): Train loss 2.534, Val loss 4.075\n",
            "Ep 8 (Step 023250): Train loss 2.570, Val loss 4.081\n",
            "Ep 8 (Step 023255): Train loss 2.684, Val loss 4.081\n",
            "Ep 8 (Step 023260): Train loss 2.701, Val loss 4.077\n",
            "Ep 8 (Step 023265): Train loss 2.624, Val loss 4.081\n",
            "Ep 8 (Step 023270): Train loss 2.545, Val loss 4.080\n",
            "Ep 8 (Step 023275): Train loss 2.761, Val loss 4.078\n",
            "Ep 8 (Step 023280): Train loss 2.688, Val loss 4.081\n",
            "Ep 8 (Step 023285): Train loss 2.668, Val loss 4.072\n",
            "Ep 8 (Step 023290): Train loss 2.768, Val loss 4.078\n",
            "Ep 8 (Step 023295): Train loss 2.604, Val loss 4.083\n",
            "Ep 8 (Step 023300): Train loss 2.637, Val loss 4.068\n",
            "Ep 8 (Step 023305): Train loss 2.739, Val loss 4.064\n",
            "Ep 8 (Step 023310): Train loss 2.759, Val loss 4.065\n",
            "Ep 8 (Step 023315): Train loss 2.701, Val loss 4.071\n",
            "Ep 8 (Step 023320): Train loss 2.520, Val loss 4.078\n",
            "Ep 8 (Step 023325): Train loss 2.497, Val loss 4.077\n",
            "Ep 8 (Step 023330): Train loss 2.683, Val loss 4.072\n",
            "Ep 8 (Step 023335): Train loss 2.546, Val loss 4.061\n",
            "Ep 8 (Step 023340): Train loss 2.927, Val loss 4.048\n",
            "Ep 8 (Step 023345): Train loss 2.657, Val loss 4.046\n",
            "Ep 8 (Step 023350): Train loss 2.395, Val loss 4.052\n",
            "Ep 8 (Step 023355): Train loss 2.469, Val loss 4.059\n",
            "Ep 8 (Step 023360): Train loss 2.318, Val loss 4.058\n",
            "Ep 8 (Step 023365): Train loss 2.649, Val loss 4.071\n",
            "Ep 8 (Step 023370): Train loss 2.366, Val loss 4.061\n",
            "Ep 8 (Step 023375): Train loss 2.605, Val loss 4.051\n",
            "Ep 8 (Step 023380): Train loss 2.666, Val loss 4.054\n",
            "Ep 8 (Step 023385): Train loss 2.478, Val loss 4.057\n",
            "Ep 8 (Step 023390): Train loss 2.752, Val loss 4.062\n",
            "Ep 8 (Step 023395): Train loss 2.742, Val loss 4.068\n",
            "Ep 8 (Step 023400): Train loss 2.303, Val loss 4.076\n",
            "Ep 8 (Step 023405): Train loss 2.770, Val loss 4.073\n",
            "Ep 8 (Step 023410): Train loss 2.668, Val loss 4.071\n",
            "Ep 8 (Step 023415): Train loss 2.909, Val loss 4.072\n",
            "Ep 8 (Step 023420): Train loss 2.543, Val loss 4.083\n",
            "Ep 8 (Step 023425): Train loss 2.780, Val loss 4.084\n",
            "Ep 8 (Step 023430): Train loss 2.443, Val loss 4.084\n",
            "Ep 8 (Step 023435): Train loss 2.561, Val loss 4.071\n",
            "Ep 8 (Step 023440): Train loss 2.861, Val loss 4.066\n",
            "Ep 8 (Step 023445): Train loss 2.414, Val loss 4.076\n",
            "Ep 8 (Step 023450): Train loss 2.478, Val loss 4.080\n",
            "Ep 8 (Step 023455): Train loss 2.632, Val loss 4.082\n",
            "Ep 8 (Step 023460): Train loss 2.639, Val loss 4.075\n",
            "Ep 8 (Step 023465): Train loss 2.500, Val loss 4.081\n",
            "Ep 8 (Step 023470): Train loss 2.424, Val loss 4.082\n",
            "Ep 8 (Step 023475): Train loss 2.436, Val loss 4.073\n",
            "Ep 8 (Step 023480): Train loss 2.805, Val loss 4.069\n",
            "Ep 8 (Step 023485): Train loss 2.297, Val loss 4.061\n",
            "Ep 8 (Step 023490): Train loss 2.398, Val loss 4.069\n",
            "Ep 8 (Step 023495): Train loss 2.760, Val loss 4.074\n",
            "Ep 8 (Step 023500): Train loss 2.367, Val loss 4.062\n",
            "Ep 8 (Step 023505): Train loss 2.551, Val loss 4.057\n",
            "Ep 8 (Step 023510): Train loss 2.567, Val loss 4.053\n",
            "Ep 8 (Step 023515): Train loss 2.657, Val loss 4.046\n",
            "Ep 8 (Step 023520): Train loss 2.676, Val loss 4.048\n",
            "Ep 8 (Step 023525): Train loss 2.496, Val loss 4.049\n",
            "Ep 8 (Step 023530): Train loss 2.542, Val loss 4.050\n",
            "Ep 8 (Step 023535): Train loss 2.428, Val loss 4.063\n",
            "Ep 8 (Step 023540): Train loss 2.676, Val loss 4.085\n",
            "Ep 8 (Step 023545): Train loss 2.442, Val loss 4.081\n",
            "Ep 8 (Step 023550): Train loss 2.516, Val loss 4.070\n",
            "Ep 8 (Step 023555): Train loss 2.358, Val loss 4.065\n",
            "Ep 8 (Step 023560): Train loss 2.661, Val loss 4.052\n",
            "Ep 8 (Step 023565): Train loss 2.584, Val loss 4.050\n",
            "Ep 8 (Step 023570): Train loss 2.618, Val loss 4.059\n",
            "Ep 8 (Step 023575): Train loss 2.708, Val loss 4.058\n",
            "Ep 8 (Step 023580): Train loss 2.469, Val loss 4.052\n",
            "Ep 8 (Step 023585): Train loss 2.693, Val loss 4.057\n",
            "Ep 8 (Step 023590): Train loss 2.790, Val loss 4.069\n",
            "Ep 8 (Step 023595): Train loss 2.354, Val loss 4.067\n",
            "Ep 8 (Step 023600): Train loss 2.326, Val loss 4.062\n",
            "Ep 8 (Step 023605): Train loss 2.637, Val loss 4.070\n",
            "Ep 8 (Step 023610): Train loss 2.465, Val loss 4.069\n",
            "Ep 8 (Step 023615): Train loss 2.568, Val loss 4.066\n",
            "Ep 8 (Step 023620): Train loss 2.595, Val loss 4.068\n",
            "Ep 8 (Step 023625): Train loss 2.622, Val loss 4.078\n",
            "Ep 8 (Step 023630): Train loss 2.683, Val loss 4.072\n",
            "Ep 8 (Step 023635): Train loss 2.514, Val loss 4.073\n",
            "Ep 8 (Step 023640): Train loss 2.599, Val loss 4.079\n",
            "Ep 8 (Step 023645): Train loss 2.849, Val loss 4.070\n",
            "Ep 8 (Step 023650): Train loss 2.642, Val loss 4.073\n",
            "Ep 8 (Step 023655): Train loss 2.491, Val loss 4.078\n",
            "Ep 8 (Step 023660): Train loss 2.546, Val loss 4.085\n",
            "Ep 8 (Step 023665): Train loss 2.682, Val loss 4.080\n",
            "Ep 8 (Step 023670): Train loss 2.579, Val loss 4.080\n",
            "Ep 8 (Step 023675): Train loss 2.320, Val loss 4.080\n",
            "Ep 8 (Step 023680): Train loss 2.461, Val loss 4.082\n",
            "Ep 8 (Step 023685): Train loss 2.783, Val loss 4.092\n",
            "Ep 8 (Step 023690): Train loss 2.288, Val loss 4.084\n",
            "Ep 8 (Step 023695): Train loss 2.632, Val loss 4.093\n",
            "Ep 8 (Step 023700): Train loss 2.607, Val loss 4.089\n",
            "Ep 8 (Step 023705): Train loss 2.889, Val loss 4.097\n",
            "Ep 8 (Step 023710): Train loss 2.709, Val loss 4.091\n",
            "Ep 8 (Step 023715): Train loss 2.680, Val loss 4.073\n",
            "Ep 8 (Step 023720): Train loss 2.811, Val loss 4.066\n",
            "Ep 8 (Step 023725): Train loss 2.690, Val loss 4.069\n",
            "Ep 8 (Step 023730): Train loss 2.463, Val loss 4.076\n",
            "Ep 8 (Step 023735): Train loss 2.424, Val loss 4.082\n",
            "Ep 8 (Step 023740): Train loss 2.496, Val loss 4.087\n",
            "Ep 8 (Step 023745): Train loss 2.451, Val loss 4.079\n",
            "Ep 8 (Step 023750): Train loss 2.544, Val loss 4.071\n",
            "Ep 8 (Step 023755): Train loss 2.622, Val loss 4.075\n",
            "Ep 8 (Step 023760): Train loss 2.567, Val loss 4.076\n",
            "Ep 8 (Step 023765): Train loss 2.393, Val loss 4.076\n",
            "Ep 8 (Step 023770): Train loss 2.580, Val loss 4.078\n",
            "Ep 8 (Step 023775): Train loss 2.635, Val loss 4.078\n",
            "Ep 8 (Step 023780): Train loss 2.844, Val loss 4.080\n",
            "Ep 8 (Step 023785): Train loss 2.591, Val loss 4.067\n",
            "Ep 8 (Step 023790): Train loss 2.669, Val loss 4.060\n",
            "Ep 8 (Step 023795): Train loss 2.645, Val loss 4.061\n",
            "Ep 8 (Step 023800): Train loss 2.888, Val loss 4.056\n",
            "Ep 8 (Step 023805): Train loss 2.638, Val loss 4.066\n",
            "Ep 8 (Step 023810): Train loss 2.800, Val loss 4.070\n",
            "Ep 8 (Step 023815): Train loss 2.522, Val loss 4.080\n",
            "Every effort moves you.  [_Exeunt._]  SCENE III. Another part of the Forest  Enter Charles and Bigot.  ROSALIND. I am glad of all my father’s father’s\n",
            "Ep 9 (Step 023820): Train loss 2.599, Val loss 4.080\n",
            "Ep 9 (Step 023825): Train loss 2.393, Val loss 4.086\n",
            "Ep 9 (Step 023830): Train loss 2.557, Val loss 4.099\n",
            "Ep 9 (Step 023835): Train loss 2.576, Val loss 4.110\n",
            "Ep 9 (Step 023840): Train loss 2.486, Val loss 4.116\n",
            "Ep 9 (Step 023845): Train loss 2.431, Val loss 4.122\n",
            "Ep 9 (Step 023850): Train loss 2.440, Val loss 4.113\n",
            "Ep 9 (Step 023855): Train loss 2.687, Val loss 4.119\n",
            "Ep 9 (Step 023860): Train loss 2.367, Val loss 4.124\n",
            "Ep 9 (Step 023865): Train loss 2.824, Val loss 4.128\n",
            "Ep 9 (Step 023870): Train loss 2.439, Val loss 4.120\n",
            "Ep 9 (Step 023875): Train loss 2.157, Val loss 4.112\n",
            "Ep 9 (Step 023880): Train loss 2.395, Val loss 4.109\n",
            "Ep 9 (Step 023885): Train loss 2.366, Val loss 4.121\n",
            "Ep 9 (Step 023890): Train loss 2.555, Val loss 4.120\n",
            "Ep 9 (Step 023895): Train loss 2.485, Val loss 4.116\n",
            "Ep 9 (Step 023900): Train loss 2.585, Val loss 4.122\n",
            "Ep 9 (Step 023905): Train loss 2.434, Val loss 4.115\n",
            "Ep 9 (Step 023910): Train loss 2.571, Val loss 4.115\n",
            "Ep 9 (Step 023915): Train loss 2.511, Val loss 4.127\n",
            "Ep 9 (Step 023920): Train loss 2.626, Val loss 4.147\n",
            "Ep 9 (Step 023925): Train loss 2.524, Val loss 4.159\n",
            "Ep 9 (Step 023930): Train loss 2.708, Val loss 4.156\n",
            "Ep 9 (Step 023935): Train loss 2.541, Val loss 4.154\n",
            "Ep 9 (Step 023940): Train loss 2.609, Val loss 4.160\n",
            "Ep 9 (Step 023945): Train loss 2.321, Val loss 4.160\n",
            "Ep 9 (Step 023950): Train loss 2.631, Val loss 4.159\n",
            "Ep 9 (Step 023955): Train loss 2.629, Val loss 4.178\n",
            "Ep 9 (Step 023960): Train loss 2.546, Val loss 4.188\n",
            "Ep 9 (Step 023965): Train loss 2.653, Val loss 4.168\n",
            "Ep 9 (Step 023970): Train loss 2.618, Val loss 4.166\n",
            "Ep 9 (Step 023975): Train loss 2.638, Val loss 4.173\n",
            "Ep 9 (Step 023980): Train loss 2.553, Val loss 4.173\n",
            "Ep 9 (Step 023985): Train loss 2.513, Val loss 4.167\n",
            "Ep 9 (Step 023990): Train loss 2.574, Val loss 4.171\n",
            "Ep 9 (Step 023995): Train loss 2.631, Val loss 4.170\n",
            "Ep 9 (Step 024000): Train loss 2.569, Val loss 4.172\n",
            "Ep 9 (Step 024005): Train loss 2.535, Val loss 4.159\n",
            "Ep 9 (Step 024010): Train loss 2.589, Val loss 4.164\n",
            "Ep 9 (Step 024015): Train loss 2.313, Val loss 4.168\n",
            "Ep 9 (Step 024020): Train loss 2.277, Val loss 4.164\n",
            "Ep 9 (Step 024025): Train loss 2.285, Val loss 4.161\n",
            "Ep 9 (Step 024030): Train loss 2.183, Val loss 4.160\n",
            "Ep 9 (Step 024035): Train loss 2.547, Val loss 4.165\n",
            "Ep 9 (Step 024040): Train loss 2.596, Val loss 4.162\n",
            "Ep 9 (Step 024045): Train loss 2.600, Val loss 4.167\n",
            "Ep 9 (Step 024050): Train loss 2.342, Val loss 4.181\n",
            "Ep 9 (Step 024055): Train loss 2.598, Val loss 4.186\n",
            "Ep 9 (Step 024060): Train loss 2.475, Val loss 4.176\n",
            "Ep 9 (Step 024065): Train loss 2.498, Val loss 4.181\n",
            "Ep 9 (Step 024070): Train loss 2.416, Val loss 4.173\n",
            "Ep 9 (Step 024075): Train loss 2.316, Val loss 4.159\n",
            "Ep 9 (Step 024080): Train loss 2.390, Val loss 4.155\n",
            "Ep 9 (Step 024085): Train loss 2.283, Val loss 4.160\n",
            "Ep 9 (Step 024090): Train loss 2.506, Val loss 4.166\n",
            "Ep 9 (Step 024095): Train loss 2.456, Val loss 4.167\n",
            "Ep 9 (Step 024100): Train loss 2.508, Val loss 4.158\n",
            "Ep 9 (Step 024105): Train loss 2.540, Val loss 4.148\n",
            "Ep 9 (Step 024110): Train loss 2.034, Val loss 4.151\n",
            "Ep 9 (Step 024115): Train loss 2.696, Val loss 4.175\n",
            "Ep 9 (Step 024120): Train loss 2.660, Val loss 4.146\n",
            "Ep 9 (Step 024125): Train loss 2.631, Val loss 4.142\n",
            "Ep 9 (Step 024130): Train loss 2.426, Val loss 4.155\n",
            "Ep 9 (Step 024135): Train loss 2.415, Val loss 4.152\n",
            "Ep 9 (Step 024140): Train loss 2.572, Val loss 4.142\n",
            "Ep 9 (Step 024145): Train loss 2.544, Val loss 4.128\n",
            "Ep 9 (Step 024150): Train loss 2.647, Val loss 4.115\n",
            "Ep 9 (Step 024155): Train loss 2.480, Val loss 4.131\n",
            "Ep 9 (Step 024160): Train loss 2.590, Val loss 4.153\n",
            "Ep 9 (Step 024165): Train loss 2.677, Val loss 4.150\n",
            "Ep 9 (Step 024170): Train loss 2.290, Val loss 4.150\n",
            "Ep 9 (Step 024175): Train loss 2.538, Val loss 4.156\n",
            "Ep 9 (Step 024180): Train loss 2.592, Val loss 4.159\n",
            "Ep 9 (Step 024185): Train loss 2.531, Val loss 4.151\n",
            "Ep 9 (Step 024190): Train loss 2.667, Val loss 4.153\n",
            "Ep 9 (Step 024195): Train loss 2.440, Val loss 4.168\n",
            "Ep 9 (Step 024200): Train loss 2.491, Val loss 4.150\n",
            "Ep 9 (Step 024205): Train loss 2.560, Val loss 4.152\n",
            "Ep 9 (Step 024210): Train loss 2.765, Val loss 4.158\n",
            "Ep 9 (Step 024215): Train loss 2.508, Val loss 4.150\n",
            "Ep 9 (Step 024220): Train loss 2.491, Val loss 4.144\n",
            "Ep 9 (Step 024225): Train loss 2.281, Val loss 4.139\n",
            "Ep 9 (Step 024230): Train loss 2.445, Val loss 4.159\n",
            "Ep 9 (Step 024235): Train loss 2.593, Val loss 4.161\n",
            "Ep 9 (Step 024240): Train loss 2.415, Val loss 4.162\n",
            "Ep 9 (Step 024245): Train loss 2.696, Val loss 4.167\n",
            "Ep 9 (Step 024250): Train loss 2.446, Val loss 4.171\n",
            "Ep 9 (Step 024255): Train loss 2.314, Val loss 4.173\n",
            "Ep 9 (Step 024260): Train loss 2.507, Val loss 4.167\n",
            "Ep 9 (Step 024265): Train loss 2.561, Val loss 4.152\n",
            "Ep 9 (Step 024270): Train loss 2.475, Val loss 4.134\n",
            "Ep 9 (Step 024275): Train loss 2.360, Val loss 4.133\n",
            "Ep 9 (Step 024280): Train loss 2.446, Val loss 4.136\n",
            "Ep 9 (Step 024285): Train loss 2.598, Val loss 4.144\n",
            "Ep 9 (Step 024290): Train loss 2.375, Val loss 4.163\n",
            "Ep 9 (Step 024295): Train loss 2.299, Val loss 4.166\n",
            "Ep 9 (Step 024300): Train loss 2.588, Val loss 4.166\n",
            "Ep 9 (Step 024305): Train loss 2.564, Val loss 4.180\n",
            "Ep 9 (Step 024310): Train loss 2.656, Val loss 4.180\n",
            "Ep 9 (Step 024315): Train loss 2.583, Val loss 4.158\n",
            "Ep 9 (Step 024320): Train loss 2.254, Val loss 4.149\n",
            "Ep 9 (Step 024325): Train loss 2.673, Val loss 4.155\n",
            "Ep 9 (Step 024330): Train loss 2.259, Val loss 4.161\n",
            "Ep 9 (Step 024335): Train loss 2.689, Val loss 4.152\n",
            "Ep 9 (Step 024340): Train loss 2.409, Val loss 4.150\n",
            "Ep 9 (Step 024345): Train loss 2.302, Val loss 4.151\n",
            "Ep 9 (Step 024350): Train loss 2.591, Val loss 4.141\n",
            "Ep 9 (Step 024355): Train loss 2.710, Val loss 4.150\n",
            "Ep 9 (Step 024360): Train loss 2.526, Val loss 4.161\n",
            "Ep 9 (Step 024365): Train loss 2.644, Val loss 4.169\n",
            "Ep 9 (Step 024370): Train loss 2.755, Val loss 4.159\n",
            "Ep 9 (Step 024375): Train loss 2.609, Val loss 4.145\n",
            "Ep 9 (Step 024380): Train loss 2.584, Val loss 4.148\n",
            "Ep 9 (Step 024385): Train loss 2.640, Val loss 4.145\n",
            "Ep 9 (Step 024390): Train loss 2.456, Val loss 4.154\n",
            "Ep 9 (Step 024395): Train loss 2.684, Val loss 4.153\n",
            "Ep 9 (Step 024400): Train loss 2.477, Val loss 4.154\n",
            "Ep 9 (Step 024405): Train loss 2.414, Val loss 4.160\n",
            "Ep 9 (Step 024410): Train loss 2.368, Val loss 4.160\n",
            "Ep 9 (Step 024415): Train loss 2.287, Val loss 4.159\n",
            "Ep 9 (Step 024420): Train loss 2.523, Val loss 4.186\n",
            "Ep 9 (Step 024425): Train loss 2.622, Val loss 4.184\n",
            "Ep 9 (Step 024430): Train loss 2.315, Val loss 4.192\n",
            "Ep 9 (Step 024435): Train loss 2.525, Val loss 4.205\n",
            "Ep 9 (Step 024440): Train loss 2.652, Val loss 4.200\n",
            "Ep 9 (Step 024445): Train loss 2.547, Val loss 4.206\n",
            "Ep 9 (Step 024450): Train loss 2.323, Val loss 4.213\n",
            "Ep 9 (Step 024455): Train loss 2.280, Val loss 4.194\n",
            "Ep 9 (Step 024460): Train loss 2.476, Val loss 4.180\n",
            "Ep 9 (Step 024465): Train loss 2.304, Val loss 4.178\n",
            "Ep 9 (Step 024470): Train loss 2.703, Val loss 4.188\n",
            "Ep 9 (Step 024475): Train loss 2.802, Val loss 4.196\n",
            "Ep 9 (Step 024480): Train loss 2.427, Val loss 4.198\n",
            "Ep 9 (Step 024485): Train loss 2.438, Val loss 4.190\n",
            "Ep 9 (Step 024490): Train loss 2.075, Val loss 4.202\n",
            "Ep 9 (Step 024495): Train loss 2.350, Val loss 4.208\n",
            "Ep 9 (Step 024500): Train loss 2.226, Val loss 4.205\n",
            "Ep 9 (Step 024505): Train loss 2.610, Val loss 4.195\n",
            "Ep 9 (Step 024510): Train loss 2.499, Val loss 4.187\n",
            "Ep 9 (Step 024515): Train loss 2.463, Val loss 4.192\n",
            "Ep 9 (Step 024520): Train loss 2.394, Val loss 4.205\n",
            "Ep 9 (Step 024525): Train loss 2.233, Val loss 4.184\n",
            "Ep 9 (Step 024530): Train loss 2.207, Val loss 4.185\n",
            "Ep 9 (Step 024535): Train loss 2.384, Val loss 4.180\n",
            "Ep 9 (Step 024540): Train loss 2.521, Val loss 4.171\n",
            "Ep 9 (Step 024545): Train loss 2.400, Val loss 4.164\n",
            "Ep 9 (Step 024550): Train loss 2.378, Val loss 4.172\n",
            "Ep 9 (Step 024555): Train loss 2.660, Val loss 4.173\n",
            "Ep 9 (Step 024560): Train loss 2.445, Val loss 4.183\n",
            "Ep 9 (Step 024565): Train loss 2.411, Val loss 4.181\n",
            "Ep 9 (Step 024570): Train loss 2.091, Val loss 4.179\n",
            "Ep 9 (Step 024575): Train loss 2.662, Val loss 4.182\n",
            "Ep 9 (Step 024580): Train loss 2.342, Val loss 4.178\n",
            "Ep 9 (Step 024585): Train loss 2.307, Val loss 4.173\n",
            "Ep 9 (Step 024590): Train loss 2.488, Val loss 4.175\n",
            "Ep 9 (Step 024595): Train loss 2.300, Val loss 4.178\n",
            "Ep 9 (Step 024600): Train loss 2.608, Val loss 4.180\n",
            "Ep 9 (Step 024605): Train loss 2.572, Val loss 4.183\n",
            "Ep 9 (Step 024610): Train loss 2.584, Val loss 4.173\n",
            "Ep 9 (Step 024615): Train loss 2.581, Val loss 4.177\n",
            "Ep 9 (Step 024620): Train loss 2.569, Val loss 4.174\n",
            "Ep 9 (Step 024625): Train loss 2.576, Val loss 4.174\n",
            "Ep 9 (Step 024630): Train loss 2.564, Val loss 4.181\n",
            "Ep 9 (Step 024635): Train loss 2.638, Val loss 4.162\n",
            "Ep 9 (Step 024640): Train loss 2.455, Val loss 4.157\n",
            "Ep 9 (Step 024645): Train loss 2.508, Val loss 4.146\n",
            "Ep 9 (Step 024650): Train loss 2.349, Val loss 4.142\n",
            "Ep 9 (Step 024655): Train loss 2.434, Val loss 4.135\n",
            "Ep 9 (Step 024660): Train loss 2.317, Val loss 4.139\n",
            "Ep 9 (Step 024665): Train loss 2.636, Val loss 4.143\n",
            "Ep 9 (Step 024670): Train loss 2.521, Val loss 4.154\n",
            "Ep 9 (Step 024675): Train loss 2.383, Val loss 4.156\n",
            "Ep 9 (Step 024680): Train loss 2.359, Val loss 4.142\n",
            "Ep 9 (Step 024685): Train loss 2.573, Val loss 4.147\n",
            "Ep 9 (Step 024690): Train loss 2.345, Val loss 4.140\n",
            "Ep 9 (Step 024695): Train loss 2.649, Val loss 4.135\n",
            "Ep 9 (Step 024700): Train loss 2.540, Val loss 4.153\n",
            "Ep 9 (Step 024705): Train loss 2.179, Val loss 4.163\n",
            "Ep 9 (Step 024710): Train loss 2.345, Val loss 4.170\n",
            "Ep 9 (Step 024715): Train loss 2.585, Val loss 4.187\n",
            "Ep 9 (Step 024720): Train loss 2.477, Val loss 4.196\n",
            "Ep 9 (Step 024725): Train loss 2.448, Val loss 4.196\n",
            "Ep 9 (Step 024730): Train loss 2.563, Val loss 4.181\n",
            "Ep 9 (Step 024735): Train loss 2.406, Val loss 4.174\n",
            "Ep 9 (Step 024740): Train loss 2.455, Val loss 4.169\n",
            "Ep 9 (Step 024745): Train loss 2.300, Val loss 4.185\n",
            "Ep 9 (Step 024750): Train loss 2.585, Val loss 4.181\n",
            "Ep 9 (Step 024755): Train loss 2.243, Val loss 4.167\n",
            "Ep 9 (Step 024760): Train loss 2.640, Val loss 4.167\n",
            "Ep 9 (Step 024765): Train loss 2.182, Val loss 4.180\n",
            "Ep 9 (Step 024770): Train loss 2.617, Val loss 4.187\n",
            "Ep 9 (Step 024775): Train loss 2.297, Val loss 4.187\n",
            "Ep 9 (Step 024780): Train loss 2.574, Val loss 4.173\n",
            "Ep 9 (Step 024785): Train loss 2.465, Val loss 4.166\n",
            "Ep 9 (Step 024790): Train loss 2.276, Val loss 4.161\n",
            "Ep 9 (Step 024795): Train loss 2.724, Val loss 4.171\n",
            "Ep 9 (Step 024800): Train loss 2.603, Val loss 4.166\n",
            "Ep 9 (Step 024805): Train loss 2.261, Val loss 4.159\n",
            "Ep 9 (Step 024810): Train loss 2.345, Val loss 4.173\n",
            "Ep 9 (Step 024815): Train loss 2.377, Val loss 4.170\n",
            "Ep 9 (Step 024820): Train loss 2.220, Val loss 4.160\n",
            "Ep 9 (Step 024825): Train loss 2.659, Val loss 4.160\n",
            "Ep 9 (Step 024830): Train loss 2.575, Val loss 4.164\n",
            "Ep 9 (Step 024835): Train loss 2.222, Val loss 4.166\n",
            "Ep 9 (Step 024840): Train loss 2.102, Val loss 4.160\n",
            "Ep 9 (Step 024845): Train loss 2.290, Val loss 4.162\n",
            "Ep 9 (Step 024850): Train loss 2.538, Val loss 4.161\n",
            "Ep 9 (Step 024855): Train loss 2.357, Val loss 4.157\n",
            "Ep 9 (Step 024860): Train loss 2.795, Val loss 4.156\n",
            "Ep 9 (Step 024865): Train loss 2.513, Val loss 4.145\n",
            "Ep 9 (Step 024870): Train loss 2.239, Val loss 4.145\n",
            "Ep 9 (Step 024875): Train loss 2.813, Val loss 4.155\n",
            "Ep 9 (Step 024880): Train loss 2.202, Val loss 4.170\n",
            "Ep 9 (Step 024885): Train loss 2.463, Val loss 4.164\n",
            "Ep 9 (Step 024890): Train loss 2.508, Val loss 4.159\n",
            "Ep 9 (Step 024895): Train loss 2.354, Val loss 4.162\n",
            "Ep 9 (Step 024900): Train loss 2.316, Val loss 4.166\n",
            "Ep 9 (Step 024905): Train loss 2.288, Val loss 4.153\n",
            "Ep 9 (Step 024910): Train loss 2.859, Val loss 4.154\n",
            "Ep 9 (Step 024915): Train loss 2.422, Val loss 4.146\n",
            "Ep 9 (Step 024920): Train loss 2.419, Val loss 4.160\n",
            "Ep 9 (Step 024925): Train loss 2.512, Val loss 4.170\n",
            "Ep 9 (Step 024930): Train loss 2.165, Val loss 4.183\n",
            "Ep 9 (Step 024935): Train loss 2.473, Val loss 4.179\n",
            "Ep 9 (Step 024940): Train loss 2.508, Val loss 4.169\n",
            "Ep 9 (Step 024945): Train loss 2.450, Val loss 4.166\n",
            "Ep 9 (Step 024950): Train loss 2.415, Val loss 4.156\n",
            "Ep 9 (Step 024955): Train loss 2.489, Val loss 4.162\n",
            "Ep 9 (Step 024960): Train loss 2.238, Val loss 4.162\n",
            "Ep 9 (Step 024965): Train loss 2.379, Val loss 4.164\n",
            "Ep 9 (Step 024970): Train loss 2.327, Val loss 4.166\n",
            "Ep 9 (Step 024975): Train loss 2.217, Val loss 4.172\n",
            "Ep 9 (Step 024980): Train loss 2.413, Val loss 4.178\n",
            "Ep 9 (Step 024985): Train loss 2.044, Val loss 4.171\n",
            "Ep 9 (Step 024990): Train loss 2.352, Val loss 4.162\n",
            "Ep 9 (Step 024995): Train loss 2.319, Val loss 4.157\n",
            "Ep 9 (Step 025000): Train loss 2.766, Val loss 4.167\n",
            "Ep 9 (Step 025005): Train loss 2.284, Val loss 4.164\n",
            "Ep 9 (Step 025010): Train loss 2.200, Val loss 4.147\n",
            "Ep 9 (Step 025015): Train loss 2.665, Val loss 4.144\n",
            "Ep 9 (Step 025020): Train loss 2.700, Val loss 4.151\n",
            "Ep 9 (Step 025025): Train loss 2.422, Val loss 4.168\n",
            "Ep 9 (Step 025030): Train loss 2.251, Val loss 4.173\n",
            "Ep 9 (Step 025035): Train loss 2.673, Val loss 4.165\n",
            "Ep 9 (Step 025040): Train loss 2.469, Val loss 4.157\n",
            "Ep 9 (Step 025045): Train loss 2.366, Val loss 4.152\n",
            "Ep 9 (Step 025050): Train loss 2.412, Val loss 4.163\n",
            "Ep 9 (Step 025055): Train loss 2.124, Val loss 4.171\n",
            "Ep 9 (Step 025060): Train loss 2.441, Val loss 4.181\n",
            "Ep 9 (Step 025065): Train loss 2.557, Val loss 4.180\n",
            "Ep 9 (Step 025070): Train loss 2.453, Val loss 4.160\n",
            "Ep 9 (Step 025075): Train loss 2.410, Val loss 4.159\n",
            "Ep 9 (Step 025080): Train loss 2.735, Val loss 4.184\n",
            "Ep 9 (Step 025085): Train loss 2.366, Val loss 4.191\n",
            "Ep 9 (Step 025090): Train loss 2.409, Val loss 4.178\n",
            "Ep 9 (Step 025095): Train loss 2.429, Val loss 4.175\n",
            "Ep 9 (Step 025100): Train loss 2.535, Val loss 4.171\n",
            "Ep 9 (Step 025105): Train loss 2.390, Val loss 4.173\n",
            "Ep 9 (Step 025110): Train loss 2.442, Val loss 4.163\n",
            "Ep 9 (Step 025115): Train loss 2.512, Val loss 4.157\n",
            "Ep 9 (Step 025120): Train loss 2.590, Val loss 4.151\n",
            "Ep 9 (Step 025125): Train loss 2.404, Val loss 4.159\n",
            "Ep 9 (Step 025130): Train loss 2.697, Val loss 4.165\n",
            "Ep 9 (Step 025135): Train loss 2.378, Val loss 4.167\n",
            "Ep 9 (Step 025140): Train loss 2.543, Val loss 4.166\n",
            "Ep 9 (Step 025145): Train loss 2.327, Val loss 4.164\n",
            "Ep 9 (Step 025150): Train loss 2.501, Val loss 4.171\n",
            "Ep 9 (Step 025155): Train loss 2.771, Val loss 4.186\n",
            "Ep 9 (Step 025160): Train loss 2.226, Val loss 4.182\n",
            "Ep 9 (Step 025165): Train loss 2.116, Val loss 4.178\n",
            "Ep 9 (Step 025170): Train loss 2.363, Val loss 4.181\n",
            "Ep 9 (Step 025175): Train loss 2.505, Val loss 4.185\n",
            "Ep 9 (Step 025180): Train loss 2.297, Val loss 4.189\n",
            "Ep 9 (Step 025185): Train loss 2.431, Val loss 4.193\n",
            "Ep 9 (Step 025190): Train loss 2.432, Val loss 4.204\n",
            "Ep 9 (Step 025195): Train loss 2.572, Val loss 4.206\n",
            "Ep 9 (Step 025200): Train loss 2.342, Val loss 4.202\n",
            "Ep 9 (Step 025205): Train loss 2.370, Val loss 4.191\n",
            "Ep 9 (Step 025210): Train loss 2.515, Val loss 4.190\n",
            "Ep 9 (Step 025215): Train loss 2.449, Val loss 4.193\n",
            "Ep 9 (Step 025220): Train loss 2.792, Val loss 4.200\n",
            "Ep 9 (Step 025225): Train loss 2.350, Val loss 4.188\n",
            "Ep 9 (Step 025230): Train loss 2.417, Val loss 4.176\n",
            "Ep 9 (Step 025235): Train loss 2.393, Val loss 4.179\n",
            "Ep 9 (Step 025240): Train loss 2.649, Val loss 4.169\n",
            "Ep 9 (Step 025245): Train loss 2.302, Val loss 4.172\n",
            "Ep 9 (Step 025250): Train loss 2.511, Val loss 4.197\n",
            "Ep 9 (Step 025255): Train loss 2.458, Val loss 4.206\n",
            "Ep 9 (Step 025260): Train loss 2.339, Val loss 4.198\n",
            "Ep 9 (Step 025265): Train loss 2.456, Val loss 4.198\n",
            "Ep 9 (Step 025270): Train loss 2.368, Val loss 4.199\n",
            "Ep 9 (Step 025275): Train loss 2.537, Val loss 4.200\n",
            "Ep 9 (Step 025280): Train loss 2.511, Val loss 4.201\n",
            "Ep 9 (Step 025285): Train loss 2.246, Val loss 4.194\n",
            "Ep 9 (Step 025290): Train loss 2.672, Val loss 4.196\n",
            "Ep 9 (Step 025295): Train loss 2.555, Val loss 4.211\n",
            "Ep 9 (Step 025300): Train loss 2.722, Val loss 4.197\n",
            "Ep 9 (Step 025305): Train loss 2.027, Val loss 4.186\n",
            "Ep 9 (Step 025310): Train loss 2.413, Val loss 4.187\n",
            "Ep 9 (Step 025315): Train loss 2.349, Val loss 4.202\n",
            "Ep 9 (Step 025320): Train loss 2.400, Val loss 4.201\n",
            "Ep 9 (Step 025325): Train loss 2.697, Val loss 4.198\n",
            "Ep 9 (Step 025330): Train loss 2.175, Val loss 4.191\n",
            "Ep 9 (Step 025335): Train loss 2.148, Val loss 4.181\n",
            "Ep 9 (Step 025340): Train loss 2.314, Val loss 4.175\n",
            "Ep 9 (Step 025345): Train loss 2.382, Val loss 4.186\n",
            "Ep 9 (Step 025350): Train loss 2.608, Val loss 4.202\n",
            "Ep 9 (Step 025355): Train loss 2.359, Val loss 4.201\n",
            "Ep 9 (Step 025360): Train loss 2.611, Val loss 4.199\n",
            "Ep 9 (Step 025365): Train loss 2.275, Val loss 4.202\n",
            "Ep 9 (Step 025370): Train loss 2.441, Val loss 4.211\n",
            "Ep 9 (Step 025375): Train loss 2.550, Val loss 4.209\n",
            "Ep 9 (Step 025380): Train loss 2.543, Val loss 4.208\n",
            "Ep 9 (Step 025385): Train loss 2.727, Val loss 4.204\n",
            "Ep 9 (Step 025390): Train loss 2.588, Val loss 4.198\n",
            "Ep 9 (Step 025395): Train loss 2.387, Val loss 4.195\n",
            "Ep 9 (Step 025400): Train loss 2.149, Val loss 4.202\n",
            "Ep 9 (Step 025405): Train loss 2.248, Val loss 4.182\n",
            "Ep 9 (Step 025410): Train loss 2.495, Val loss 4.183\n",
            "Ep 9 (Step 025415): Train loss 2.600, Val loss 4.186\n",
            "Ep 9 (Step 025420): Train loss 2.195, Val loss 4.179\n",
            "Ep 9 (Step 025425): Train loss 2.236, Val loss 4.181\n",
            "Ep 9 (Step 025430): Train loss 2.485, Val loss 4.198\n",
            "Ep 9 (Step 025435): Train loss 2.523, Val loss 4.204\n",
            "Ep 9 (Step 025440): Train loss 2.391, Val loss 4.204\n",
            "Ep 9 (Step 025445): Train loss 2.524, Val loss 4.196\n",
            "Ep 9 (Step 025450): Train loss 2.222, Val loss 4.196\n",
            "Ep 9 (Step 025455): Train loss 2.385, Val loss 4.194\n",
            "Ep 9 (Step 025460): Train loss 2.317, Val loss 4.197\n",
            "Ep 9 (Step 025465): Train loss 2.293, Val loss 4.187\n",
            "Ep 9 (Step 025470): Train loss 2.468, Val loss 4.183\n",
            "Ep 9 (Step 025475): Train loss 2.381, Val loss 4.178\n",
            "Ep 9 (Step 025480): Train loss 2.481, Val loss 4.184\n",
            "Ep 9 (Step 025485): Train loss 2.519, Val loss 4.173\n",
            "Ep 9 (Step 025490): Train loss 2.333, Val loss 4.155\n",
            "Ep 9 (Step 025495): Train loss 2.370, Val loss 4.146\n",
            "Ep 9 (Step 025500): Train loss 2.513, Val loss 4.152\n",
            "Ep 9 (Step 025505): Train loss 2.424, Val loss 4.152\n",
            "Ep 9 (Step 025510): Train loss 2.464, Val loss 4.143\n",
            "Ep 9 (Step 025515): Train loss 2.640, Val loss 4.138\n",
            "Ep 9 (Step 025520): Train loss 2.213, Val loss 4.161\n",
            "Ep 9 (Step 025525): Train loss 2.515, Val loss 4.162\n",
            "Ep 9 (Step 025530): Train loss 2.286, Val loss 4.156\n",
            "Ep 9 (Step 025535): Train loss 2.356, Val loss 4.150\n",
            "Ep 9 (Step 025540): Train loss 2.168, Val loss 4.153\n",
            "Ep 9 (Step 025545): Train loss 2.084, Val loss 4.161\n",
            "Ep 9 (Step 025550): Train loss 2.114, Val loss 4.158\n",
            "Ep 9 (Step 025555): Train loss 2.403, Val loss 4.170\n",
            "Ep 9 (Step 025560): Train loss 2.454, Val loss 4.161\n",
            "Ep 9 (Step 025565): Train loss 2.327, Val loss 4.154\n",
            "Ep 9 (Step 025570): Train loss 2.264, Val loss 4.161\n",
            "Ep 9 (Step 025575): Train loss 2.460, Val loss 4.157\n",
            "Ep 9 (Step 025580): Train loss 2.599, Val loss 4.168\n",
            "Ep 9 (Step 025585): Train loss 2.153, Val loss 4.165\n",
            "Ep 9 (Step 025590): Train loss 2.814, Val loss 4.166\n",
            "Ep 9 (Step 025595): Train loss 2.336, Val loss 4.174\n",
            "Ep 9 (Step 025600): Train loss 2.287, Val loss 4.170\n",
            "Ep 9 (Step 025605): Train loss 2.276, Val loss 4.181\n",
            "Ep 9 (Step 025610): Train loss 2.314, Val loss 4.169\n",
            "Ep 9 (Step 025615): Train loss 2.076, Val loss 4.150\n",
            "Ep 9 (Step 025620): Train loss 2.394, Val loss 4.165\n",
            "Ep 9 (Step 025625): Train loss 2.400, Val loss 4.183\n",
            "Ep 9 (Step 025630): Train loss 2.824, Val loss 4.172\n",
            "Ep 9 (Step 025635): Train loss 2.239, Val loss 4.172\n",
            "Ep 9 (Step 025640): Train loss 2.135, Val loss 4.170\n",
            "Ep 9 (Step 025645): Train loss 2.369, Val loss 4.167\n",
            "Ep 9 (Step 025650): Train loss 2.437, Val loss 4.158\n",
            "Ep 9 (Step 025655): Train loss 2.415, Val loss 4.154\n",
            "Ep 9 (Step 025660): Train loss 2.262, Val loss 4.159\n",
            "Ep 9 (Step 025665): Train loss 2.601, Val loss 4.156\n",
            "Ep 9 (Step 025670): Train loss 2.314, Val loss 4.155\n",
            "Ep 9 (Step 025675): Train loss 1.966, Val loss 4.148\n",
            "Ep 9 (Step 025680): Train loss 2.417, Val loss 4.147\n",
            "Ep 9 (Step 025685): Train loss 2.148, Val loss 4.153\n",
            "Ep 9 (Step 025690): Train loss 2.541, Val loss 4.155\n",
            "Ep 9 (Step 025695): Train loss 2.381, Val loss 4.147\n",
            "Ep 9 (Step 025700): Train loss 2.666, Val loss 4.130\n",
            "Ep 9 (Step 025705): Train loss 2.395, Val loss 4.111\n",
            "Ep 9 (Step 025710): Train loss 2.305, Val loss 4.110\n",
            "Ep 9 (Step 025715): Train loss 2.417, Val loss 4.107\n",
            "Ep 9 (Step 025720): Train loss 2.468, Val loss 4.127\n",
            "Ep 9 (Step 025725): Train loss 2.486, Val loss 4.140\n",
            "Ep 9 (Step 025730): Train loss 2.483, Val loss 4.123\n",
            "Ep 9 (Step 025735): Train loss 2.020, Val loss 4.121\n",
            "Ep 9 (Step 025740): Train loss 2.188, Val loss 4.141\n",
            "Ep 9 (Step 025745): Train loss 2.507, Val loss 4.142\n",
            "Ep 9 (Step 025750): Train loss 2.548, Val loss 4.131\n",
            "Ep 9 (Step 025755): Train loss 2.676, Val loss 4.128\n",
            "Ep 9 (Step 025760): Train loss 2.299, Val loss 4.130\n",
            "Ep 9 (Step 025765): Train loss 2.577, Val loss 4.132\n",
            "Ep 9 (Step 025770): Train loss 2.243, Val loss 4.131\n",
            "Ep 9 (Step 025775): Train loss 2.240, Val loss 4.130\n",
            "Ep 9 (Step 025780): Train loss 2.449, Val loss 4.131\n",
            "Ep 9 (Step 025785): Train loss 2.320, Val loss 4.135\n",
            "Ep 9 (Step 025790): Train loss 2.328, Val loss 4.119\n",
            "Ep 9 (Step 025795): Train loss 2.615, Val loss 4.113\n",
            "Ep 9 (Step 025800): Train loss 2.403, Val loss 4.111\n",
            "Ep 9 (Step 025805): Train loss 2.340, Val loss 4.108\n",
            "Ep 9 (Step 025810): Train loss 2.438, Val loss 4.103\n",
            "Ep 9 (Step 025815): Train loss 2.042, Val loss 4.103\n",
            "Ep 9 (Step 025820): Train loss 2.472, Val loss 4.114\n",
            "Ep 9 (Step 025825): Train loss 2.298, Val loss 4.121\n",
            "Ep 9 (Step 025830): Train loss 2.443, Val loss 4.120\n",
            "Ep 9 (Step 025835): Train loss 2.357, Val loss 4.113\n",
            "Ep 9 (Step 025840): Train loss 2.515, Val loss 4.114\n",
            "Ep 9 (Step 025845): Train loss 2.329, Val loss 4.134\n",
            "Ep 9 (Step 025850): Train loss 2.259, Val loss 4.149\n",
            "Ep 9 (Step 025855): Train loss 2.528, Val loss 4.151\n",
            "Ep 9 (Step 025860): Train loss 2.222, Val loss 4.130\n",
            "Ep 9 (Step 025865): Train loss 2.514, Val loss 4.109\n",
            "Ep 9 (Step 025870): Train loss 2.535, Val loss 4.105\n",
            "Ep 9 (Step 025875): Train loss 2.315, Val loss 4.110\n",
            "Ep 9 (Step 025880): Train loss 2.447, Val loss 4.115\n",
            "Ep 9 (Step 025885): Train loss 2.327, Val loss 4.114\n",
            "Ep 9 (Step 025890): Train loss 2.257, Val loss 4.111\n",
            "Ep 9 (Step 025895): Train loss 2.101, Val loss 4.110\n",
            "Ep 9 (Step 025900): Train loss 2.413, Val loss 4.110\n",
            "Ep 9 (Step 025905): Train loss 2.318, Val loss 4.114\n",
            "Ep 9 (Step 025910): Train loss 2.574, Val loss 4.109\n",
            "Ep 9 (Step 025915): Train loss 2.776, Val loss 4.095\n",
            "Ep 9 (Step 025920): Train loss 2.309, Val loss 4.106\n",
            "Ep 9 (Step 025925): Train loss 2.373, Val loss 4.117\n",
            "Ep 9 (Step 025930): Train loss 2.405, Val loss 4.121\n",
            "Ep 9 (Step 025935): Train loss 2.491, Val loss 4.113\n",
            "Ep 9 (Step 025940): Train loss 2.232, Val loss 4.106\n",
            "Ep 9 (Step 025945): Train loss 2.391, Val loss 4.100\n",
            "Ep 9 (Step 025950): Train loss 2.360, Val loss 4.108\n",
            "Ep 9 (Step 025955): Train loss 2.325, Val loss 4.118\n",
            "Ep 9 (Step 025960): Train loss 2.391, Val loss 4.114\n",
            "Ep 9 (Step 025965): Train loss 2.144, Val loss 4.127\n",
            "Ep 9 (Step 025970): Train loss 2.525, Val loss 4.116\n",
            "Ep 9 (Step 025975): Train loss 2.464, Val loss 4.106\n",
            "Ep 9 (Step 025980): Train loss 2.376, Val loss 4.104\n",
            "Ep 9 (Step 025985): Train loss 2.412, Val loss 4.106\n",
            "Ep 9 (Step 025990): Train loss 2.364, Val loss 4.118\n",
            "Ep 9 (Step 025995): Train loss 2.188, Val loss 4.118\n",
            "Ep 9 (Step 026000): Train loss 2.346, Val loss 4.120\n",
            "Ep 9 (Step 026005): Train loss 2.376, Val loss 4.113\n",
            "Ep 9 (Step 026010): Train loss 2.581, Val loss 4.111\n",
            "Ep 9 (Step 026015): Train loss 2.270, Val loss 4.119\n",
            "Ep 9 (Step 026020): Train loss 2.415, Val loss 4.120\n",
            "Ep 9 (Step 026025): Train loss 2.407, Val loss 4.132\n",
            "Ep 9 (Step 026030): Train loss 2.387, Val loss 4.136\n",
            "Ep 9 (Step 026035): Train loss 2.520, Val loss 4.135\n",
            "Ep 9 (Step 026040): Train loss 2.265, Val loss 4.147\n",
            "Ep 9 (Step 026045): Train loss 2.471, Val loss 4.140\n",
            "Ep 9 (Step 026050): Train loss 2.407, Val loss 4.129\n",
            "Ep 9 (Step 026055): Train loss 2.475, Val loss 4.123\n",
            "Ep 9 (Step 026060): Train loss 2.443, Val loss 4.119\n",
            "Ep 9 (Step 026065): Train loss 2.421, Val loss 4.130\n",
            "Ep 9 (Step 026070): Train loss 1.999, Val loss 4.137\n",
            "Ep 9 (Step 026075): Train loss 2.324, Val loss 4.141\n",
            "Ep 9 (Step 026080): Train loss 2.326, Val loss 4.158\n",
            "Ep 9 (Step 026085): Train loss 2.561, Val loss 4.152\n",
            "Ep 9 (Step 026090): Train loss 2.571, Val loss 4.142\n",
            "Ep 9 (Step 026095): Train loss 2.284, Val loss 4.148\n",
            "Ep 9 (Step 026100): Train loss 2.342, Val loss 4.145\n",
            "Ep 9 (Step 026105): Train loss 2.516, Val loss 4.153\n",
            "Ep 9 (Step 026110): Train loss 2.442, Val loss 4.152\n",
            "Ep 9 (Step 026115): Train loss 2.310, Val loss 4.146\n",
            "Ep 9 (Step 026120): Train loss 2.542, Val loss 4.155\n",
            "Ep 9 (Step 026125): Train loss 2.408, Val loss 4.167\n",
            "Ep 9 (Step 026130): Train loss 2.494, Val loss 4.173\n",
            "Ep 9 (Step 026135): Train loss 2.242, Val loss 4.159\n",
            "Ep 9 (Step 026140): Train loss 2.448, Val loss 4.157\n",
            "Ep 9 (Step 026145): Train loss 2.235, Val loss 4.158\n",
            "Ep 9 (Step 026150): Train loss 2.391, Val loss 4.145\n",
            "Ep 9 (Step 026155): Train loss 2.275, Val loss 4.144\n",
            "Ep 9 (Step 026160): Train loss 2.456, Val loss 4.135\n",
            "Ep 9 (Step 026165): Train loss 2.362, Val loss 4.116\n",
            "Ep 9 (Step 026170): Train loss 2.420, Val loss 4.106\n",
            "Ep 9 (Step 026175): Train loss 2.381, Val loss 4.111\n",
            "Ep 9 (Step 026180): Train loss 2.461, Val loss 4.107\n",
            "Ep 9 (Step 026185): Train loss 2.242, Val loss 4.107\n",
            "Ep 9 (Step 026190): Train loss 2.689, Val loss 4.112\n",
            "Ep 9 (Step 026195): Train loss 2.382, Val loss 4.128\n",
            "Ep 9 (Step 026200): Train loss 2.351, Val loss 4.125\n",
            "Ep 9 (Step 026205): Train loss 2.218, Val loss 4.128\n",
            "Ep 9 (Step 026210): Train loss 2.608, Val loss 4.128\n",
            "Ep 9 (Step 026215): Train loss 2.452, Val loss 4.125\n",
            "Ep 9 (Step 026220): Train loss 1.954, Val loss 4.119\n",
            "Ep 9 (Step 026225): Train loss 2.319, Val loss 4.122\n",
            "Ep 9 (Step 026230): Train loss 2.356, Val loss 4.122\n",
            "Ep 9 (Step 026235): Train loss 2.438, Val loss 4.135\n",
            "Ep 9 (Step 026240): Train loss 2.626, Val loss 4.150\n",
            "Ep 9 (Step 026245): Train loss 2.359, Val loss 4.148\n",
            "Ep 9 (Step 026250): Train loss 2.114, Val loss 4.136\n",
            "Ep 9 (Step 026255): Train loss 2.401, Val loss 4.137\n",
            "Ep 9 (Step 026260): Train loss 2.374, Val loss 4.125\n",
            "Ep 9 (Step 026265): Train loss 2.304, Val loss 4.126\n",
            "Ep 9 (Step 026270): Train loss 2.465, Val loss 4.133\n",
            "Ep 9 (Step 026275): Train loss 2.513, Val loss 4.125\n",
            "Ep 9 (Step 026280): Train loss 2.370, Val loss 4.129\n",
            "Ep 9 (Step 026285): Train loss 2.364, Val loss 4.152\n",
            "Ep 9 (Step 026290): Train loss 2.502, Val loss 4.161\n",
            "Ep 9 (Step 026295): Train loss 2.326, Val loss 4.143\n",
            "Ep 9 (Step 026300): Train loss 2.249, Val loss 4.132\n",
            "Ep 9 (Step 026305): Train loss 2.561, Val loss 4.138\n",
            "Ep 9 (Step 026310): Train loss 2.397, Val loss 4.152\n",
            "Ep 9 (Step 026315): Train loss 2.523, Val loss 4.138\n",
            "Ep 9 (Step 026320): Train loss 2.390, Val loss 4.133\n",
            "Ep 9 (Step 026325): Train loss 2.330, Val loss 4.129\n",
            "Ep 9 (Step 026330): Train loss 2.240, Val loss 4.134\n",
            "Ep 9 (Step 026335): Train loss 2.570, Val loss 4.153\n",
            "Ep 9 (Step 026340): Train loss 2.517, Val loss 4.161\n",
            "Ep 9 (Step 026345): Train loss 2.381, Val loss 4.160\n",
            "Ep 9 (Step 026350): Train loss 2.657, Val loss 4.142\n",
            "Ep 9 (Step 026355): Train loss 2.280, Val loss 4.131\n",
            "Ep 9 (Step 026360): Train loss 2.138, Val loss 4.148\n",
            "Ep 9 (Step 026365): Train loss 2.341, Val loss 4.150\n",
            "Ep 9 (Step 026370): Train loss 2.315, Val loss 4.139\n",
            "Ep 9 (Step 026375): Train loss 2.499, Val loss 4.144\n",
            "Ep 9 (Step 026380): Train loss 2.248, Val loss 4.139\n",
            "Ep 9 (Step 026385): Train loss 2.327, Val loss 4.147\n",
            "Ep 9 (Step 026390): Train loss 2.099, Val loss 4.148\n",
            "Ep 9 (Step 026395): Train loss 2.431, Val loss 4.133\n",
            "Ep 9 (Step 026400): Train loss 2.449, Val loss 4.123\n",
            "Ep 9 (Step 026405): Train loss 2.481, Val loss 4.120\n",
            "Ep 9 (Step 026410): Train loss 2.413, Val loss 4.121\n",
            "Ep 9 (Step 026415): Train loss 2.293, Val loss 4.142\n",
            "Ep 9 (Step 026420): Train loss 2.281, Val loss 4.141\n",
            "Ep 9 (Step 026425): Train loss 2.501, Val loss 4.129\n",
            "Ep 9 (Step 026430): Train loss 2.270, Val loss 4.129\n",
            "Ep 9 (Step 026435): Train loss 2.330, Val loss 4.132\n",
            "Ep 9 (Step 026440): Train loss 2.340, Val loss 4.118\n",
            "Ep 9 (Step 026445): Train loss 2.244, Val loss 4.114\n",
            "Ep 9 (Step 026450): Train loss 2.371, Val loss 4.115\n",
            "Ep 9 (Step 026455): Train loss 2.321, Val loss 4.108\n",
            "Ep 9 (Step 026460): Train loss 2.416, Val loss 4.105\n",
            "Ep 9 (Step 026465): Train loss 2.428, Val loss 4.103\n",
            "Ep 9 (Step 026470): Train loss 2.062, Val loss 4.108\n",
            "Ep 9 (Step 026475): Train loss 2.184, Val loss 4.110\n",
            "Ep 9 (Step 026480): Train loss 2.428, Val loss 4.121\n",
            "Ep 9 (Step 026485): Train loss 2.327, Val loss 4.111\n",
            "Ep 9 (Step 026490): Train loss 2.182, Val loss 4.104\n",
            "Ep 9 (Step 026495): Train loss 2.413, Val loss 4.109\n",
            "Ep 9 (Step 026500): Train loss 2.323, Val loss 4.109\n",
            "Ep 9 (Step 026505): Train loss 2.282, Val loss 4.111\n",
            "Ep 9 (Step 026510): Train loss 2.266, Val loss 4.108\n",
            "Ep 9 (Step 026515): Train loss 2.565, Val loss 4.094\n",
            "Ep 9 (Step 026520): Train loss 2.311, Val loss 4.093\n",
            "Ep 9 (Step 026525): Train loss 2.242, Val loss 4.094\n",
            "Ep 9 (Step 026530): Train loss 2.366, Val loss 4.101\n",
            "Ep 9 (Step 026535): Train loss 2.291, Val loss 4.102\n",
            "Ep 9 (Step 026540): Train loss 2.428, Val loss 4.102\n",
            "Ep 9 (Step 026545): Train loss 2.254, Val loss 4.101\n",
            "Ep 9 (Step 026550): Train loss 2.460, Val loss 4.110\n",
            "Ep 9 (Step 026555): Train loss 2.412, Val loss 4.094\n",
            "Ep 9 (Step 026560): Train loss 2.237, Val loss 4.089\n",
            "Ep 9 (Step 026565): Train loss 2.521, Val loss 4.084\n",
            "Ep 9 (Step 026570): Train loss 2.233, Val loss 4.102\n",
            "Ep 9 (Step 026575): Train loss 2.538, Val loss 4.112\n",
            "Ep 9 (Step 026580): Train loss 2.576, Val loss 4.116\n",
            "Ep 9 (Step 026585): Train loss 2.181, Val loss 4.102\n",
            "Ep 9 (Step 026590): Train loss 2.619, Val loss 4.101\n",
            "Ep 9 (Step 026595): Train loss 2.443, Val loss 4.115\n",
            "Ep 9 (Step 026600): Train loss 2.362, Val loss 4.109\n",
            "Ep 9 (Step 026605): Train loss 2.303, Val loss 4.089\n",
            "Ep 9 (Step 026610): Train loss 2.351, Val loss 4.088\n",
            "Ep 9 (Step 026615): Train loss 2.364, Val loss 4.099\n",
            "Ep 9 (Step 026620): Train loss 2.283, Val loss 4.096\n",
            "Ep 9 (Step 026625): Train loss 2.647, Val loss 4.095\n",
            "Ep 9 (Step 026630): Train loss 2.281, Val loss 4.093\n",
            "Ep 9 (Step 026635): Train loss 2.501, Val loss 4.103\n",
            "Ep 9 (Step 026640): Train loss 2.481, Val loss 4.099\n",
            "Ep 9 (Step 026645): Train loss 2.269, Val loss 4.092\n",
            "Ep 9 (Step 026650): Train loss 2.054, Val loss 4.106\n",
            "Ep 9 (Step 026655): Train loss 2.331, Val loss 4.121\n",
            "Ep 9 (Step 026660): Train loss 2.226, Val loss 4.123\n",
            "Ep 9 (Step 026665): Train loss 1.809, Val loss 4.114\n",
            "Ep 9 (Step 026670): Train loss 2.619, Val loss 4.126\n",
            "Ep 9 (Step 026675): Train loss 2.190, Val loss 4.129\n",
            "Ep 9 (Step 026680): Train loss 2.312, Val loss 4.129\n",
            "Ep 9 (Step 026685): Train loss 2.225, Val loss 4.127\n",
            "Ep 9 (Step 026690): Train loss 2.353, Val loss 4.117\n",
            "Ep 9 (Step 026695): Train loss 2.286, Val loss 4.108\n",
            "Ep 9 (Step 026700): Train loss 2.305, Val loss 4.104\n",
            "Ep 9 (Step 026705): Train loss 2.210, Val loss 4.105\n",
            "Ep 9 (Step 026710): Train loss 2.485, Val loss 4.101\n",
            "Ep 9 (Step 026715): Train loss 2.168, Val loss 4.108\n",
            "Ep 9 (Step 026720): Train loss 2.342, Val loss 4.107\n",
            "Ep 9 (Step 026725): Train loss 2.283, Val loss 4.095\n",
            "Ep 9 (Step 026730): Train loss 2.348, Val loss 4.103\n",
            "Ep 9 (Step 026735): Train loss 2.445, Val loss 4.099\n",
            "Ep 9 (Step 026740): Train loss 2.340, Val loss 4.074\n",
            "Ep 9 (Step 026745): Train loss 2.373, Val loss 4.081\n",
            "Ep 9 (Step 026750): Train loss 2.232, Val loss 4.099\n",
            "Ep 9 (Step 026755): Train loss 2.470, Val loss 4.094\n",
            "Ep 9 (Step 026760): Train loss 2.137, Val loss 4.080\n",
            "Ep 9 (Step 026765): Train loss 2.478, Val loss 4.074\n",
            "Ep 9 (Step 026770): Train loss 2.497, Val loss 4.088\n",
            "Ep 9 (Step 026775): Train loss 2.301, Val loss 4.098\n",
            "Ep 9 (Step 026780): Train loss 2.317, Val loss 4.090\n",
            "Ep 9 (Step 026785): Train loss 2.482, Val loss 4.078\n",
            "Ep 9 (Step 026790): Train loss 2.176, Val loss 4.079\n",
            "Every effort moves you, sir, And you shall find me to your worship.  ANTONIO. I will not be satisfied.  [_Exit._]  LODOVICO. I will not be sorry for this.  LOD\n",
            "Ep 10 (Step 026795): Train loss 2.246, Val loss 4.079\n",
            "Ep 10 (Step 026800): Train loss 2.427, Val loss 4.081\n",
            "Ep 10 (Step 026805): Train loss 2.367, Val loss 4.101\n",
            "Ep 10 (Step 026810): Train loss 2.272, Val loss 4.120\n",
            "Ep 10 (Step 026815): Train loss 2.383, Val loss 4.120\n",
            "Ep 10 (Step 026820): Train loss 2.183, Val loss 4.127\n",
            "Ep 10 (Step 026825): Train loss 2.341, Val loss 4.151\n",
            "Ep 10 (Step 026830): Train loss 1.967, Val loss 4.158\n",
            "Ep 10 (Step 026835): Train loss 2.381, Val loss 4.155\n",
            "Ep 10 (Step 026840): Train loss 2.383, Val loss 4.147\n",
            "Ep 10 (Step 026845): Train loss 2.299, Val loss 4.147\n",
            "Ep 10 (Step 026850): Train loss 2.261, Val loss 4.163\n",
            "Ep 10 (Step 026855): Train loss 2.300, Val loss 4.190\n",
            "Ep 10 (Step 026860): Train loss 2.296, Val loss 4.184\n",
            "Ep 10 (Step 026865): Train loss 2.179, Val loss 4.181\n",
            "Ep 10 (Step 026870): Train loss 2.420, Val loss 4.184\n",
            "Ep 10 (Step 026875): Train loss 2.357, Val loss 4.184\n",
            "Ep 10 (Step 026880): Train loss 2.138, Val loss 4.173\n",
            "Ep 10 (Step 026885): Train loss 2.405, Val loss 4.173\n",
            "Ep 10 (Step 026890): Train loss 2.101, Val loss 4.179\n",
            "Ep 10 (Step 026895): Train loss 2.179, Val loss 4.164\n",
            "Ep 10 (Step 026900): Train loss 2.388, Val loss 4.170\n",
            "Ep 10 (Step 026905): Train loss 2.180, Val loss 4.176\n",
            "Ep 10 (Step 026910): Train loss 2.266, Val loss 4.176\n",
            "Ep 10 (Step 026915): Train loss 2.087, Val loss 4.185\n",
            "Ep 10 (Step 026920): Train loss 2.348, Val loss 4.179\n",
            "Ep 10 (Step 026925): Train loss 2.256, Val loss 4.184\n",
            "Ep 10 (Step 026930): Train loss 2.298, Val loss 4.181\n",
            "Ep 10 (Step 026935): Train loss 2.135, Val loss 4.174\n",
            "Ep 10 (Step 026940): Train loss 2.188, Val loss 4.173\n",
            "Ep 10 (Step 026945): Train loss 2.227, Val loss 4.178\n",
            "Ep 10 (Step 026950): Train loss 2.397, Val loss 4.192\n",
            "Ep 10 (Step 026955): Train loss 2.377, Val loss 4.195\n",
            "Ep 10 (Step 026960): Train loss 2.140, Val loss 4.183\n",
            "Ep 10 (Step 026965): Train loss 2.416, Val loss 4.172\n",
            "Ep 10 (Step 026970): Train loss 2.269, Val loss 4.188\n",
            "Ep 10 (Step 026975): Train loss 2.335, Val loss 4.198\n",
            "Ep 10 (Step 026980): Train loss 2.218, Val loss 4.198\n",
            "Ep 10 (Step 026985): Train loss 2.161, Val loss 4.189\n",
            "Ep 10 (Step 026990): Train loss 2.103, Val loss 4.188\n",
            "Ep 10 (Step 026995): Train loss 2.356, Val loss 4.196\n",
            "Ep 10 (Step 027000): Train loss 1.966, Val loss 4.197\n",
            "Ep 10 (Step 027005): Train loss 2.287, Val loss 4.204\n",
            "Ep 10 (Step 027010): Train loss 2.263, Val loss 4.222\n",
            "Ep 10 (Step 027015): Train loss 2.290, Val loss 4.209\n",
            "Ep 10 (Step 027020): Train loss 2.164, Val loss 4.214\n",
            "Ep 10 (Step 027025): Train loss 2.172, Val loss 4.225\n",
            "Ep 10 (Step 027030): Train loss 2.409, Val loss 4.215\n",
            "Ep 10 (Step 027035): Train loss 2.368, Val loss 4.216\n",
            "Ep 10 (Step 027040): Train loss 1.938, Val loss 4.212\n",
            "Ep 10 (Step 027045): Train loss 2.117, Val loss 4.214\n",
            "Ep 10 (Step 027050): Train loss 2.144, Val loss 4.222\n",
            "Ep 10 (Step 027055): Train loss 2.322, Val loss 4.232\n",
            "Ep 10 (Step 027060): Train loss 2.540, Val loss 4.224\n",
            "Ep 10 (Step 027065): Train loss 2.413, Val loss 4.220\n",
            "Ep 10 (Step 027070): Train loss 2.144, Val loss 4.230\n",
            "Ep 10 (Step 027075): Train loss 2.094, Val loss 4.234\n",
            "Ep 10 (Step 027080): Train loss 2.387, Val loss 4.225\n",
            "Ep 10 (Step 027085): Train loss 2.186, Val loss 4.236\n",
            "Ep 10 (Step 027090): Train loss 2.431, Val loss 4.229\n",
            "Ep 10 (Step 027095): Train loss 2.034, Val loss 4.230\n",
            "Ep 10 (Step 027100): Train loss 2.159, Val loss 4.226\n",
            "Ep 10 (Step 027105): Train loss 2.407, Val loss 4.245\n",
            "Ep 10 (Step 027110): Train loss 2.088, Val loss 4.242\n",
            "Ep 10 (Step 027115): Train loss 2.406, Val loss 4.225\n",
            "Ep 10 (Step 027120): Train loss 2.074, Val loss 4.219\n",
            "Ep 10 (Step 027125): Train loss 2.219, Val loss 4.230\n",
            "Ep 10 (Step 027130): Train loss 2.324, Val loss 4.245\n",
            "Ep 10 (Step 027135): Train loss 2.296, Val loss 4.253\n",
            "Ep 10 (Step 027140): Train loss 2.501, Val loss 4.257\n",
            "Ep 10 (Step 027145): Train loss 2.306, Val loss 4.246\n",
            "Ep 10 (Step 027150): Train loss 2.209, Val loss 4.260\n",
            "Ep 10 (Step 027155): Train loss 2.336, Val loss 4.262\n",
            "Ep 10 (Step 027160): Train loss 2.203, Val loss 4.248\n",
            "Ep 10 (Step 027165): Train loss 2.117, Val loss 4.246\n",
            "Ep 10 (Step 027170): Train loss 2.229, Val loss 4.237\n",
            "Ep 10 (Step 027175): Train loss 2.204, Val loss 4.230\n",
            "Ep 10 (Step 027180): Train loss 2.423, Val loss 4.236\n",
            "Ep 10 (Step 027185): Train loss 1.996, Val loss 4.221\n",
            "Ep 10 (Step 027190): Train loss 2.321, Val loss 4.216\n",
            "Ep 10 (Step 027195): Train loss 2.200, Val loss 4.223\n",
            "Ep 10 (Step 027200): Train loss 2.410, Val loss 4.239\n",
            "Ep 10 (Step 027205): Train loss 2.303, Val loss 4.229\n",
            "Ep 10 (Step 027210): Train loss 1.862, Val loss 4.247\n",
            "Ep 10 (Step 027215): Train loss 2.471, Val loss 4.243\n",
            "Ep 10 (Step 027220): Train loss 2.377, Val loss 4.254\n",
            "Ep 10 (Step 027225): Train loss 2.199, Val loss 4.249\n",
            "Ep 10 (Step 027230): Train loss 2.035, Val loss 4.240\n",
            "Ep 10 (Step 027235): Train loss 2.017, Val loss 4.233\n",
            "Ep 10 (Step 027240): Train loss 2.344, Val loss 4.214\n",
            "Ep 10 (Step 027245): Train loss 2.266, Val loss 4.198\n",
            "Ep 10 (Step 027250): Train loss 2.311, Val loss 4.204\n",
            "Ep 10 (Step 027255): Train loss 2.086, Val loss 4.201\n",
            "Ep 10 (Step 027260): Train loss 1.933, Val loss 4.195\n",
            "Ep 10 (Step 027265): Train loss 2.307, Val loss 4.197\n",
            "Ep 10 (Step 027270): Train loss 1.982, Val loss 4.220\n",
            "Ep 10 (Step 027275): Train loss 2.272, Val loss 4.247\n",
            "Ep 10 (Step 027280): Train loss 2.284, Val loss 4.254\n",
            "Ep 10 (Step 027285): Train loss 1.975, Val loss 4.258\n",
            "Ep 10 (Step 027290): Train loss 2.123, Val loss 4.256\n",
            "Ep 10 (Step 027295): Train loss 2.256, Val loss 4.234\n",
            "Ep 10 (Step 027300): Train loss 2.253, Val loss 4.247\n",
            "Ep 10 (Step 027305): Train loss 2.251, Val loss 4.239\n",
            "Ep 10 (Step 027310): Train loss 2.026, Val loss 4.232\n",
            "Ep 10 (Step 027315): Train loss 2.291, Val loss 4.225\n",
            "Ep 10 (Step 027320): Train loss 2.262, Val loss 4.214\n",
            "Ep 10 (Step 027325): Train loss 2.567, Val loss 4.219\n",
            "Ep 10 (Step 027330): Train loss 2.236, Val loss 4.210\n",
            "Ep 10 (Step 027335): Train loss 2.314, Val loss 4.210\n",
            "Ep 10 (Step 027340): Train loss 2.386, Val loss 4.217\n",
            "Ep 10 (Step 027345): Train loss 2.049, Val loss 4.216\n",
            "Ep 10 (Step 027350): Train loss 2.053, Val loss 4.240\n",
            "Ep 10 (Step 027355): Train loss 1.833, Val loss 4.251\n",
            "Ep 10 (Step 027360): Train loss 2.541, Val loss 4.255\n",
            "Ep 10 (Step 027365): Train loss 2.000, Val loss 4.267\n",
            "Ep 10 (Step 027370): Train loss 2.103, Val loss 4.260\n",
            "Ep 10 (Step 027375): Train loss 2.446, Val loss 4.252\n",
            "Ep 10 (Step 027380): Train loss 2.272, Val loss 4.267\n",
            "Ep 10 (Step 027385): Train loss 2.231, Val loss 4.248\n",
            "Ep 10 (Step 027390): Train loss 2.121, Val loss 4.254\n",
            "Ep 10 (Step 027395): Train loss 2.240, Val loss 4.285\n",
            "Ep 10 (Step 027400): Train loss 2.320, Val loss 4.306\n",
            "Ep 10 (Step 027405): Train loss 1.925, Val loss 4.297\n",
            "Ep 10 (Step 027410): Train loss 2.261, Val loss 4.293\n",
            "Ep 10 (Step 027415): Train loss 2.231, Val loss 4.288\n",
            "Ep 10 (Step 027420): Train loss 2.390, Val loss 4.273\n",
            "Ep 10 (Step 027425): Train loss 2.284, Val loss 4.279\n",
            "Ep 10 (Step 027430): Train loss 2.182, Val loss 4.304\n",
            "Ep 10 (Step 027435): Train loss 2.413, Val loss 4.295\n",
            "Ep 10 (Step 027440): Train loss 2.214, Val loss 4.279\n",
            "Ep 10 (Step 027445): Train loss 2.462, Val loss 4.257\n",
            "Ep 10 (Step 027450): Train loss 2.182, Val loss 4.260\n",
            "Ep 10 (Step 027455): Train loss 2.168, Val loss 4.267\n",
            "Ep 10 (Step 027460): Train loss 2.087, Val loss 4.262\n",
            "Ep 10 (Step 027465): Train loss 2.461, Val loss 4.248\n",
            "Ep 10 (Step 027470): Train loss 2.430, Val loss 4.255\n",
            "Ep 10 (Step 027475): Train loss 1.999, Val loss 4.263\n",
            "Ep 10 (Step 027480): Train loss 1.909, Val loss 4.267\n",
            "Ep 10 (Step 027485): Train loss 2.082, Val loss 4.269\n",
            "Ep 10 (Step 027490): Train loss 2.065, Val loss 4.270\n",
            "Ep 10 (Step 027495): Train loss 2.092, Val loss 4.266\n",
            "Ep 10 (Step 027500): Train loss 2.235, Val loss 4.266\n",
            "Ep 10 (Step 027505): Train loss 2.445, Val loss 4.257\n",
            "Ep 10 (Step 027510): Train loss 2.452, Val loss 4.254\n",
            "Ep 10 (Step 027515): Train loss 2.426, Val loss 4.266\n",
            "Ep 10 (Step 027520): Train loss 2.088, Val loss 4.262\n",
            "Ep 10 (Step 027525): Train loss 2.479, Val loss 4.257\n",
            "Ep 10 (Step 027530): Train loss 2.174, Val loss 4.247\n",
            "Ep 10 (Step 027535): Train loss 2.171, Val loss 4.256\n",
            "Ep 10 (Step 027540): Train loss 2.010, Val loss 4.261\n",
            "Ep 10 (Step 027545): Train loss 2.011, Val loss 4.257\n",
            "Ep 10 (Step 027550): Train loss 2.029, Val loss 4.250\n",
            "Ep 10 (Step 027555): Train loss 2.204, Val loss 4.242\n",
            "Ep 10 (Step 027560): Train loss 2.057, Val loss 4.235\n",
            "Ep 10 (Step 027565): Train loss 2.333, Val loss 4.226\n",
            "Ep 10 (Step 027570): Train loss 2.231, Val loss 4.228\n",
            "Ep 10 (Step 027575): Train loss 2.149, Val loss 4.245\n",
            "Ep 10 (Step 027580): Train loss 2.084, Val loss 4.231\n",
            "Ep 10 (Step 027585): Train loss 1.922, Val loss 4.224\n",
            "Ep 10 (Step 027590): Train loss 1.963, Val loss 4.237\n",
            "Ep 10 (Step 027595): Train loss 2.052, Val loss 4.242\n",
            "Ep 10 (Step 027600): Train loss 2.293, Val loss 4.254\n",
            "Ep 10 (Step 027605): Train loss 2.438, Val loss 4.273\n",
            "Ep 10 (Step 027610): Train loss 2.092, Val loss 4.268\n",
            "Ep 10 (Step 027615): Train loss 2.081, Val loss 4.271\n",
            "Ep 10 (Step 027620): Train loss 2.199, Val loss 4.272\n",
            "Ep 10 (Step 027625): Train loss 2.221, Val loss 4.266\n",
            "Ep 10 (Step 027630): Train loss 2.202, Val loss 4.270\n",
            "Ep 10 (Step 027635): Train loss 2.347, Val loss 4.262\n",
            "Ep 10 (Step 027640): Train loss 2.110, Val loss 4.252\n",
            "Ep 10 (Step 027645): Train loss 2.090, Val loss 4.266\n",
            "Ep 10 (Step 027650): Train loss 2.148, Val loss 4.278\n",
            "Ep 10 (Step 027655): Train loss 2.097, Val loss 4.271\n",
            "Ep 10 (Step 027660): Train loss 2.540, Val loss 4.253\n",
            "Ep 10 (Step 027665): Train loss 2.230, Val loss 4.253\n",
            "Ep 10 (Step 027670): Train loss 1.923, Val loss 4.255\n",
            "Ep 10 (Step 027675): Train loss 2.307, Val loss 4.239\n",
            "Ep 10 (Step 027680): Train loss 2.356, Val loss 4.241\n",
            "Ep 10 (Step 027685): Train loss 2.217, Val loss 4.239\n",
            "Ep 10 (Step 027690): Train loss 2.062, Val loss 4.232\n",
            "Ep 10 (Step 027695): Train loss 2.158, Val loss 4.230\n",
            "Ep 10 (Step 027700): Train loss 1.988, Val loss 4.234\n",
            "Ep 10 (Step 027705): Train loss 1.974, Val loss 4.233\n",
            "Ep 10 (Step 027710): Train loss 2.238, Val loss 4.240\n",
            "Ep 10 (Step 027715): Train loss 2.088, Val loss 4.238\n",
            "Ep 10 (Step 027720): Train loss 2.310, Val loss 4.239\n",
            "Ep 10 (Step 027725): Train loss 2.384, Val loss 4.245\n",
            "Ep 10 (Step 027730): Train loss 2.244, Val loss 4.254\n",
            "Ep 10 (Step 027735): Train loss 2.240, Val loss 4.256\n",
            "Ep 10 (Step 027740): Train loss 2.085, Val loss 4.249\n",
            "Ep 10 (Step 027745): Train loss 2.110, Val loss 4.237\n",
            "Ep 10 (Step 027750): Train loss 1.951, Val loss 4.232\n",
            "Ep 10 (Step 027755): Train loss 2.604, Val loss 4.251\n",
            "Ep 10 (Step 027760): Train loss 2.311, Val loss 4.250\n",
            "Ep 10 (Step 027765): Train loss 1.887, Val loss 4.248\n",
            "Ep 10 (Step 027770): Train loss 2.021, Val loss 4.252\n",
            "Ep 10 (Step 027775): Train loss 2.066, Val loss 4.246\n",
            "Ep 10 (Step 027780): Train loss 2.370, Val loss 4.234\n",
            "Ep 10 (Step 027785): Train loss 2.249, Val loss 4.241\n",
            "Ep 10 (Step 027790): Train loss 1.974, Val loss 4.247\n",
            "Ep 10 (Step 027795): Train loss 1.962, Val loss 4.248\n",
            "Ep 10 (Step 027800): Train loss 2.030, Val loss 4.256\n",
            "Ep 10 (Step 027805): Train loss 2.220, Val loss 4.252\n",
            "Ep 10 (Step 027810): Train loss 2.512, Val loss 4.248\n",
            "Ep 10 (Step 027815): Train loss 2.021, Val loss 4.247\n",
            "Ep 10 (Step 027820): Train loss 2.067, Val loss 4.225\n",
            "Ep 10 (Step 027825): Train loss 2.163, Val loss 4.242\n",
            "Ep 10 (Step 027830): Train loss 2.101, Val loss 4.242\n",
            "Ep 10 (Step 027835): Train loss 2.088, Val loss 4.231\n",
            "Ep 10 (Step 027840): Train loss 1.925, Val loss 4.246\n",
            "Ep 10 (Step 027845): Train loss 2.433, Val loss 4.248\n",
            "Ep 10 (Step 027850): Train loss 2.189, Val loss 4.252\n",
            "Ep 10 (Step 027855): Train loss 2.513, Val loss 4.247\n",
            "Ep 10 (Step 027860): Train loss 2.182, Val loss 4.250\n",
            "Ep 10 (Step 027865): Train loss 2.317, Val loss 4.249\n",
            "Ep 10 (Step 027870): Train loss 2.334, Val loss 4.247\n",
            "Ep 10 (Step 027875): Train loss 1.971, Val loss 4.237\n",
            "Ep 10 (Step 027880): Train loss 2.264, Val loss 4.234\n",
            "Ep 10 (Step 027885): Train loss 1.988, Val loss 4.247\n",
            "Ep 10 (Step 027890): Train loss 1.872, Val loss 4.266\n",
            "Ep 10 (Step 027895): Train loss 2.148, Val loss 4.273\n",
            "Ep 10 (Step 027900): Train loss 1.999, Val loss 4.275\n",
            "Ep 10 (Step 027905): Train loss 2.101, Val loss 4.254\n",
            "Ep 10 (Step 027910): Train loss 1.867, Val loss 4.246\n",
            "Ep 10 (Step 027915): Train loss 2.069, Val loss 4.253\n",
            "Ep 10 (Step 027920): Train loss 2.073, Val loss 4.259\n",
            "Ep 10 (Step 027925): Train loss 2.345, Val loss 4.260\n",
            "Ep 10 (Step 027930): Train loss 2.385, Val loss 4.260\n",
            "Ep 10 (Step 027935): Train loss 2.218, Val loss 4.261\n",
            "Ep 10 (Step 027940): Train loss 2.166, Val loss 4.251\n",
            "Ep 10 (Step 027945): Train loss 2.418, Val loss 4.236\n",
            "Ep 10 (Step 027950): Train loss 2.248, Val loss 4.213\n",
            "Ep 10 (Step 027955): Train loss 2.171, Val loss 4.203\n",
            "Ep 10 (Step 027960): Train loss 2.329, Val loss 4.218\n",
            "Ep 10 (Step 027965): Train loss 2.159, Val loss 4.237\n",
            "Ep 10 (Step 027970): Train loss 2.186, Val loss 4.240\n",
            "Ep 10 (Step 027975): Train loss 2.143, Val loss 4.232\n",
            "Ep 10 (Step 027980): Train loss 2.146, Val loss 4.214\n",
            "Ep 10 (Step 027985): Train loss 2.330, Val loss 4.207\n",
            "Ep 10 (Step 027990): Train loss 2.240, Val loss 4.205\n",
            "Ep 10 (Step 027995): Train loss 2.464, Val loss 4.209\n",
            "Ep 10 (Step 028000): Train loss 2.329, Val loss 4.222\n",
            "Ep 10 (Step 028005): Train loss 2.225, Val loss 4.223\n",
            "Ep 10 (Step 028010): Train loss 2.445, Val loss 4.221\n",
            "Ep 10 (Step 028015): Train loss 2.354, Val loss 4.216\n",
            "Ep 10 (Step 028020): Train loss 2.143, Val loss 4.220\n",
            "Ep 10 (Step 028025): Train loss 2.336, Val loss 4.240\n",
            "Ep 10 (Step 028030): Train loss 2.271, Val loss 4.222\n",
            "Ep 10 (Step 028035): Train loss 2.130, Val loss 4.220\n",
            "Ep 10 (Step 028040): Train loss 2.377, Val loss 4.214\n",
            "Ep 10 (Step 028045): Train loss 2.029, Val loss 4.233\n",
            "Ep 10 (Step 028050): Train loss 2.256, Val loss 4.233\n",
            "Ep 10 (Step 028055): Train loss 2.021, Val loss 4.217\n",
            "Ep 10 (Step 028060): Train loss 2.342, Val loss 4.221\n",
            "Ep 10 (Step 028065): Train loss 2.223, Val loss 4.247\n",
            "Ep 10 (Step 028070): Train loss 2.258, Val loss 4.256\n",
            "Ep 10 (Step 028075): Train loss 2.092, Val loss 4.233\n",
            "Ep 10 (Step 028080): Train loss 2.535, Val loss 4.216\n",
            "Ep 10 (Step 028085): Train loss 2.008, Val loss 4.221\n",
            "Ep 10 (Step 028090): Train loss 2.084, Val loss 4.220\n",
            "Ep 10 (Step 028095): Train loss 2.059, Val loss 4.230\n",
            "Ep 10 (Step 028100): Train loss 2.051, Val loss 4.226\n",
            "Ep 10 (Step 028105): Train loss 2.045, Val loss 4.230\n",
            "Ep 10 (Step 028110): Train loss 2.283, Val loss 4.228\n",
            "Ep 10 (Step 028115): Train loss 1.979, Val loss 4.226\n",
            "Ep 10 (Step 028120): Train loss 2.003, Val loss 4.229\n",
            "Ep 10 (Step 028125): Train loss 2.265, Val loss 4.224\n",
            "Ep 10 (Step 028130): Train loss 2.254, Val loss 4.229\n",
            "Ep 10 (Step 028135): Train loss 2.080, Val loss 4.222\n",
            "Ep 10 (Step 028140): Train loss 2.245, Val loss 4.223\n",
            "Ep 10 (Step 028145): Train loss 2.191, Val loss 4.226\n",
            "Ep 10 (Step 028150): Train loss 1.994, Val loss 4.240\n",
            "Ep 10 (Step 028155): Train loss 2.315, Val loss 4.257\n",
            "Ep 10 (Step 028160): Train loss 2.197, Val loss 4.250\n",
            "Ep 10 (Step 028165): Train loss 2.116, Val loss 4.241\n",
            "Ep 10 (Step 028170): Train loss 2.262, Val loss 4.249\n",
            "Ep 10 (Step 028175): Train loss 2.333, Val loss 4.251\n",
            "Ep 10 (Step 028180): Train loss 2.082, Val loss 4.246\n",
            "Ep 10 (Step 028185): Train loss 2.079, Val loss 4.242\n",
            "Ep 10 (Step 028190): Train loss 2.346, Val loss 4.224\n",
            "Ep 10 (Step 028195): Train loss 2.021, Val loss 4.218\n",
            "Ep 10 (Step 028200): Train loss 2.111, Val loss 4.200\n",
            "Ep 10 (Step 028205): Train loss 2.326, Val loss 4.200\n",
            "Ep 10 (Step 028210): Train loss 2.271, Val loss 4.210\n",
            "Ep 10 (Step 028215): Train loss 1.896, Val loss 4.198\n",
            "Ep 10 (Step 028220): Train loss 2.235, Val loss 4.217\n",
            "Ep 10 (Step 028225): Train loss 2.377, Val loss 4.242\n",
            "Ep 10 (Step 028230): Train loss 2.474, Val loss 4.247\n",
            "Ep 10 (Step 028235): Train loss 2.248, Val loss 4.240\n",
            "Ep 10 (Step 028240): Train loss 2.082, Val loss 4.243\n",
            "Ep 10 (Step 028245): Train loss 2.200, Val loss 4.229\n",
            "Ep 10 (Step 028250): Train loss 2.273, Val loss 4.228\n",
            "Ep 10 (Step 028255): Train loss 2.288, Val loss 4.225\n",
            "Ep 10 (Step 028260): Train loss 2.203, Val loss 4.202\n",
            "Ep 10 (Step 028265): Train loss 2.470, Val loss 4.191\n",
            "Ep 10 (Step 028270): Train loss 2.437, Val loss 4.198\n",
            "Ep 10 (Step 028275): Train loss 1.929, Val loss 4.207\n",
            "Ep 10 (Step 028280): Train loss 2.267, Val loss 4.201\n",
            "Ep 10 (Step 028285): Train loss 2.362, Val loss 4.219\n",
            "Ep 10 (Step 028290): Train loss 2.158, Val loss 4.242\n",
            "Ep 10 (Step 028295): Train loss 2.142, Val loss 4.236\n",
            "Ep 10 (Step 028300): Train loss 2.439, Val loss 4.227\n",
            "Ep 10 (Step 028305): Train loss 1.960, Val loss 4.226\n",
            "Ep 10 (Step 028310): Train loss 2.234, Val loss 4.230\n",
            "Ep 10 (Step 028315): Train loss 2.255, Val loss 4.234\n",
            "Ep 10 (Step 028320): Train loss 2.228, Val loss 4.234\n",
            "Ep 10 (Step 028325): Train loss 2.015, Val loss 4.239\n",
            "Ep 10 (Step 028330): Train loss 2.285, Val loss 4.239\n",
            "Ep 10 (Step 028335): Train loss 2.008, Val loss 4.231\n",
            "Ep 10 (Step 028340): Train loss 2.314, Val loss 4.222\n",
            "Ep 10 (Step 028345): Train loss 1.944, Val loss 4.222\n",
            "Ep 10 (Step 028350): Train loss 2.178, Val loss 4.228\n",
            "Ep 10 (Step 028355): Train loss 1.995, Val loss 4.229\n",
            "Ep 10 (Step 028360): Train loss 2.052, Val loss 4.229\n",
            "Ep 10 (Step 028365): Train loss 2.101, Val loss 4.202\n",
            "Ep 10 (Step 028370): Train loss 1.971, Val loss 4.192\n",
            "Ep 10 (Step 028375): Train loss 1.959, Val loss 4.207\n",
            "Ep 10 (Step 028380): Train loss 2.169, Val loss 4.212\n",
            "Ep 10 (Step 028385): Train loss 2.397, Val loss 4.220\n",
            "Ep 10 (Step 028390): Train loss 2.133, Val loss 4.218\n",
            "Ep 10 (Step 028395): Train loss 1.929, Val loss 4.204\n",
            "Ep 10 (Step 028400): Train loss 2.142, Val loss 4.203\n",
            "Ep 10 (Step 028405): Train loss 2.153, Val loss 4.202\n",
            "Ep 10 (Step 028410): Train loss 2.090, Val loss 4.198\n",
            "Ep 10 (Step 028415): Train loss 1.877, Val loss 4.207\n",
            "Ep 10 (Step 028420): Train loss 2.125, Val loss 4.199\n",
            "Ep 10 (Step 028425): Train loss 2.200, Val loss 4.191\n",
            "Ep 10 (Step 028430): Train loss 2.262, Val loss 4.186\n",
            "Ep 10 (Step 028435): Train loss 2.133, Val loss 4.184\n",
            "Ep 10 (Step 028440): Train loss 2.290, Val loss 4.176\n",
            "Ep 10 (Step 028445): Train loss 2.269, Val loss 4.175\n",
            "Ep 10 (Step 028450): Train loss 1.970, Val loss 4.190\n",
            "Ep 10 (Step 028455): Train loss 1.808, Val loss 4.190\n",
            "Ep 10 (Step 028460): Train loss 2.188, Val loss 4.193\n",
            "Ep 10 (Step 028465): Train loss 1.975, Val loss 4.215\n",
            "Ep 10 (Step 028470): Train loss 2.190, Val loss 4.215\n",
            "Ep 10 (Step 028475): Train loss 2.037, Val loss 4.217\n",
            "Ep 10 (Step 028480): Train loss 2.255, Val loss 4.199\n",
            "Ep 10 (Step 028485): Train loss 2.151, Val loss 4.193\n",
            "Ep 10 (Step 028490): Train loss 1.920, Val loss 4.184\n",
            "Ep 10 (Step 028495): Train loss 1.773, Val loss 4.193\n",
            "Ep 10 (Step 028500): Train loss 2.416, Val loss 4.193\n",
            "Ep 10 (Step 028505): Train loss 1.965, Val loss 4.191\n",
            "Ep 10 (Step 028510): Train loss 2.133, Val loss 4.188\n",
            "Ep 10 (Step 028515): Train loss 2.338, Val loss 4.189\n",
            "Ep 10 (Step 028520): Train loss 2.232, Val loss 4.190\n",
            "Ep 10 (Step 028525): Train loss 2.376, Val loss 4.201\n",
            "Ep 10 (Step 028530): Train loss 2.356, Val loss 4.219\n",
            "Ep 10 (Step 028535): Train loss 2.473, Val loss 4.216\n",
            "Ep 10 (Step 028540): Train loss 2.231, Val loss 4.209\n",
            "Ep 10 (Step 028545): Train loss 2.024, Val loss 4.198\n",
            "Ep 10 (Step 028550): Train loss 2.227, Val loss 4.207\n",
            "Ep 10 (Step 028555): Train loss 2.160, Val loss 4.210\n",
            "Ep 10 (Step 028560): Train loss 2.077, Val loss 4.225\n",
            "Ep 10 (Step 028565): Train loss 2.005, Val loss 4.231\n",
            "Ep 10 (Step 028570): Train loss 2.150, Val loss 4.220\n",
            "Ep 10 (Step 028575): Train loss 2.262, Val loss 4.229\n",
            "Ep 10 (Step 028580): Train loss 2.005, Val loss 4.237\n",
            "Ep 10 (Step 028585): Train loss 2.342, Val loss 4.248\n",
            "Ep 10 (Step 028590): Train loss 2.026, Val loss 4.242\n",
            "Ep 10 (Step 028595): Train loss 2.177, Val loss 4.236\n",
            "Ep 10 (Step 028600): Train loss 1.982, Val loss 4.230\n",
            "Ep 10 (Step 028605): Train loss 2.290, Val loss 4.226\n",
            "Ep 10 (Step 028610): Train loss 2.525, Val loss 4.217\n",
            "Ep 10 (Step 028615): Train loss 2.114, Val loss 4.226\n",
            "Ep 10 (Step 028620): Train loss 1.977, Val loss 4.226\n",
            "Ep 10 (Step 028625): Train loss 2.006, Val loss 4.228\n",
            "Ep 10 (Step 028630): Train loss 2.100, Val loss 4.226\n",
            "Ep 10 (Step 028635): Train loss 2.026, Val loss 4.230\n",
            "Ep 10 (Step 028640): Train loss 2.344, Val loss 4.229\n",
            "Ep 10 (Step 028645): Train loss 2.242, Val loss 4.231\n",
            "Ep 10 (Step 028650): Train loss 2.002, Val loss 4.228\n",
            "Ep 10 (Step 028655): Train loss 2.155, Val loss 4.240\n",
            "Ep 10 (Step 028660): Train loss 2.114, Val loss 4.244\n",
            "Ep 10 (Step 028665): Train loss 2.032, Val loss 4.218\n",
            "Ep 10 (Step 028670): Train loss 1.955, Val loss 4.210\n",
            "Ep 10 (Step 028675): Train loss 2.261, Val loss 4.223\n",
            "Ep 10 (Step 028680): Train loss 2.213, Val loss 4.221\n",
            "Ep 10 (Step 028685): Train loss 2.233, Val loss 4.225\n",
            "Ep 10 (Step 028690): Train loss 2.282, Val loss 4.235\n",
            "Ep 10 (Step 028695): Train loss 2.157, Val loss 4.237\n",
            "Ep 10 (Step 028700): Train loss 2.201, Val loss 4.234\n",
            "Ep 10 (Step 028705): Train loss 2.065, Val loss 4.228\n",
            "Ep 10 (Step 028710): Train loss 2.099, Val loss 4.238\n",
            "Ep 10 (Step 028715): Train loss 1.828, Val loss 4.232\n",
            "Ep 10 (Step 028720): Train loss 1.970, Val loss 4.231\n",
            "Ep 10 (Step 028725): Train loss 2.263, Val loss 4.230\n",
            "Ep 10 (Step 028730): Train loss 1.928, Val loss 4.230\n",
            "Ep 10 (Step 028735): Train loss 1.839, Val loss 4.225\n",
            "Ep 10 (Step 028740): Train loss 2.141, Val loss 4.223\n",
            "Ep 10 (Step 028745): Train loss 2.406, Val loss 4.226\n",
            "Ep 10 (Step 028750): Train loss 2.119, Val loss 4.245\n",
            "Ep 10 (Step 028755): Train loss 2.238, Val loss 4.260\n",
            "Ep 10 (Step 028760): Train loss 2.056, Val loss 4.259\n",
            "Ep 10 (Step 028765): Train loss 2.313, Val loss 4.247\n",
            "Ep 10 (Step 028770): Train loss 2.051, Val loss 4.250\n",
            "Ep 10 (Step 028775): Train loss 2.090, Val loss 4.259\n",
            "Ep 10 (Step 028780): Train loss 2.422, Val loss 4.227\n",
            "Ep 10 (Step 028785): Train loss 2.330, Val loss 4.220\n",
            "Ep 10 (Step 028790): Train loss 1.949, Val loss 4.218\n",
            "Ep 10 (Step 028795): Train loss 2.060, Val loss 4.213\n",
            "Ep 10 (Step 028800): Train loss 2.006, Val loss 4.216\n",
            "Ep 10 (Step 028805): Train loss 2.052, Val loss 4.204\n",
            "Ep 10 (Step 028810): Train loss 2.162, Val loss 4.202\n",
            "Ep 10 (Step 028815): Train loss 2.089, Val loss 4.216\n",
            "Ep 10 (Step 028820): Train loss 2.176, Val loss 4.227\n",
            "Ep 10 (Step 028825): Train loss 1.928, Val loss 4.214\n",
            "Ep 10 (Step 028830): Train loss 2.018, Val loss 4.202\n",
            "Ep 10 (Step 028835): Train loss 2.081, Val loss 4.216\n",
            "Ep 10 (Step 028840): Train loss 2.083, Val loss 4.208\n",
            "Ep 10 (Step 028845): Train loss 2.164, Val loss 4.189\n",
            "Ep 10 (Step 028850): Train loss 2.296, Val loss 4.186\n",
            "Ep 10 (Step 028855): Train loss 1.888, Val loss 4.178\n",
            "Ep 10 (Step 028860): Train loss 2.055, Val loss 4.184\n",
            "Ep 10 (Step 028865): Train loss 2.000, Val loss 4.195\n",
            "Ep 10 (Step 028870): Train loss 1.946, Val loss 4.196\n",
            "Ep 10 (Step 028875): Train loss 2.092, Val loss 4.190\n",
            "Ep 10 (Step 028880): Train loss 2.096, Val loss 4.187\n",
            "Ep 10 (Step 028885): Train loss 2.082, Val loss 4.195\n",
            "Ep 10 (Step 028890): Train loss 2.100, Val loss 4.188\n",
            "Ep 10 (Step 028895): Train loss 2.087, Val loss 4.190\n",
            "Ep 10 (Step 028900): Train loss 1.941, Val loss 4.182\n",
            "Ep 10 (Step 028905): Train loss 2.228, Val loss 4.180\n",
            "Ep 10 (Step 028910): Train loss 2.105, Val loss 4.185\n",
            "Ep 10 (Step 028915): Train loss 2.192, Val loss 4.199\n",
            "Ep 10 (Step 028920): Train loss 2.244, Val loss 4.200\n",
            "Ep 10 (Step 028925): Train loss 2.211, Val loss 4.204\n",
            "Ep 10 (Step 028930): Train loss 2.185, Val loss 4.202\n",
            "Ep 10 (Step 028935): Train loss 1.955, Val loss 4.193\n",
            "Ep 10 (Step 028940): Train loss 2.052, Val loss 4.181\n",
            "Ep 10 (Step 028945): Train loss 2.053, Val loss 4.181\n",
            "Ep 10 (Step 028950): Train loss 2.104, Val loss 4.187\n",
            "Ep 10 (Step 028955): Train loss 2.039, Val loss 4.179\n",
            "Ep 10 (Step 028960): Train loss 2.083, Val loss 4.180\n",
            "Ep 10 (Step 028965): Train loss 2.261, Val loss 4.189\n",
            "Ep 10 (Step 028970): Train loss 1.944, Val loss 4.199\n",
            "Ep 10 (Step 028975): Train loss 2.047, Val loss 4.196\n",
            "Ep 10 (Step 028980): Train loss 2.184, Val loss 4.192\n",
            "Ep 10 (Step 028985): Train loss 1.887, Val loss 4.190\n",
            "Ep 10 (Step 028990): Train loss 2.122, Val loss 4.198\n",
            "Ep 10 (Step 028995): Train loss 2.086, Val loss 4.193\n",
            "Ep 10 (Step 029000): Train loss 1.933, Val loss 4.188\n",
            "Ep 10 (Step 029005): Train loss 2.066, Val loss 4.193\n",
            "Ep 10 (Step 029010): Train loss 2.419, Val loss 4.190\n",
            "Ep 10 (Step 029015): Train loss 2.268, Val loss 4.180\n",
            "Ep 10 (Step 029020): Train loss 2.243, Val loss 4.180\n",
            "Ep 10 (Step 029025): Train loss 2.165, Val loss 4.184\n",
            "Ep 10 (Step 029030): Train loss 2.061, Val loss 4.185\n",
            "Ep 10 (Step 029035): Train loss 2.238, Val loss 4.186\n",
            "Ep 10 (Step 029040): Train loss 1.909, Val loss 4.189\n",
            "Ep 10 (Step 029045): Train loss 2.206, Val loss 4.193\n",
            "Ep 10 (Step 029050): Train loss 2.031, Val loss 4.197\n",
            "Ep 10 (Step 029055): Train loss 1.909, Val loss 4.204\n",
            "Ep 10 (Step 029060): Train loss 2.148, Val loss 4.218\n",
            "Ep 10 (Step 029065): Train loss 2.112, Val loss 4.221\n",
            "Ep 10 (Step 029070): Train loss 2.063, Val loss 4.213\n",
            "Ep 10 (Step 029075): Train loss 1.931, Val loss 4.217\n",
            "Ep 10 (Step 029080): Train loss 1.931, Val loss 4.225\n",
            "Ep 10 (Step 029085): Train loss 2.033, Val loss 4.237\n",
            "Ep 10 (Step 029090): Train loss 1.915, Val loss 4.215\n",
            "Ep 10 (Step 029095): Train loss 2.166, Val loss 4.203\n",
            "Ep 10 (Step 029100): Train loss 1.982, Val loss 4.197\n",
            "Ep 10 (Step 029105): Train loss 2.216, Val loss 4.194\n",
            "Ep 10 (Step 029110): Train loss 2.049, Val loss 4.193\n",
            "Ep 10 (Step 029115): Train loss 1.966, Val loss 4.190\n",
            "Ep 10 (Step 029120): Train loss 2.255, Val loss 4.191\n",
            "Ep 10 (Step 029125): Train loss 2.096, Val loss 4.192\n",
            "Ep 10 (Step 029130): Train loss 1.924, Val loss 4.200\n",
            "Ep 10 (Step 029135): Train loss 1.975, Val loss 4.207\n",
            "Ep 10 (Step 029140): Train loss 2.183, Val loss 4.194\n",
            "Ep 10 (Step 029145): Train loss 2.119, Val loss 4.175\n",
            "Ep 10 (Step 029150): Train loss 2.215, Val loss 4.169\n",
            "Ep 10 (Step 029155): Train loss 2.035, Val loss 4.171\n",
            "Ep 10 (Step 029160): Train loss 2.150, Val loss 4.177\n",
            "Ep 10 (Step 029165): Train loss 2.120, Val loss 4.179\n",
            "Ep 10 (Step 029170): Train loss 2.112, Val loss 4.182\n",
            "Ep 10 (Step 029175): Train loss 2.252, Val loss 4.179\n",
            "Ep 10 (Step 029180): Train loss 2.210, Val loss 4.175\n",
            "Ep 10 (Step 029185): Train loss 2.102, Val loss 4.177\n",
            "Ep 10 (Step 029190): Train loss 1.837, Val loss 4.178\n",
            "Ep 10 (Step 029195): Train loss 1.993, Val loss 4.179\n",
            "Ep 10 (Step 029200): Train loss 2.262, Val loss 4.187\n",
            "Ep 10 (Step 029205): Train loss 2.055, Val loss 4.185\n",
            "Ep 10 (Step 029210): Train loss 2.096, Val loss 4.181\n",
            "Ep 10 (Step 029215): Train loss 2.272, Val loss 4.170\n",
            "Ep 10 (Step 029220): Train loss 1.841, Val loss 4.174\n",
            "Ep 10 (Step 029225): Train loss 2.080, Val loss 4.173\n",
            "Ep 10 (Step 029230): Train loss 2.163, Val loss 4.156\n",
            "Ep 10 (Step 029235): Train loss 2.232, Val loss 4.156\n",
            "Ep 10 (Step 029240): Train loss 2.096, Val loss 4.155\n",
            "Ep 10 (Step 029245): Train loss 1.835, Val loss 4.149\n",
            "Ep 10 (Step 029250): Train loss 2.124, Val loss 4.154\n",
            "Ep 10 (Step 029255): Train loss 2.134, Val loss 4.165\n",
            "Ep 10 (Step 029260): Train loss 1.924, Val loss 4.179\n",
            "Ep 10 (Step 029265): Train loss 2.624, Val loss 4.195\n",
            "Ep 10 (Step 029270): Train loss 2.077, Val loss 4.214\n",
            "Ep 10 (Step 029275): Train loss 2.101, Val loss 4.204\n",
            "Ep 10 (Step 029280): Train loss 2.021, Val loss 4.183\n",
            "Ep 10 (Step 029285): Train loss 1.865, Val loss 4.175\n",
            "Ep 10 (Step 029290): Train loss 2.052, Val loss 4.172\n",
            "Ep 10 (Step 029295): Train loss 1.876, Val loss 4.174\n",
            "Ep 10 (Step 029300): Train loss 2.034, Val loss 4.162\n",
            "Ep 10 (Step 029305): Train loss 1.785, Val loss 4.149\n",
            "Ep 10 (Step 029310): Train loss 1.910, Val loss 4.141\n",
            "Ep 10 (Step 029315): Train loss 1.874, Val loss 4.147\n",
            "Ep 10 (Step 029320): Train loss 1.973, Val loss 4.156\n",
            "Ep 10 (Step 029325): Train loss 2.213, Val loss 4.149\n",
            "Ep 10 (Step 029330): Train loss 2.260, Val loss 4.150\n",
            "Ep 10 (Step 029335): Train loss 2.069, Val loss 4.167\n",
            "Ep 10 (Step 029340): Train loss 2.008, Val loss 4.177\n",
            "Ep 10 (Step 029345): Train loss 2.253, Val loss 4.149\n",
            "Ep 10 (Step 029350): Train loss 1.965, Val loss 4.152\n",
            "Ep 10 (Step 029355): Train loss 2.085, Val loss 4.164\n",
            "Ep 10 (Step 029360): Train loss 2.132, Val loss 4.167\n",
            "Ep 10 (Step 029365): Train loss 2.093, Val loss 4.183\n",
            "Ep 10 (Step 029370): Train loss 2.092, Val loss 4.164\n",
            "Ep 10 (Step 029375): Train loss 2.165, Val loss 4.164\n",
            "Ep 10 (Step 029380): Train loss 2.152, Val loss 4.164\n",
            "Ep 10 (Step 029385): Train loss 2.136, Val loss 4.151\n",
            "Ep 10 (Step 029390): Train loss 2.010, Val loss 4.150\n",
            "Ep 10 (Step 029395): Train loss 1.842, Val loss 4.164\n",
            "Ep 10 (Step 029400): Train loss 1.880, Val loss 4.159\n",
            "Ep 10 (Step 029405): Train loss 2.452, Val loss 4.156\n",
            "Ep 10 (Step 029410): Train loss 2.236, Val loss 4.163\n",
            "Ep 10 (Step 029415): Train loss 2.106, Val loss 4.176\n",
            "Ep 10 (Step 029420): Train loss 2.196, Val loss 4.188\n",
            "Ep 10 (Step 029425): Train loss 2.027, Val loss 4.192\n",
            "Ep 10 (Step 029430): Train loss 2.048, Val loss 4.175\n",
            "Ep 10 (Step 029435): Train loss 2.068, Val loss 4.166\n",
            "Ep 10 (Step 029440): Train loss 2.127, Val loss 4.168\n",
            "Ep 10 (Step 029445): Train loss 2.091, Val loss 4.173\n",
            "Ep 10 (Step 029450): Train loss 2.170, Val loss 4.190\n",
            "Ep 10 (Step 029455): Train loss 2.232, Val loss 4.205\n",
            "Ep 10 (Step 029460): Train loss 2.196, Val loss 4.203\n",
            "Ep 10 (Step 029465): Train loss 1.981, Val loss 4.208\n",
            "Ep 10 (Step 029470): Train loss 2.459, Val loss 4.201\n",
            "Ep 10 (Step 029475): Train loss 2.089, Val loss 4.185\n",
            "Ep 10 (Step 029480): Train loss 2.078, Val loss 4.174\n",
            "Ep 10 (Step 029485): Train loss 2.098, Val loss 4.165\n",
            "Ep 10 (Step 029490): Train loss 1.915, Val loss 4.165\n",
            "Ep 10 (Step 029495): Train loss 2.103, Val loss 4.174\n",
            "Ep 10 (Step 029500): Train loss 2.157, Val loss 4.179\n",
            "Ep 10 (Step 029505): Train loss 2.213, Val loss 4.176\n",
            "Ep 10 (Step 029510): Train loss 2.299, Val loss 4.168\n",
            "Ep 10 (Step 029515): Train loss 2.128, Val loss 4.151\n",
            "Ep 10 (Step 029520): Train loss 1.736, Val loss 4.138\n",
            "Ep 10 (Step 029525): Train loss 1.931, Val loss 4.151\n",
            "Ep 10 (Step 029530): Train loss 2.099, Val loss 4.162\n",
            "Ep 10 (Step 029535): Train loss 2.026, Val loss 4.169\n",
            "Ep 10 (Step 029540): Train loss 2.212, Val loss 4.169\n",
            "Ep 10 (Step 029545): Train loss 1.948, Val loss 4.185\n",
            "Ep 10 (Step 029550): Train loss 2.136, Val loss 4.195\n",
            "Ep 10 (Step 029555): Train loss 2.062, Val loss 4.187\n",
            "Ep 10 (Step 029560): Train loss 2.089, Val loss 4.186\n",
            "Ep 10 (Step 029565): Train loss 2.127, Val loss 4.180\n",
            "Ep 10 (Step 029570): Train loss 2.076, Val loss 4.174\n",
            "Ep 10 (Step 029575): Train loss 2.007, Val loss 4.165\n",
            "Ep 10 (Step 029580): Train loss 2.285, Val loss 4.167\n",
            "Ep 10 (Step 029585): Train loss 2.200, Val loss 4.167\n",
            "Ep 10 (Step 029590): Train loss 2.049, Val loss 4.165\n",
            "Ep 10 (Step 029595): Train loss 2.170, Val loss 4.167\n",
            "Ep 10 (Step 029600): Train loss 2.031, Val loss 4.176\n",
            "Ep 10 (Step 029605): Train loss 2.083, Val loss 4.171\n",
            "Ep 10 (Step 029610): Train loss 2.071, Val loss 4.166\n",
            "Ep 10 (Step 029615): Train loss 2.126, Val loss 4.176\n",
            "Ep 10 (Step 029620): Train loss 2.196, Val loss 4.186\n",
            "Ep 10 (Step 029625): Train loss 2.165, Val loss 4.187\n",
            "Ep 10 (Step 029630): Train loss 2.107, Val loss 4.188\n",
            "Ep 10 (Step 029635): Train loss 2.248, Val loss 4.171\n",
            "Ep 10 (Step 029640): Train loss 2.199, Val loss 4.165\n",
            "Ep 10 (Step 029645): Train loss 1.929, Val loss 4.180\n",
            "Ep 10 (Step 029650): Train loss 2.227, Val loss 4.186\n",
            "Ep 10 (Step 029655): Train loss 2.018, Val loss 4.169\n",
            "Ep 10 (Step 029660): Train loss 2.044, Val loss 4.152\n",
            "Ep 10 (Step 029665): Train loss 2.025, Val loss 4.178\n",
            "Ep 10 (Step 029670): Train loss 1.898, Val loss 4.199\n",
            "Ep 10 (Step 029675): Train loss 2.058, Val loss 4.190\n",
            "Ep 10 (Step 029680): Train loss 1.908, Val loss 4.187\n",
            "Ep 10 (Step 029685): Train loss 2.035, Val loss 4.194\n",
            "Ep 10 (Step 029690): Train loss 2.176, Val loss 4.197\n",
            "Ep 10 (Step 029695): Train loss 1.962, Val loss 4.189\n",
            "Ep 10 (Step 029700): Train loss 1.895, Val loss 4.171\n",
            "Ep 10 (Step 029705): Train loss 2.069, Val loss 4.168\n",
            "Ep 10 (Step 029710): Train loss 2.060, Val loss 4.162\n",
            "Ep 10 (Step 029715): Train loss 2.237, Val loss 4.157\n",
            "Ep 10 (Step 029720): Train loss 2.075, Val loss 4.159\n",
            "Ep 10 (Step 029725): Train loss 2.215, Val loss 4.162\n",
            "Ep 10 (Step 029730): Train loss 2.172, Val loss 4.164\n",
            "Ep 10 (Step 029735): Train loss 2.005, Val loss 4.179\n",
            "Ep 10 (Step 029740): Train loss 2.096, Val loss 4.201\n",
            "Ep 10 (Step 029745): Train loss 2.075, Val loss 4.208\n",
            "Ep 10 (Step 029750): Train loss 2.145, Val loss 4.197\n",
            "Ep 10 (Step 029755): Train loss 1.936, Val loss 4.200\n",
            "Ep 10 (Step 029760): Train loss 1.973, Val loss 4.199\n",
            "Ep 10 (Step 029765): Train loss 1.986, Val loss 4.197\n",
            "Every effort moves you to the utmost of the King, And, for you, my Lord of Cambridge, I’ll make a thousand pound.  GLOUCESTER. I’ll wait upon your Grace, And I�\n",
            "Training completed in 62.47 minutes.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "f3012207",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T06:24:52.072636Z",
          "start_time": "2025-01-09T06:24:49.827535Z"
        },
        "id": "f3012207",
        "outputId": "40173819-6a27-4791-e068-be415f09405b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T06:26:42.350721Z",
          "start_time": "2025-01-09T06:26:40.201036Z"
        },
        "id": "242ee74c13102b9a",
        "outputId": "c4a458c5-9c46-44f3-c439-51b4861b9bbb"
      },
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "id": "242ee74c13102b9a",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you to the utmost of the King,\n",
            "And, for you, my Lord of Cambridge,\n",
            "I’ll make a\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T06:26:45.549385Z",
          "start_time": "2025-01-09T06:26:45.542159Z"
        },
        "id": "8e2b1bd2e8f8f2b"
      },
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "id": "8e2b1bd2e8f8f2b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6b67a93815df682"
      },
      "cell_type": "markdown",
      "source": [
        "**Top-k and Temperature**\n",
        "\n",
        " **Top-k** limits the model's choice to the *k* most probable next words, discarding less likely options. **Temperature** scales the probability distribution; higher values increase randomness by making the distribution more uniform, while lower values favor the most likely word by making it more peaked.  For instance, when creating a poem, high temperature with low Top-k might yield unusual word choices. Conversely, low temperature with high Top-k would produce a more predictable, coherent verse.  In essence, they dictate how \"focused\" versus \"creative\" the generated text becomes."
      ],
      "id": "6b67a93815df682"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T06:26:48.861257Z",
          "start_time": "2025-01-09T06:26:48.248391Z"
        },
        "id": "82bead5dd339a179",
        "outputId": "6f5c1249-037f-4449-cf61-9c629fadf4bd"
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "id": "82bead5dd339a179",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you, I will spare for you. Here it was much\n",
            "a gentleman and\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-09T06:26:56.639638Z",
          "start_time": "2025-01-09T06:26:56.027106Z"
        },
        "id": "20e4e6a685d321b9"
      },
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "id": "20e4e6a685d321b9",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "explorer",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}