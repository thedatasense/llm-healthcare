{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b14e532-9f4f-49af-aa92-2e7e5b6a1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json            # For parsing JSON data\n",
    "import random          # For setting seeds and shuffling data\n",
    "import requests        # For downloading dataset from URL\n",
    "import torch           # Main PyTorch library\n",
    "from torch.utils.data import Dataset, DataLoader  # For dataset handling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria  # HuggingFace components\n",
    "from tqdm import tqdm   # Progress bar utilities\n",
    "import re               # For text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576dd0ab-9f1b-48a6-a56a-2fa661a74e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 1 - Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "# Set the CUDA device to GPU 1\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using device: {torch.cuda.current_device()} - {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e5d1fd-ea47-4eff-a45e-f45ada732551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set Python's built-in random seed\n",
    "    random.seed(seed)\n",
    "    # Set PyTorch's CPU random seed\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for all available GPUs\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Request cuDNN to use deterministic algorithms\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Disable cuDNN's auto-tuner for consistent behavior\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f76f85e-dedc-4f86-a513-c54e9199dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(instruction, solution=None):\n",
    "    # Add solution with end token if provided\n",
    "    wrapped_solution = \"\"\n",
    "    if solution:\n",
    "        wrapped_solution = f\"\\n{solution}\\n<|im_end|>\"\n",
    "\n",
    "    # Build chat format with system, user, and assistant messages\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\" + wrapped_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122d6462-27f1-4ff3-997f-44e9cbb3374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, text, return_tensor=False):\n",
    "    # If tensor output is requested, encode with PyTorch tensors\n",
    "    if return_tensor:\n",
    "        return tokenizer.encode(\n",
    "            text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "        )\n",
    "    # Otherwise return list of token IDs\n",
    "    else:\n",
    "        return tokenizer.encode(text, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2271f148-f019-4b61-9de3-def461352160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    Custom stopping criteria for text generation.\n",
    "    Stops when a specific end token sequence is generated.\n",
    "\n",
    "    Args:\n",
    "        end_tokens (list): Token IDs that signal generation should stop\n",
    "        device: Device where the model is running\n",
    "    \"\"\"\n",
    "    def __init__(self, end_tokens, device):\n",
    "        self.end_tokens = torch.tensor(end_tokens).to(device)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        \"\"\"\n",
    "        Checks if generation should stop for each sequence.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Current generated token IDs\n",
    "            scores: Token probabilities\n",
    "\n",
    "        Returns:\n",
    "            tensor: Boolean tensor indicating which sequences should stop\n",
    "        \"\"\"\n",
    "        should_stop = []\n",
    "\n",
    "        # Check each sequence for end tokens\n",
    "        for sequence in input_ids:\n",
    "            if len(sequence) >= len(self.end_tokens):\n",
    "                # Compare last tokens with end tokens\n",
    "                last_tokens = sequence[-len(self.end_tokens):]\n",
    "                should_stop.append(torch.all(last_tokens == self.end_tokens))\n",
    "            else:\n",
    "                should_stop.append(False)\n",
    "\n",
    "        return torch.tensor(should_stop, device=input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2fe432-c6b5-4a12-bf5d-c2aa69722a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptCompletionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for instruction-completion pairs.\n",
    "    Handles the conversion of text data into model-ready format.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries containing instructions and solutions\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of examples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single training example.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the example to fetch\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains input_ids, labels, prompt, and expected completion\n",
    "        \"\"\"\n",
    "        # Get example from dataset\n",
    "        item = self.data[idx]\n",
    "        # Build full prompt with instruction\n",
    "        prompt = build_prompt(item[\"instruction\"])\n",
    "        # Format completion with end token\n",
    "        completion = f\"\"\"{item[\"solution\"]}\\n<|im_end|>\"\"\"\n",
    "\n",
    "        # Convert text to token IDs\n",
    "        encoded_prompt = encode_text(self.tokenizer, prompt)\n",
    "        encoded_completion = encode_text(self.tokenizer, completion)\n",
    "        eos_token = [self.tokenizer.eos_token_id]\n",
    "\n",
    "        # Combine for full input sequence\n",
    "        input_ids = encoded_prompt + encoded_completion + eos_token\n",
    "        # Create labels: -100 for prompt (ignored in loss)\n",
    "        labels = [-100] * len(encoded_prompt) + encoded_completion + eos_token\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"prompt\": prompt,\n",
    "            \"expected_completion\": completion\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "411cdd28-1ae5-40f2-8f4f-07ca63df7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collates batch of examples into training-ready format.\n",
    "    Handles padding and conversion to tensors.\n",
    "\n",
    "    Args:\n",
    "        batch: List of examples from Dataset\n",
    "\n",
    "    Returns:\n",
    "        tuple: (input_ids, attention_mask, labels, prompts, expected_completions)\n",
    "    \"\"\"\n",
    "    # Find longest sequence for padding\n",
    "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
    "\n",
    "    # Pad input sequences\n",
    "    input_ids = [\n",
    "        item[\"input_ids\"] +\n",
    "        [tokenizer.pad_token_id] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # Pad label sequences\n",
    "    labels = [\n",
    "        item[\"labels\"] +\n",
    "        [-100] * (max_length - len(item[\"labels\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # Create attention masks\n",
    "    attention_mask = [\n",
    "        [1] * len(item[\"input_ids\"]) +\n",
    "        [0] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    prompts = [item[\"prompt\"] for item in batch]\n",
    "    expected_completions = [item[\"expected_completion\"] for item in batch]\n",
    "\n",
    "    return (\n",
    "        torch.tensor(input_ids),\n",
    "        torch.tensor(attention_mask),\n",
    "        torch.tensor(labels),\n",
    "        prompts,\n",
    "        expected_completions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9012200-0b98-4fd7-952d-518abcf02f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalizes text for consistent comparison.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "        str: Normalized text\n",
    "    \"\"\"\n",
    "    # Remove leading/trailing whitespace and convert to lowercase\n",
    "    text = text.strip().lower()\n",
    "    # Replace multiple whitespace characters with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1b5eac-0f9b-4c8d-a2fd-5f1a31b47a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Generates text completion for a given prompt.\n",
    "\n",
    "    Args:\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Associated tokenizer\n",
    "        prompt (str): Input prompt\n",
    "        max_new_tokens (int): Maximum number of tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        str: Generated completion\n",
    "    \"\"\"\n",
    "    # Encode prompt and move to model's device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Setup end token detection\n",
    "    end_tokens = tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)\n",
    "    stopping_criteria = [EndTokenStoppingCriteria(end_tokens, model.device)]\n",
    "\n",
    "    # Generate completion\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids[\"input_ids\"],\n",
    "        attention_mask=input_ids[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )[0]\n",
    "\n",
    "    # Extract and decode only the generated part\n",
    "    generated_ids = output_ids[input_ids[\"input_ids\"].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_ids).strip()\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86788fc3-62ed-4bec-8409-23abcd19426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_path, test_input):\n",
    "    \"\"\"\n",
    "    Tests a saved model on a single input.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to saved model\n",
    "        test_input (str): Instruction to test\n",
    "    \"\"\"\n",
    "    # Setup device and load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Generate and display prediction\n",
    "    prompt = build_prompt(test_input)\n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "\n",
    "    print(f\"\\nInput: {test_input}\")\n",
    "    print(f\"Full generated text: {generated_text}\")\n",
    "    print(f\"\"\"Cleaned response: {generated_text.replace(\"<|im_end|>\", \"\").strip()}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f90910f-e76c-43aa-ae7d-87072cb88bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_prepare_data(data_url, tokenizer, batch_size, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Downloads and prepares dataset for training.\n",
    "\n",
    "    Args:\n",
    "        data_url (str): URL of the dataset\n",
    "        tokenizer: Tokenizer for text processing\n",
    "        batch_size (int): Batch size for DataLoader\n",
    "        test_ratio (float): Proportion of data for testing\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Download dataset\n",
    "    response = requests.get(data_url)\n",
    "    dataset = []\n",
    "    # Parse each line as an instruction-solution pair\n",
    "    for line in response.text.splitlines():\n",
    "        if line.strip():  # Skip empty lines\n",
    "            entry = json.loads(line)\n",
    "            dataset.append({\n",
    "                \"instruction\": entry[\"instruction\"],\n",
    "                \"solution\": entry[\"solution\"]\n",
    "            })\n",
    "\n",
    "    # Split into train and test sets\n",
    "    random.shuffle(dataset)\n",
    "    split_index = int(len(dataset) * (1 - test_ratio))\n",
    "    train_data = dataset[:split_index]\n",
    "    test_data = dataset[split_index:]\n",
    "\n",
    "    # Print dataset statistics\n",
    "    print(f\"\\nDataset size: {len(dataset)}\")\n",
    "    print(f\"Training samples: {len(train_data)}\")\n",
    "    print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = PromptCompletionDataset(train_data, tokenizer)\n",
    "    test_dataset = PromptCompletionDataset(test_data, tokenizer)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17219ed2-cc18-4cb9-a64b-00bc1380f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters():\n",
    "    \"\"\"\n",
    "    Returns training hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (num_epochs, batch_size, learning_rate)\n",
    "    \"\"\"\n",
    "    # Fewer epochs for instruction tuning as it's more data-efficient\n",
    "    num_epochs = 4\n",
    "    # Standard batch size that works well with most GPU memory\n",
    "    batch_size = 16\n",
    "    # Standard learning rate for fine-tuning transformers\n",
    "    learning_rate = 5e-5\n",
    "\n",
    "    return num_epochs, batch_size, learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8aa5047d-0660-4bdf-a7fa-41c2a15f520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "data_url = \"https://www.thelmbook.com/data/instruct\"\n",
    "model_name = \"openai-community/gpt2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78ca8f16-b9a7-4b8f-a271-d4c87408b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eece0680-fd1a-410d-8744-4cde5fffbbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 510\n",
      "Training samples: 459\n",
      "Test samples: 51\n"
     ]
    }
   ],
   "source": [
    "# Get hyperparameters and prepare data\n",
    "num_epochs, batch_size, learning_rate = get_hyperparameters()\n",
    "train_loader, test_loader = download_and_prepare_data(data_url, tokenizer, batch_size)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49d693aa-6630-4f3e-ae68-55cc95ef04dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 1 - Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "# Set the CUDA device to GPU 1\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using device: {torch.cuda.current_device()} - {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8af6f9a5-3268-476b-a6c7-016af18f8099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 29/29 [00:06<00:00,  4.49it/s, Loss=1.6] \n",
      "Epoch 2/4: 100%|██████████| 29/29 [00:06<00:00,  4.79it/s, Loss=1.02] \n",
      "Epoch 3/4: 100%|██████████| 29/29 [00:06<00:00,  4.67it/s, Loss=0.679]\n",
      "Epoch 4/4: 100%|██████████| 29/29 [00:06<00:00,  4.60it/s, Loss=0.421]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # Tracks cumulative loss for the epoch\n",
    "    num_batches = 0  # Tracks number of batches processed\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # Unpack batch and move to device\n",
    "        input_ids, attention_mask, labels, _, _ = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass: compute model outputs and loss\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass: compute gradients and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        average_loss = total_loss / num_batches\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"Loss\": average_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daa11428-c22a-403e-9b77-a3969fdd5a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing finetuned model:\n",
      "Using device: cuda\n",
      "\n",
      "Input: how to kill a bird?\n",
      "Full generated text: a bird is a dangerous and often lethal threat to humans and other animals.\n",
      "<|im_end|>\n",
      "Cleaned response: a bird is a dangerous and often lethal threat to humans and other animals.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./models/finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./models/finetuned_model\")\n",
    "\n",
    "print(\"\\nTesting finetuned model:\")\n",
    "test_input = \"how to kill a bird?\"\n",
    "test_model(\"./finetuned_model\", test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50e7e8-159f-4d45-963f-be7756391c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explorer",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
